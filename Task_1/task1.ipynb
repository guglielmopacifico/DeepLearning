{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1245ce030>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from Common import NeuralNet\n",
    "import time\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "torch.manual_seed(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pinns:\n",
    "    def __init__(self, n_int_, n_sb_, n_tb_, alpha_f_=0.05, alpha_s_=0.08, h_f_=5, h_s_=6, T_hot_=4, T_0_=1, U_f_=1):\n",
    "        self.n_int = n_int_     # n_int_:= number of intertior points\n",
    "        self.n_sb = n_sb_       # n_sb_ := number of spatial boundary points\n",
    "        self.n_tb = n_tb_       # n_tb_ := number of time boundary points\n",
    "\n",
    "        # Set the paremeters of the equation\n",
    "        self.alpha_f = alpha_f_\n",
    "        self.alpha_s = alpha_s_\n",
    "        self.h_f = h_f_\n",
    "        self.h_s = h_s_\n",
    "        self.T_hot = T_hot_\n",
    "        self.T_0 = T_0_\n",
    "        self.U_f = U_f_\n",
    "\n",
    "        # Extrema of the solution domain (t,x) in [0, t]x[0, L]\n",
    "        self.domain_extrema = torch.tensor([[0, 1],  # Time dimension\n",
    "                                            [0, 1]])  # Space dimension\n",
    "\n",
    "        # Number of space dimensions\n",
    "        self.space_dimensions = 1\n",
    "\n",
    "        # Parameter to balance role of data and PDE\n",
    "        self.lambda_u = 10\n",
    "\n",
    "        # F Dense NN to approximate the solution of the two underlying reaction-convection-diffusion equations\n",
    "        self.approximate_solution = NeuralNet(input_dimension=self.domain_extrema.shape[0], output_dimension=2, # is a NN with input_dim=2 (time & space), output_dim=2 (fluid_temp & solid_temp)\n",
    "                                              n_hidden_layers=4,\n",
    "                                              neurons=20,\n",
    "                                              regularization_param=0.,\n",
    "                                              regularization_exp=2.,\n",
    "                                              retrain_seed=42)\n",
    "\n",
    "        # Generator of Sobol sequences\n",
    "        self.soboleng = torch.quasirandom.SobolEngine(dimension=self.domain_extrema.shape[0])   # it will create a 2 cloumns tensor, the rows nunmber is specified after every time it is used\n",
    "\n",
    "        # Training sets S_sb, S_tb, S_int as torch dataloader\n",
    "        self.training_set_sb, self.training_set_tb, self.training_set_int = self.assemble_datasets()\n",
    "\n",
    "    ################################################################################################\n",
    "    # Function returning the input-output tensor required to assemble the training set S_tb corresponding to the temporal boundary\n",
    "    def add_temporal_boundary_points(self):\n",
    "        t0 = self.domain_extrema[0, 0]\n",
    "        input_tb = self.soboleng.draw(self.n_tb)    # input_sb has two columns (t, x) both with random numbers in the two respective domains\n",
    "        input_tb[:, 0] = torch.full(input_tb[:, 0].shape, t0)   # overwrite the entier column of time with t0\n",
    "        output_tb = torch.full(input_tb.shape, self.T_0)    # also the output has to be with 2 columns, one for T_f & one for T_s\n",
    "\n",
    "        return input_tb, output_tb  # input_tb is the sequence of x_n; output_tb is the sequence u0(x_n)\n",
    "\n",
    "    # Function returning the input-output tensor required to assemble the training set S_sb corresponding to the spatial boundary\n",
    "    def add_spatial_boundary_points(self):\n",
    "        x0 = self.domain_extrema[1, 0]\n",
    "        xL = self.domain_extrema[1, 1]\n",
    "\n",
    "        input_sb = self.soboleng.draw(self.n_sb)\n",
    "\n",
    "        input_sb_0 = torch.clone(input_sb)\n",
    "        input_sb_0[:, 1] = torch.full(input_sb_0[:, 1].shape, x0)\n",
    "\n",
    "        input_sb_L = torch.clone(input_sb)\n",
    "        input_sb_L[:, 1] = torch.full(input_sb_L[:, 1].shape, xL)\n",
    "\n",
    "        # There are 2 conditions on the function T -> T_x0 & T_xL\n",
    "        # there are 2 conditions on the function dT/dx -> T_x0_dx & T_xL_dx\n",
    "        # however we don't have all these contidions for both T_f and T_s (T = [T_f, T_s])\n",
    "        # We will make 3 of the 4 outputs and fill the vacant conditions with 999\n",
    "        # and remove them when computing the loss\n",
    "\n",
    "        # condition T_x0\n",
    "        output_sb_0 = ((self.T_hot-self.T_0)/(1+torch.exp(-200*(input_sb-0.25)))+self.T_0)  # is a tensor with two columns [T_f, T_s], however we will just want to use the first one\n",
    "        output_sb_0[:,1] = torch.full(input_sb[:, 1].shape, 999)\n",
    "        # condition T_x0_dx\n",
    "        output_sb_0_dx = torch.zeros_like(input_sb)     # is a tensor with two columns as before, we will just want the second one (T_s_dx)\n",
    "        output_sb_0_dx[:,0] = torch.full(input_sb[:, 1].shape, 999)\n",
    "        # conditions T_xL_dx\n",
    "        output_sb_L_dx = torch.zeros_like(input_sb)     # is a tensor with two columns a before, of this we will use both\n",
    "        \n",
    "        # requires the grad for these tensors as we will have to compute the derivatives\n",
    "        input_sb_0.requires_grad_()\n",
    "        input_sb_L.requires_grad_()\n",
    "\n",
    "        return torch.cat([input_sb_0, input_sb_0, input_sb_L], 0), torch.cat([output_sb_0, output_sb_0_dx, output_sb_L_dx], 0)  # we return it like this so we just have to devide input & output in 3 to get each condition\n",
    "\n",
    "    #  Function returning the input-output tensor required to assemble the training set S_int corresponding to the interior domain where the PDE is enforced\n",
    "    def add_interior_points(self):\n",
    "        input_int = self.soboleng.draw(self.n_int)\n",
    "        output_int = torch.zeros_like(input_int)\n",
    "        \n",
    "        return input_int, output_int\n",
    "\n",
    "    # Function returning the training sets S_sb, S_tb, S_int as dataloader\n",
    "    def assemble_datasets(self):\n",
    "        input_sb, output_sb = self.add_spatial_boundary_points()   # S_sb\n",
    "        input_tb, output_tb = self.add_temporal_boundary_points()  # S_tb\n",
    "        input_int, output_int = self.add_interior_points()         # S_int\n",
    "\n",
    "        training_set_sb = DataLoader(torch.utils.data.TensorDataset(input_sb, output_sb), batch_size=3*self.space_dimensions*self.n_sb, shuffle=False)  #batch_size has *2 since there are 2 space boundaries conditions\n",
    "        training_set_tb = DataLoader(torch.utils.data.TensorDataset(input_tb, output_tb), batch_size=self.n_tb, shuffle=False)\n",
    "        training_set_int = DataLoader(torch.utils.data.TensorDataset(input_int, output_int), batch_size=self.n_int, shuffle=False)\n",
    "\n",
    "        return training_set_sb, training_set_tb, training_set_int\n",
    "\n",
    "    ################################################################################################\n",
    "    # Function to compute the terms required in the definition of the TEMPORAL boundary residual\n",
    "    def apply_initial_condition(self, input_tb):\n",
    "        u_pred_tb = self.approximate_solution(input_tb)\n",
    "        \n",
    "        return u_pred_tb\n",
    "\n",
    "    # Function to compute the terms required in the definition of the SPATIAL boundary residual\n",
    "    def apply_boundary_conditions(self, input_sb):\n",
    "        # input_tb is a tensor of size [3*self.n_sb, 2]\n",
    "        # as defined in \"add_spatial_boundary_points\" we have 3 boundary conditions\n",
    "        # we then have to devide the input_sb in 3, compute the 3 boundary conditions prediction and return them\n",
    "        #Â as done in \"add_spatial_boundary_points\", we insert 777 for the vacant conditions\n",
    "        assert (input_sb.requires_grad==True)   # make sure the grad is requested so we can compute the derivatives\n",
    "\n",
    "        # condition T_x0\n",
    "        input_sb_0 = input_sb[:int(input_sb.shape[0]/3), :]\n",
    "        u_pred_sb_0 = self.approximate_solution(input_sb_0)\n",
    "        u_pred_sb_0[:, 1] = torch.full(u_pred_sb_0[:, 1].shape, 777)\n",
    "        \n",
    "        # condition T_x0_dx, here the inputs are the same as for T_x0 and so also u_pred, we will then use them\n",
    "        u_pred_sb_0_dx = torch.zeros_like(u_pred_sb_0)\n",
    "        u_pred_sb_0_dx[:, 0] = torch.full(u_pred_sb_0_dx[:, 0].shape, 777)\n",
    "         \n",
    "        u_pred_Ts_x0 = self.approximate_solution(input_sb_0)[:, 1]   # just the Ts component from the prediction\n",
    "        u_pred_Ts_x0_dx = torch.autograd.grad(u_pred_Ts_x0.sum(), input_sb_0 , create_graph=True)[0][:, 1]    # just the dx component\n",
    "        u_pred_sb_0_dx[:, 1] = u_pred_Ts_x0_dx\n",
    "\n",
    "        # conditions T_xL_dx\n",
    "        input_sb_L = input_sb[int(2*input_sb.shape[0]/3):, :]\n",
    "        u_pred_sb_L = self.approximate_solution(input_sb_L)\n",
    "        u_pred_sb_L_dx = torch.zeros_like(u_pred_sb_L)\n",
    "\n",
    "        u_pred_Tf_xL = u_pred_sb_L[:, 0]   # just the Tf component\n",
    "        u_pred_Ts_xL = u_pred_sb_L[:, 1]   # just the Ts component\n",
    "\n",
    "        u_pred_sb_L_dx[:, 0] = torch.autograd.grad(u_pred_Tf_xL.sum(), input_sb_L , create_graph=True)[0][:, 1]    # gradiend of Tf\n",
    "        u_pred_sb_L_dx[:, 1] = torch.autograd.grad(u_pred_Ts_xL.sum(), input_sb_L , create_graph=True)[0][:, 1]    # gradiend of Ts\n",
    "\n",
    "        return torch.cat([u_pred_sb_0, u_pred_sb_0_dx, u_pred_sb_L_dx], 0)\n",
    "\n",
    "    # Function to compute the PDE residuals\n",
    "    def compute_pde_residual(self, input_int):\n",
    "        input_int.requires_grad = True\n",
    "        u = self.approximate_solution(input_int)\n",
    "\n",
    "        # grad compute the gradient of a \"SCALAR\" function L with respect to some input nxm TENSOR Z=[[x1, y1],[x2,y2],[x3,y3],...,[xn,yn]], m=2\n",
    "        # it returns grad_L = [[dL/dx1, dL/dy1],[dL/dx2, dL/dy2],[dL/dx3, dL/dy3],...,[dL/dxn, dL/dyn]]\n",
    "        # Note: pytorch considers a tensor [u1, u2,u3, ... ,un] a vectorial function\n",
    "        # whereas sum_u = u1 + u2 + u3 + u4 + ... + un as a \"scalar\" one\n",
    "\n",
    "        # In our case ui = u(xi), therefore the line below returns:\n",
    "        # grad_u = [[dsum_u/dx1, dsum_u/dy1],[dsum_u/dx2, dsum_u/dy2],[dsum_u/dx3, dL/dy3],...,[dsum_u/dxm, dsum_u/dyn]]\n",
    "        # and dsum_u/dxi = d(u1 + u2 + u3 + u4 + ... + un)/dxi = d(u(x1) + u(x2) u3(x3) + u4(x4) + ... + u(xn))/dxi = dui/dxi\n",
    "\n",
    "        # Since u for us is u = (uf, us), we have to devide the two cases\n",
    "        uf = u[:, 0]\n",
    "        grad_uf = torch.autograd.grad(uf.sum(), input_int, create_graph=True)[0]\n",
    "        grad_uf_t = grad_uf[:, 0]\n",
    "        grad_uf_x = grad_uf[:, 1]\n",
    "        grad_uf_xx = torch.autograd.grad(grad_uf_x.sum(), input_int, create_graph=True)[0][:, 1]\n",
    "\n",
    "        us = u[:, 1]\n",
    "        grad_us = torch.autograd.grad(us.sum(), input_int, create_graph=True)[0]\n",
    "        grad_us_t = grad_us[:, 0]\n",
    "        grad_us_x = grad_us[:, 1]\n",
    "        grad_us_xx = torch.autograd.grad(grad_us_x.sum(), input_int, create_graph=True)[0][:, 1]\n",
    "\n",
    "        residual_1 = (grad_uf_t + self.U_f*grad_uf_x) - (self.alpha_f*grad_uf_xx - self.h_f*(uf-us))\n",
    "        residual_2 = (grad_us_t) - (self.alpha_s*grad_us_xx + self.h_s*(uf-us))\n",
    "        \n",
    "        return residual_1.reshape(-1, ), residual_2.reshape(-1, )\n",
    "\n",
    "    # Function to compute the total loss (weighted sum of spatial boundary loss, temporal boundary loss and interior loss)\n",
    "    def compute_loss(self, inp_train_sb, u_train_sb, inp_train_tb, u_train_tb, inp_train_int, verbose=True):\n",
    "        u_pred_sb = self.apply_boundary_conditions(inp_train_sb)\n",
    "        u_pred_tb = self.apply_initial_condition(inp_train_tb)\n",
    "\n",
    "        assert (u_pred_sb.shape[1] == u_train_sb.shape[1])\n",
    "        assert (u_pred_tb.shape[1] == u_train_tb.shape[1])\n",
    "\n",
    "\n",
    "        r1_int, r2_int = self.compute_pde_residual(inp_train_int)\n",
    "        r_tb = u_train_tb - u_pred_tb\n",
    "        # For the boundary conditions we have to consider just the one we have in our problem\n",
    "        # condition Tf_x0\n",
    "        u_train_sb_0 = u_train_sb[:int(u_train_sb.shape[0]/3), 0]\n",
    "        u_pred_sb_0 = u_pred_sb[:int(u_pred_sb.shape[0]/3), 0]\n",
    "\n",
    "        r_sb_0 = u_train_sb_0 - u_pred_sb_0\n",
    "\n",
    "        # condition Ts_x0_dx\n",
    "        u_train_sb_0_dx = u_train_sb[int(u_train_sb.shape[0]/3):int(2*u_train_sb.shape[0]/3), 1]\n",
    "        u_pred_sb_0_dx = u_pred_sb[int(u_train_sb.shape[0]/3):int(2*u_train_sb.shape[0]/3), 1]\n",
    "\n",
    "        r_sb_0_dx = u_train_sb_0_dx - u_pred_sb_0_dx\n",
    "\n",
    "        # conditions Tf/s_xL_dx\n",
    "        u_train_sb_L_dx = u_train_sb[int(2*u_train_sb.shape[0]/3):, :]\n",
    "        u_pred_sb_L_dx = u_pred_sb[int(2*u_train_sb.shape[0]/3):, :]\n",
    "\n",
    "        r_sb_L_dx = u_train_sb_L_dx - u_pred_sb_L_dx\n",
    "\n",
    "        loss_sb = torch.mean(abs(r_sb_0) ** 2) + torch.mean(abs(r_sb_0_dx) ** 2) + torch.mean(abs(r_sb_L_dx) ** 2)\n",
    "        loss_tb = torch.mean(abs(r_tb) ** 2)\n",
    "        loss_int = torch.mean(abs(r1_int) ** 2) + torch.mean(abs(r2_int) ** 2)\n",
    "\n",
    "        loss_u = loss_sb + loss_tb\n",
    "\n",
    "        loss = torch.log10(self.lambda_u * (loss_sb + loss_tb) + loss_int)\n",
    "        # loss = torch.log10((loss_sb + 0*loss_tb) + 0*loss_int)\n",
    "        if verbose: print(\"Total loss: \", round(loss.item(), 4), \"| PDE Loss: \", round(torch.log10(loss_u).item(), 4), \"| Function Loss: \", round(torch.log10(loss_int).item(), 4))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    ################################################################################################\n",
    "    def fit(self, num_epochs, optimizer, verbose=True):\n",
    "        history = list()\n",
    "\n",
    "        # Loop over epochs\n",
    "        for epoch in range(num_epochs):\n",
    "            if verbose: print(\"################################ \", epoch, \" ################################\")\n",
    "\n",
    "            for j, ((inp_train_sb, u_train_sb), (inp_train_tb, u_train_tb), (inp_train_int, u_train_int)) in enumerate(zip(self.training_set_sb, self.training_set_tb, self.training_set_int)):\n",
    "                def closure():\n",
    "                    optimizer.zero_grad()\n",
    "                    loss = self.compute_loss(inp_train_sb, u_train_sb, inp_train_tb, u_train_tb, inp_train_int, verbose=verbose)\n",
    "                    loss.backward()\n",
    "\n",
    "                    history.append(loss.item())\n",
    "                    return loss\n",
    "                \n",
    "                optimizer.step(closure=closure)\n",
    "\n",
    "        print('Final Loss: ', history[-1])\n",
    "\n",
    "        return history\n",
    "\n",
    "    ################################################################################################\n",
    "    def plotting(self):\n",
    "        inputs = self.soboleng.draw(100000)\n",
    "\n",
    "        output = self.approximate_solution(inputs).reshape(-1, )\n",
    "        exact_output = self.exact_solution(inputs).reshape(-1, )\n",
    "\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(16, 8), dpi=150)\n",
    "        im1 = axs[0].scatter(inputs[:, 1].detach(), inputs[:, 0].detach(), c=exact_output.detach(), cmap=\"jet\")\n",
    "        axs[0].set_xlabel(\"x\")\n",
    "        axs[0].set_ylabel(\"t\")\n",
    "        plt.colorbar(im1, ax=axs[0])\n",
    "        axs[0].grid(True, which=\"both\", ls=\":\")\n",
    "        im2 = axs[1].scatter(inputs[:, 1].detach(), inputs[:, 0].detach(), c=output.detach(), cmap=\"jet\")\n",
    "        axs[1].set_xlabel(\"x\")\n",
    "        axs[1].set_ylabel(\"t\")\n",
    "        plt.colorbar(im2, ax=axs[1])\n",
    "        axs[1].grid(True, which=\"both\", ls=\":\")\n",
    "        axs[0].set_title(\"Exact Solution\")\n",
    "        axs[1].set_title(\"Approximate Solution\")\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "        err = (torch.mean((output - exact_output) ** 2) / torch.mean(exact_output ** 2)) ** 0.5 * 100\n",
    "        print(\"L2 Relative Error Norm: \", err.item(), \"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_int = 256\n",
    "n_sb = 64\n",
    "n_tb = 64\n",
    "\n",
    "pinn = Pinns(n_int, n_sb, n_tb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 1\n",
    "optimizer_LBFGS = optim.LBFGS(pinn.approximate_solution.parameters(),\n",
    "                              lr=float(0.5),\n",
    "                              max_iter=50000,\n",
    "                              max_eval=50000,\n",
    "                              history_size=150,\n",
    "                              line_search_fn=\"strong_wolfe\",\n",
    "                              tolerance_change=1.0 * np.finfo(float).eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################  0  ################################\n",
      "Total loss:  2.3566 | PDE Loss:  1.1432 | Function Loss:  1.9457\n",
      "Total loss:  2.3012 | PDE Loss:  1.1177 | Function Loss:  1.8385\n",
      "Total loss:  2.1035 | PDE Loss:  0.9068 | Function Loss:  1.665\n",
      "Total loss:  1.9923 | PDE Loss:  0.8433 | Function Loss:  1.4556\n",
      "Total loss:  1.8049 | PDE Loss:  0.7457 | Function Loss:  0.9101\n",
      "Total loss:  1.7422 | PDE Loss:  0.7097 | Function Loss:  0.5996\n",
      "Total loss:  1.7097 | PDE Loss:  0.6703 | Function Loss:  0.6478\n",
      "Total loss:  1.6176 | PDE Loss:  0.4676 | Function Loss:  1.0829\n",
      "Total loss:  1.5142 | PDE Loss:  0.3453 | Function Loss:  1.0223\n",
      "Total loss:  1.4218 | PDE Loss:  0.2322 | Function Loss:  0.9705\n",
      "Total loss:  1.4072 | PDE Loss:  0.215 | Function Loss:  0.9605\n",
      "Total loss:  1.362 | PDE Loss:  0.1742 | Function Loss:  0.9072\n",
      "Total loss:  1.3305 | PDE Loss:  0.1292 | Function Loss:  0.8998\n",
      "Total loss:  1.3089 | PDE Loss:  0.0995 | Function Loss:  0.8916\n",
      "Total loss:  1.2792 | PDE Loss:  0.0627 | Function Loss:  0.873\n",
      "Total loss:  1.2456 | PDE Loss:  0.0561 | Function Loss:  0.7943\n",
      "Total loss:  1.2194 | PDE Loss:  0.066 | Function Loss:  0.6929\n",
      "Total loss:  1.1925 | PDE Loss:  0.0552 | Function Loss:  0.6257\n",
      "Total loss:  1.1324 | PDE Loss:  0.0075 | Function Loss:  0.5302\n",
      "Total loss:  1.054 | PDE Loss:  -0.0566 | Function Loss:  0.4059\n",
      "Total loss:  1.0336 | PDE Loss:  -0.0942 | Function Loss:  0.4402\n",
      "Total loss:  1.0191 | PDE Loss:  -0.1023 | Function Loss:  0.4063\n",
      "Total loss:  1.0118 | PDE Loss:  -0.1035 | Function Loss:  0.3795\n",
      "Total loss:  1.0027 | PDE Loss:  -0.1127 | Function Loss:  0.3706\n",
      "Total loss:  0.9889 | PDE Loss:  -0.1257 | Function Loss:  0.3543\n",
      "Total loss:  0.9689 | PDE Loss:  -0.1579 | Function Loss:  0.3722\n",
      "Total loss:  0.9337 | PDE Loss:  -0.2039 | Function Loss:  0.3676\n",
      "Total loss:  0.8939 | PDE Loss:  -0.2617 | Function Loss:  0.3726\n",
      "Total loss:  0.8766 | PDE Loss:  -0.2774 | Function Loss:  0.3517\n",
      "Total loss:  0.864 | PDE Loss:  -0.2819 | Function Loss:  0.3194\n",
      "Total loss:  0.8461 | PDE Loss:  -0.3024 | Function Loss:  0.308\n",
      "Total loss:  0.8311 | PDE Loss:  -0.3119 | Function Loss:  0.2791\n",
      "Total loss:  0.8118 | PDE Loss:  -0.3273 | Function Loss:  0.2498\n",
      "Total loss:  0.7909 | PDE Loss:  -0.3444 | Function Loss:  0.2185\n",
      "Total loss:  0.7785 | PDE Loss:  -0.3563 | Function Loss:  0.2049\n",
      "Total loss:  0.768 | PDE Loss:  -0.3755 | Function Loss:  0.2171\n",
      "Total loss:  0.7616 | PDE Loss:  -0.3857 | Function Loss:  0.2204\n",
      "Total loss:  0.7557 | PDE Loss:  -0.4051 | Function Loss:  0.2463\n",
      "Total loss:  0.7506 | PDE Loss:  -0.4094 | Function Loss:  0.2392\n",
      "Total loss:  0.7422 | PDE Loss:  -0.4127 | Function Loss:  0.2192\n",
      "Total loss:  0.7305 | PDE Loss:  -0.4168 | Function Loss:  0.1894\n",
      "Total loss:  0.7191 | PDE Loss:  -0.422 | Function Loss:  0.1619\n",
      "Total loss:  0.7038 | PDE Loss:  -0.4317 | Function Loss:  0.1318\n",
      "Total loss:  0.6896 | PDE Loss:  -0.4403 | Function Loss:  0.1019\n",
      "Total loss:  0.6759 | PDE Loss:  -0.4499 | Function Loss:  0.0763\n",
      "Total loss:  0.6599 | PDE Loss:  -0.4598 | Function Loss:  0.0417\n",
      "Total loss:  0.6401 | PDE Loss:  -0.4736 | Function Loss:  0.0028\n",
      "Total loss:  0.616 | PDE Loss:  -0.4824 | Function Loss:  -0.0772\n",
      "Total loss:  0.6017 | PDE Loss:  -0.4915 | Function Loss:  -0.1121\n",
      "Total loss:  0.5915 | PDE Loss:  -0.4983 | Function Loss:  -0.1374\n",
      "Total loss:  0.5799 | PDE Loss:  -0.5 | Function Loss:  -0.1945\n",
      "Total loss:  0.5681 | PDE Loss:  -0.5044 | Function Loss:  -0.2453\n",
      "Total loss:  0.5586 | PDE Loss:  -0.5023 | Function Loss:  -0.3245\n",
      "Total loss:  0.554 | PDE Loss:  -0.5047 | Function Loss:  -0.344\n",
      "Total loss:  0.5496 | PDE Loss:  -0.5067 | Function Loss:  -0.3654\n",
      "Total loss:  0.5457 | PDE Loss:  -0.511 | Function Loss:  -0.3666\n",
      "Total loss:  0.5419 | PDE Loss:  -0.5147 | Function Loss:  -0.3709\n",
      "Total loss:  0.5345 | PDE Loss:  -0.5218 | Function Loss:  -0.3809\n",
      "Total loss:  0.5216 | PDE Loss:  -0.5352 | Function Loss:  -0.3904\n",
      "Total loss:  0.5061 | PDE Loss:  -0.5502 | Function Loss:  -0.4091\n",
      "Total loss:  0.4918 | PDE Loss:  -0.5738 | Function Loss:  -0.3617\n",
      "Total loss:  0.4847 | PDE Loss:  -0.5806 | Function Loss:  -0.3702\n",
      "Total loss:  0.478 | PDE Loss:  -0.5866 | Function Loss:  -0.381\n",
      "Total loss:  0.4738 | PDE Loss:  -0.5897 | Function Loss:  -0.3922\n",
      "Total loss:  0.47 | PDE Loss:  -0.5914 | Function Loss:  -0.4099\n",
      "Total loss:  0.4661 | PDE Loss:  -0.5938 | Function Loss:  -0.424\n",
      "Total loss:  0.4579 | PDE Loss:  -0.5994 | Function Loss:  -0.4504\n",
      "Total loss:  0.4422 | PDE Loss:  -0.6135 | Function Loss:  -0.477\n",
      "Total loss:  0.4305 | PDE Loss:  -0.6248 | Function Loss:  -0.4919\n",
      "Total loss:  0.4206 | PDE Loss:  -0.6372 | Function Loss:  -0.4838\n",
      "Total loss:  0.4139 | PDE Loss:  -0.646 | Function Loss:  -0.476\n",
      "Total loss:  0.4094 | PDE Loss:  -0.6581 | Function Loss:  -0.4319\n",
      "Total loss:  0.4053 | PDE Loss:  -0.6634 | Function Loss:  -0.4295\n",
      "Total loss:  0.3999 | PDE Loss:  -0.6701 | Function Loss:  -0.4272\n",
      "Total loss:  0.3939 | PDE Loss:  -0.6788 | Function Loss:  -0.4182\n",
      "Total loss:  0.3885 | PDE Loss:  -0.6854 | Function Loss:  -0.4169\n",
      "Total loss:  0.3844 | PDE Loss:  -0.692 | Function Loss:  -0.4078\n",
      "Total loss:  0.3756 | PDE Loss:  -0.704 | Function Loss:  -0.4006\n",
      "Total loss:  0.3667 | PDE Loss:  -0.7181 | Function Loss:  -0.3842\n",
      "Total loss:  0.3588 | PDE Loss:  -0.7268 | Function Loss:  -0.389\n",
      "Total loss:  0.3536 | PDE Loss:  -0.7339 | Function Loss:  -0.3852\n",
      "Total loss:  0.3471 | PDE Loss:  -0.7399 | Function Loss:  -0.3935\n",
      "Total loss:  0.3415 | PDE Loss:  -0.7455 | Function Loss:  -0.3996\n",
      "Total loss:  0.3352 | PDE Loss:  -0.7529 | Function Loss:  -0.401\n",
      "Total loss:  0.3297 | PDE Loss:  -0.7585 | Function Loss:  -0.4063\n",
      "Total loss:  0.3261 | PDE Loss:  -0.7663 | Function Loss:  -0.3913\n",
      "Total loss:  0.3228 | PDE Loss:  -0.7696 | Function Loss:  -0.3945\n",
      "Total loss:  0.3192 | PDE Loss:  -0.7723 | Function Loss:  -0.4018\n",
      "Total loss:  0.3113 | PDE Loss:  -0.7794 | Function Loss:  -0.4132\n",
      "Total loss:  0.3001 | PDE Loss:  -0.7898 | Function Loss:  -0.4284\n",
      "Total loss:  0.2882 | PDE Loss:  -0.8053 | Function Loss:  -0.4245\n",
      "Total loss:  0.2763 | PDE Loss:  -0.8255 | Function Loss:  -0.4037\n",
      "Total loss:  0.2658 | PDE Loss:  -0.8456 | Function Loss:  -0.3794\n",
      "Total loss:  0.2569 | PDE Loss:  -0.8664 | Function Loss:  -0.3503\n",
      "Total loss:  0.2494 | PDE Loss:  -0.8778 | Function Loss:  -0.346\n",
      "Total loss:  0.2445 | PDE Loss:  -0.8885 | Function Loss:  -0.3341\n",
      "Total loss:  0.2383 | PDE Loss:  -0.8971 | Function Loss:  -0.3339\n",
      "Total loss:  0.23 | PDE Loss:  -0.9136 | Function Loss:  -0.3205\n",
      "Total loss:  0.2232 | PDE Loss:  -0.9271 | Function Loss:  -0.3108\n",
      "Total loss:  0.216 | PDE Loss:  -0.9387 | Function Loss:  -0.3075\n",
      "Total loss:  0.2109 | PDE Loss:  -0.9538 | Function Loss:  -0.29\n",
      "Total loss:  0.2061 | PDE Loss:  -0.9581 | Function Loss:  -0.2957\n",
      "Total loss:  0.2004 | PDE Loss:  -0.9644 | Function Loss:  -0.3001\n",
      "Total loss:  0.1914 | PDE Loss:  -0.9684 | Function Loss:  -0.3203\n",
      "Total loss:  0.1784 | PDE Loss:  -0.968 | Function Loss:  -0.3652\n",
      "Total loss:  0.17 | PDE Loss:  -0.9612 | Function Loss:  -0.4139\n",
      "Total loss:  0.1623 | PDE Loss:  -0.9624 | Function Loss:  -0.4404\n",
      "Total loss:  0.1592 | PDE Loss:  -0.963 | Function Loss:  -0.4514\n",
      "Total loss:  0.1562 | PDE Loss:  -0.9629 | Function Loss:  -0.464\n",
      "Total loss:  0.1519 | PDE Loss:  -0.9635 | Function Loss:  -0.4802\n",
      "Total loss:  0.1459 | PDE Loss:  -0.9635 | Function Loss:  -0.5062\n",
      "Total loss:  0.1382 | PDE Loss:  -0.9652 | Function Loss:  -0.5358\n",
      "Total loss:  0.1282 | PDE Loss:  -0.9696 | Function Loss:  -0.567\n",
      "Total loss:  0.1158 | PDE Loss:  -0.9794 | Function Loss:  -0.5902\n",
      "Total loss:  0.1037 | PDE Loss:  -0.9984 | Function Loss:  -0.5755\n",
      "Total loss:  0.0967 | PDE Loss:  -1.0081 | Function Loss:  -0.5722\n",
      "Total loss:  0.0919 | PDE Loss:  -1.0186 | Function Loss:  -0.5563\n",
      "Total loss:  0.0879 | PDE Loss:  -1.0293 | Function Loss:  -0.538\n",
      "Total loss:  0.0851 | PDE Loss:  -1.0341 | Function Loss:  -0.5345\n",
      "Total loss:  0.083 | PDE Loss:  -1.0411 | Function Loss:  -0.5219\n",
      "Total loss:  0.0796 | PDE Loss:  -1.0467 | Function Loss:  -0.5186\n",
      "Total loss:  0.0671 | PDE Loss:  -1.0672 | Function Loss:  -0.508\n",
      "Total loss:  0.0552 | PDE Loss:  -1.0836 | Function Loss:  -0.5077\n",
      "Total loss:  0.045 | PDE Loss:  -1.1042 | Function Loss:  -0.4913\n",
      "Total loss:  0.0373 | PDE Loss:  -1.1116 | Function Loss:  -0.5\n",
      "Total loss:  0.0294 | PDE Loss:  -1.1216 | Function Loss:  -0.5028\n",
      "Total loss:  0.0186 | PDE Loss:  -1.1343 | Function Loss:  -0.5092\n",
      "Total loss:  0.0094 | PDE Loss:  -1.1447 | Function Loss:  -0.5152\n",
      "Total loss:  0.0035 | PDE Loss:  -1.1555 | Function Loss:  -0.5101\n",
      "Total loss:  -0.0009 | PDE Loss:  -1.1619 | Function Loss:  -0.5099\n",
      "Total loss:  -0.0077 | PDE Loss:  -1.1713 | Function Loss:  -0.5109\n",
      "Total loss:  -0.0198 | PDE Loss:  -1.1877 | Function Loss:  -0.5138\n",
      "Total loss:  -0.034 | PDE Loss:  -1.2073 | Function Loss:  -0.5168\n",
      "Total loss:  -0.0449 | PDE Loss:  -1.221 | Function Loss:  -0.522\n",
      "Total loss:  -0.0527 | PDE Loss:  -1.2284 | Function Loss:  -0.5305\n",
      "Total loss:  -0.0592 | PDE Loss:  -1.2361 | Function Loss:  -0.5348\n",
      "Total loss:  -0.0628 | PDE Loss:  -1.23 | Function Loss:  -0.5582\n",
      "Total loss:  -0.0681 | PDE Loss:  -1.2339 | Function Loss:  -0.5664\n",
      "Total loss:  -0.072 | PDE Loss:  -1.2346 | Function Loss:  -0.5774\n",
      "Total loss:  -0.075 | PDE Loss:  -1.233 | Function Loss:  -0.5909\n",
      "Total loss:  -0.0768 | PDE Loss:  -1.2331 | Function Loss:  -0.5964\n",
      "Total loss:  -0.0792 | PDE Loss:  -1.2337 | Function Loss:  -0.603\n",
      "Total loss:  -0.0829 | PDE Loss:  -1.2355 | Function Loss:  -0.6111\n",
      "Total loss:  -0.0869 | PDE Loss:  -1.2384 | Function Loss:  -0.6178\n",
      "Total loss:  -0.0934 | PDE Loss:  -1.2429 | Function Loss:  -0.629\n",
      "Total loss:  -0.1006 | PDE Loss:  -1.2484 | Function Loss:  -0.6407\n",
      "Total loss:  -0.1081 | PDE Loss:  -1.2539 | Function Loss:  -0.6528\n",
      "Total loss:  -0.1162 | PDE Loss:  -1.2608 | Function Loss:  -0.664\n",
      "Total loss:  -0.1263 | PDE Loss:  -1.2692 | Function Loss:  -0.6785\n",
      "Total loss:  -0.1359 | PDE Loss:  -1.2772 | Function Loss:  -0.6923\n",
      "Total loss:  -0.1421 | PDE Loss:  -1.2843 | Function Loss:  -0.6961\n",
      "Total loss:  -0.1493 | PDE Loss:  -1.29 | Function Loss:  -0.7074\n",
      "Total loss:  -0.1546 | PDE Loss:  -1.2949 | Function Loss:  -0.7135\n",
      "Total loss:  -0.1617 | PDE Loss:  -1.3003 | Function Loss:  -0.7253\n",
      "Total loss:  -0.1673 | PDE Loss:  -1.3051 | Function Loss:  -0.7329\n",
      "Total loss:  -0.172 | PDE Loss:  -1.3084 | Function Loss:  -0.7412\n",
      "Total loss:  -0.177 | PDE Loss:  -1.3128 | Function Loss:  -0.7481\n",
      "Total loss:  -0.1814 | PDE Loss:  -1.316 | Function Loss:  -0.7556\n",
      "Total loss:  -0.1844 | PDE Loss:  -1.3211 | Function Loss:  -0.7531\n",
      "Total loss:  -0.1867 | PDE Loss:  -1.323 | Function Loss:  -0.7563\n",
      "Total loss:  -0.1892 | PDE Loss:  -1.3266 | Function Loss:  -0.7559\n",
      "Total loss:  -0.1922 | PDE Loss:  -1.331 | Function Loss:  -0.7551\n",
      "Total loss:  -0.1951 | PDE Loss:  -1.3349 | Function Loss:  -0.7555\n",
      "Total loss:  -0.1985 | PDE Loss:  -1.3395 | Function Loss:  -0.7556\n",
      "Total loss:  -0.203 | PDE Loss:  -1.3451 | Function Loss:  -0.7574\n",
      "Total loss:  -0.2096 | PDE Loss:  -1.353 | Function Loss:  -0.7606\n",
      "Total loss:  -0.2166 | PDE Loss:  -1.364 | Function Loss:  -0.7573\n",
      "Total loss:  -0.221 | PDE Loss:  -1.3688 | Function Loss:  -0.761\n",
      "Total loss:  -0.2252 | PDE Loss:  -1.3746 | Function Loss:  -0.7611\n",
      "Total loss:  -0.2293 | PDE Loss:  -1.3784 | Function Loss:  -0.7661\n",
      "Total loss:  -0.2324 | PDE Loss:  -1.3809 | Function Loss:  -0.7707\n",
      "Total loss:  -0.2357 | PDE Loss:  -1.3829 | Function Loss:  -0.7771\n",
      "Total loss:  -0.2401 | PDE Loss:  -1.3841 | Function Loss:  -0.7895\n",
      "Total loss:  -0.2447 | PDE Loss:  -1.3857 | Function Loss:  -0.8018\n",
      "Total loss:  -0.25 | PDE Loss:  -1.3866 | Function Loss:  -0.8189\n",
      "Total loss:  -0.2553 | PDE Loss:  -1.3893 | Function Loss:  -0.8314\n",
      "Total loss:  -0.2606 | PDE Loss:  -1.3922 | Function Loss:  -0.8432\n",
      "Total loss:  -0.2652 | PDE Loss:  -1.397 | Function Loss:  -0.8472\n",
      "Total loss:  -0.2712 | PDE Loss:  -1.4029 | Function Loss:  -0.8535\n",
      "Total loss:  -0.277 | PDE Loss:  -1.4104 | Function Loss:  -0.8547\n",
      "Total loss:  -0.2821 | PDE Loss:  -1.4161 | Function Loss:  -0.858\n",
      "Total loss:  -0.2855 | PDE Loss:  -1.4195 | Function Loss:  -0.8615\n",
      "Total loss:  -0.2883 | PDE Loss:  -1.4229 | Function Loss:  -0.8627\n",
      "Total loss:  -0.2913 | PDE Loss:  -1.4258 | Function Loss:  -0.8659\n",
      "Total loss:  -0.2935 | PDE Loss:  -1.4276 | Function Loss:  -0.8693\n",
      "Total loss:  -0.2973 | PDE Loss:  -1.4326 | Function Loss:  -0.8699\n",
      "Total loss:  -0.3048 | PDE Loss:  -1.443 | Function Loss:  -0.8695\n",
      "Total loss:  -0.3122 | PDE Loss:  -1.4552 | Function Loss:  -0.8643\n",
      "Total loss:  -0.3176 | PDE Loss:  -1.4656 | Function Loss:  -0.857\n",
      "Total loss:  -0.3221 | PDE Loss:  -1.4758 | Function Loss:  -0.8478\n",
      "Total loss:  -0.3264 | PDE Loss:  -1.483 | Function Loss:  -0.8453\n",
      "Total loss:  -0.3302 | PDE Loss:  -1.4907 | Function Loss:  -0.8404\n",
      "Total loss:  -0.3346 | PDE Loss:  -1.4981 | Function Loss:  -0.8381\n",
      "Total loss:  -0.3412 | PDE Loss:  -1.5089 | Function Loss:  -0.8356\n",
      "Total loss:  -0.3499 | PDE Loss:  -1.5244 | Function Loss:  -0.83\n",
      "Total loss:  -0.3566 | PDE Loss:  -1.5378 | Function Loss:  -0.8237\n",
      "Total loss:  -0.3637 | PDE Loss:  -1.5487 | Function Loss:  -0.8233\n",
      "Total loss:  -0.3688 | PDE Loss:  -1.5648 | Function Loss:  -0.8087\n",
      "Total loss:  -0.3716 | PDE Loss:  -1.5649 | Function Loss:  -0.8162\n",
      "Total loss:  -0.3735 | PDE Loss:  -1.5673 | Function Loss:  -0.8173\n",
      "Total loss:  -0.3755 | PDE Loss:  -1.5677 | Function Loss:  -0.8222\n",
      "Total loss:  -0.3801 | PDE Loss:  -1.5716 | Function Loss:  -0.8281\n",
      "Total loss:  -0.3901 | PDE Loss:  -1.582 | Function Loss:  -0.8374\n",
      "Total loss:  -0.3988 | PDE Loss:  -1.5903 | Function Loss:  -0.8467\n",
      "Total loss:  -0.4055 | PDE Loss:  -1.5981 | Function Loss:  -0.8515\n",
      "Total loss:  -0.4104 | PDE Loss:  -1.6027 | Function Loss:  -0.857\n",
      "Total loss:  -0.4174 | PDE Loss:  -1.6101 | Function Loss:  -0.8631\n",
      "Total loss:  -0.4251 | PDE Loss:  -1.6197 | Function Loss:  -0.8675\n",
      "Total loss:  -0.431 | PDE Loss:  -1.6265 | Function Loss:  -0.8717\n",
      "Total loss:  -0.4382 | PDE Loss:  -1.6381 | Function Loss:  -0.8713\n",
      "Total loss:  -0.4462 | PDE Loss:  -1.6525 | Function Loss:  -0.8686\n",
      "Total loss:  -0.4545 | PDE Loss:  -1.6624 | Function Loss:  -0.8742\n",
      "Total loss:  -0.4643 | PDE Loss:  -1.678 | Function Loss:  -0.8747\n",
      "Total loss:  -0.4747 | PDE Loss:  -1.6846 | Function Loss:  -0.8912\n",
      "Total loss:  -0.4806 | PDE Loss:  -1.6918 | Function Loss:  -0.8951\n",
      "Total loss:  -0.4832 | PDE Loss:  -1.6926 | Function Loss:  -0.9004\n",
      "Total loss:  -0.4842 | PDE Loss:  -1.6916 | Function Loss:  -0.9047\n",
      "Total loss:  -0.4852 | PDE Loss:  -1.6903 | Function Loss:  -0.9096\n",
      "Total loss:  -0.486 | PDE Loss:  -1.6918 | Function Loss:  -0.9093\n",
      "Total loss:  -0.488 | PDE Loss:  -1.694 | Function Loss:  -0.9109\n",
      "Total loss:  -0.4919 | PDE Loss:  -1.6989 | Function Loss:  -0.913\n",
      "Total loss:  -0.4977 | PDE Loss:  -1.7108 | Function Loss:  -0.9091\n",
      "Total loss:  -0.5064 | PDE Loss:  -1.7276 | Function Loss:  -0.9053\n",
      "Total loss:  -0.5175 | PDE Loss:  -1.754 | Function Loss:  -0.8945\n",
      "Total loss:  -0.5293 | PDE Loss:  -1.7765 | Function Loss:  -0.8918\n",
      "Total loss:  -0.5408 | PDE Loss:  -1.8048 | Function Loss:  -0.8824\n",
      "Total loss:  -0.5488 | PDE Loss:  -1.8129 | Function Loss:  -0.8902\n",
      "Total loss:  -0.5532 | PDE Loss:  -1.8185 | Function Loss:  -0.8931\n",
      "Total loss:  -0.558 | PDE Loss:  -1.8238 | Function Loss:  -0.8973\n",
      "Total loss:  -0.5629 | PDE Loss:  -1.8231 | Function Loss:  -0.909\n",
      "Total loss:  -0.5674 | PDE Loss:  -1.8299 | Function Loss:  -0.9106\n",
      "Total loss:  -0.5756 | PDE Loss:  -1.842 | Function Loss:  -0.9142\n",
      "Total loss:  -0.5862 | PDE Loss:  -1.8554 | Function Loss:  -0.9216\n",
      "Total loss:  -0.5936 | PDE Loss:  -1.8695 | Function Loss:  -0.9213\n",
      "Total loss:  -0.5982 | PDE Loss:  -1.885 | Function Loss:  -0.9139\n",
      "Total loss:  -0.6012 | PDE Loss:  -1.8893 | Function Loss:  -0.9156\n",
      "Total loss:  -0.6029 | PDE Loss:  -1.8966 | Function Loss:  -0.9114\n",
      "Total loss:  -0.6048 | PDE Loss:  -1.8993 | Function Loss:  -0.9124\n",
      "Total loss:  -0.6063 | PDE Loss:  -1.9002 | Function Loss:  -0.9146\n",
      "Total loss:  -0.6079 | PDE Loss:  -1.9001 | Function Loss:  -0.9179\n",
      "Total loss:  -0.6099 | PDE Loss:  -1.897 | Function Loss:  -0.9253\n",
      "Total loss:  -0.6125 | PDE Loss:  -1.895 | Function Loss:  -0.933\n",
      "Total loss:  -0.6161 | PDE Loss:  -1.8908 | Function Loss:  -0.9453\n",
      "Total loss:  -0.6214 | PDE Loss:  -1.8876 | Function Loss:  -0.9603\n",
      "Total loss:  -0.6272 | PDE Loss:  -1.8846 | Function Loss:  -0.9768\n",
      "Total loss:  -0.6327 | PDE Loss:  -1.8856 | Function Loss:  -0.9877\n",
      "Total loss:  -0.6371 | PDE Loss:  -1.8916 | Function Loss:  -0.9903\n",
      "Total loss:  -0.6418 | PDE Loss:  -1.8959 | Function Loss:  -0.9955\n",
      "Total loss:  -0.6453 | PDE Loss:  -1.9034 | Function Loss:  -0.994\n",
      "Total loss:  -0.6491 | PDE Loss:  -1.9129 | Function Loss:  -0.9909\n",
      "Total loss:  -0.6546 | PDE Loss:  -1.9243 | Function Loss:  -0.9894\n",
      "Total loss:  -0.6602 | PDE Loss:  -1.9361 | Function Loss:  -0.988\n",
      "Total loss:  -0.6651 | PDE Loss:  -1.9461 | Function Loss:  -0.9871\n",
      "Total loss:  -0.6685 | PDE Loss:  -1.9463 | Function Loss:  -0.9941\n",
      "Total loss:  -0.6714 | PDE Loss:  -1.9516 | Function Loss:  -0.9944\n",
      "Total loss:  -0.6757 | PDE Loss:  -1.9522 | Function Loss:  -1.0026\n",
      "Total loss:  -0.681 | PDE Loss:  -1.9561 | Function Loss:  -1.0097\n",
      "Total loss:  -0.6876 | PDE Loss:  -1.9661 | Function Loss:  -1.0123\n",
      "Total loss:  -0.6962 | PDE Loss:  -1.979 | Function Loss:  -1.0163\n",
      "Total loss:  -0.7081 | PDE Loss:  -1.9976 | Function Loss:  -1.021\n",
      "Total loss:  -0.7299 | PDE Loss:  -2.0548 | Function Loss:  -1.0083\n",
      "Total loss:  -0.7463 | PDE Loss:  -2.0592 | Function Loss:  -1.0357\n",
      "Total loss:  -0.7528 | PDE Loss:  -2.1021 | Function Loss:  -1.0103\n",
      "Total loss:  -0.7523 | PDE Loss:  -2.1251 | Function Loss:  -0.9918\n",
      "Total loss:  -0.759 | PDE Loss:  -2.1238 | Function Loss:  -1.0045\n",
      "Total loss:  -0.7668 | PDE Loss:  -2.121 | Function Loss:  -1.0204\n",
      "Total loss:  -0.7696 | PDE Loss:  -2.1249 | Function Loss:  -1.0225\n",
      "Total loss:  -0.7721 | PDE Loss:  -2.1272 | Function Loss:  -1.025\n",
      "Total loss:  -0.7748 | PDE Loss:  -2.1362 | Function Loss:  -1.0229\n",
      "Total loss:  -0.7781 | PDE Loss:  -2.1461 | Function Loss:  -1.0211\n",
      "Total loss:  -0.7811 | PDE Loss:  -2.1524 | Function Loss:  -1.0217\n",
      "Total loss:  -0.7839 | PDE Loss:  -2.1573 | Function Loss:  -1.0229\n",
      "Total loss:  -0.7882 | PDE Loss:  -2.1621 | Function Loss:  -1.0268\n",
      "Total loss:  -0.7948 | PDE Loss:  -2.1709 | Function Loss:  -1.0318\n",
      "Total loss:  -0.8038 | PDE Loss:  -2.1735 | Function Loss:  -1.0456\n",
      "Total loss:  -0.8133 | PDE Loss:  -2.1833 | Function Loss:  -1.0548\n",
      "Total loss:  -0.8246 | PDE Loss:  -2.1895 | Function Loss:  -1.0699\n",
      "Total loss:  -0.8326 | PDE Loss:  -2.1875 | Function Loss:  -1.0858\n",
      "Total loss:  -0.8367 | PDE Loss:  -2.1961 | Function Loss:  -1.0863\n",
      "Total loss:  -0.8404 | PDE Loss:  -2.1874 | Function Loss:  -1.0998\n",
      "Total loss:  -0.8437 | PDE Loss:  -2.1912 | Function Loss:  -1.1027\n",
      "Total loss:  -0.8487 | PDE Loss:  -2.2027 | Function Loss:  -1.1025\n",
      "Total loss:  -0.855 | PDE Loss:  -2.2038 | Function Loss:  -1.113\n",
      "Total loss:  -0.8609 | PDE Loss:  -2.2192 | Function Loss:  -1.1114\n",
      "Total loss:  -0.8665 | PDE Loss:  -2.2177 | Function Loss:  -1.1226\n",
      "Total loss:  -0.8711 | PDE Loss:  -2.2201 | Function Loss:  -1.129\n",
      "Total loss:  -0.8752 | PDE Loss:  -2.2179 | Function Loss:  -1.1382\n",
      "Total loss:  -0.8781 | PDE Loss:  -2.2211 | Function Loss:  -1.1408\n",
      "Total loss:  -0.8806 | PDE Loss:  -2.2225 | Function Loss:  -1.1444\n",
      "Total loss:  -0.8834 | PDE Loss:  -2.2237 | Function Loss:  -1.1485\n",
      "Total loss:  -0.8909 | PDE Loss:  -2.2328 | Function Loss:  -1.1546\n",
      "Total loss:  -0.9017 | PDE Loss:  -2.2373 | Function Loss:  -1.1706\n",
      "Total loss:  -0.911 | PDE Loss:  -2.2524 | Function Loss:  -1.1751\n",
      "Total loss:  -0.917 | PDE Loss:  -2.2555 | Function Loss:  -1.1836\n",
      "Total loss:  -0.9232 | PDE Loss:  -2.2662 | Function Loss:  -1.186\n",
      "Total loss:  -0.926 | PDE Loss:  -2.2687 | Function Loss:  -1.1891\n",
      "Total loss:  -0.9272 | PDE Loss:  -2.2696 | Function Loss:  -1.1905\n",
      "Total loss:  -0.928 | PDE Loss:  -2.2688 | Function Loss:  -1.1926\n",
      "Total loss:  -0.9287 | PDE Loss:  -2.2696 | Function Loss:  -1.1932\n",
      "Total loss:  -0.9294 | PDE Loss:  -2.2694 | Function Loss:  -1.1948\n",
      "Total loss:  -0.9307 | PDE Loss:  -2.2711 | Function Loss:  -1.1957\n",
      "Total loss:  -0.9334 | PDE Loss:  -2.2734 | Function Loss:  -1.1987\n",
      "Total loss:  -0.9378 | PDE Loss:  -2.278 | Function Loss:  -1.2029\n",
      "Total loss:  -0.9436 | PDE Loss:  -2.2872 | Function Loss:  -1.2058\n",
      "Total loss:  -0.9494 | PDE Loss:  -2.2958 | Function Loss:  -1.2094\n",
      "Total loss:  -0.9555 | PDE Loss:  -2.3088 | Function Loss:  -1.2099\n",
      "Total loss:  -0.9601 | PDE Loss:  -2.3177 | Function Loss:  -1.211\n",
      "Total loss:  -0.9635 | PDE Loss:  -2.3221 | Function Loss:  -1.2138\n",
      "Total loss:  -0.9656 | PDE Loss:  -2.3202 | Function Loss:  -1.219\n",
      "Total loss:  -0.9677 | PDE Loss:  -2.3258 | Function Loss:  -1.2183\n",
      "Total loss:  -0.9703 | PDE Loss:  -2.3253 | Function Loss:  -1.2232\n",
      "Total loss:  -0.9743 | PDE Loss:  -2.3291 | Function Loss:  -1.2275\n",
      "Total loss:  -0.9809 | PDE Loss:  -2.3384 | Function Loss:  -1.232\n",
      "Total loss:  -0.9899 | PDE Loss:  -2.3504 | Function Loss:  -1.2387\n",
      "Total loss:  -1.0013 | PDE Loss:  -2.3737 | Function Loss:  -1.241\n",
      "Total loss:  -1.0126 | PDE Loss:  -2.4039 | Function Loss:  -1.239\n",
      "Total loss:  -1.021 | PDE Loss:  -2.4264 | Function Loss:  -1.238\n",
      "Total loss:  -1.027 | PDE Loss:  -2.4437 | Function Loss:  -1.2368\n",
      "Total loss:  -1.0314 | PDE Loss:  -2.446 | Function Loss:  -1.2425\n",
      "Total loss:  -1.0348 | PDE Loss:  -2.4512 | Function Loss:  -1.2448\n",
      "Total loss:  -1.0373 | PDE Loss:  -2.4504 | Function Loss:  -1.2493\n",
      "Total loss:  -1.0392 | PDE Loss:  -2.4501 | Function Loss:  -1.2526\n",
      "Total loss:  -1.0407 | PDE Loss:  -2.4547 | Function Loss:  -1.2522\n",
      "Total loss:  -1.0423 | PDE Loss:  -2.4533 | Function Loss:  -1.2556\n",
      "Total loss:  -1.0436 | PDE Loss:  -2.4589 | Function Loss:  -1.2543\n",
      "Total loss:  -1.0448 | PDE Loss:  -2.4608 | Function Loss:  -1.255\n",
      "Total loss:  -1.0464 | PDE Loss:  -2.4668 | Function Loss:  -1.2539\n",
      "Total loss:  -1.0479 | PDE Loss:  -2.4704 | Function Loss:  -1.2541\n",
      "Total loss:  -1.0499 | PDE Loss:  -2.4729 | Function Loss:  -1.2559\n",
      "Total loss:  -1.053 | PDE Loss:  -2.4818 | Function Loss:  -1.2555\n",
      "Total loss:  -1.0569 | PDE Loss:  -2.4834 | Function Loss:  -1.2608\n",
      "Total loss:  -1.062 | PDE Loss:  -2.4891 | Function Loss:  -1.2655\n",
      "Total loss:  -1.0686 | PDE Loss:  -2.4906 | Function Loss:  -1.2751\n",
      "Total loss:  -1.0748 | PDE Loss:  -2.4949 | Function Loss:  -1.2824\n",
      "Total loss:  -1.0783 | PDE Loss:  -2.4966 | Function Loss:  -1.2871\n",
      "Total loss:  -1.0806 | PDE Loss:  -2.4969 | Function Loss:  -1.2906\n",
      "Total loss:  -1.0829 | PDE Loss:  -2.5014 | Function Loss:  -1.2915\n",
      "Total loss:  -1.0853 | PDE Loss:  -2.5012 | Function Loss:  -1.2955\n",
      "Total loss:  -1.0879 | PDE Loss:  -2.5068 | Function Loss:  -1.2963\n",
      "Total loss:  -1.0924 | PDE Loss:  -2.5086 | Function Loss:  -1.3024\n",
      "Total loss:  -1.0988 | PDE Loss:  -2.5193 | Function Loss:  -1.3062\n",
      "Total loss:  -1.1075 | PDE Loss:  -2.5305 | Function Loss:  -1.3134\n",
      "Total loss:  -1.1162 | PDE Loss:  -2.5415 | Function Loss:  -1.3207\n",
      "Total loss:  -1.1217 | PDE Loss:  -2.555 | Function Loss:  -1.3215\n",
      "Total loss:  -1.1248 | PDE Loss:  -2.5612 | Function Loss:  -1.3228\n",
      "Total loss:  -1.1276 | PDE Loss:  -2.5651 | Function Loss:  -1.325\n",
      "Total loss:  -1.1297 | PDE Loss:  -2.572 | Function Loss:  -1.3244\n",
      "Total loss:  -1.1317 | PDE Loss:  -2.5772 | Function Loss:  -1.3245\n",
      "Total loss:  -1.1347 | PDE Loss:  -2.5885 | Function Loss:  -1.3229\n",
      "Total loss:  -1.1388 | PDE Loss:  -2.6006 | Function Loss:  -1.3228\n",
      "Total loss:  -1.1425 | PDE Loss:  -2.6189 | Function Loss:  -1.3189\n",
      "Total loss:  -1.1453 | PDE Loss:  -2.62 | Function Loss:  -1.3227\n",
      "Total loss:  -1.1472 | PDE Loss:  -2.6283 | Function Loss:  -1.3213\n",
      "Total loss:  -1.1483 | PDE Loss:  -2.6312 | Function Loss:  -1.3215\n",
      "Total loss:  -1.1493 | PDE Loss:  -2.6261 | Function Loss:  -1.3255\n",
      "Total loss:  -1.1502 | PDE Loss:  -2.6282 | Function Loss:  -1.3258\n",
      "Total loss:  -1.1514 | PDE Loss:  -2.6279 | Function Loss:  -1.3278\n",
      "Total loss:  -1.1534 | PDE Loss:  -2.6262 | Function Loss:  -1.3316\n",
      "Total loss:  -1.1565 | PDE Loss:  -2.627 | Function Loss:  -1.3359\n",
      "Total loss:  -1.16 | PDE Loss:  -2.6283 | Function Loss:  -1.3405\n",
      "Total loss:  -1.1627 | PDE Loss:  -2.6303 | Function Loss:  -1.3436\n",
      "Total loss:  -1.165 | PDE Loss:  -2.636 | Function Loss:  -1.3443\n",
      "Total loss:  -1.1673 | PDE Loss:  -2.6419 | Function Loss:  -1.3447\n",
      "Total loss:  -1.1703 | PDE Loss:  -2.6484 | Function Loss:  -1.3458\n",
      "Total loss:  -1.174 | PDE Loss:  -2.6591 | Function Loss:  -1.3462\n",
      "Total loss:  -1.1783 | PDE Loss:  -2.6642 | Function Loss:  -1.3501\n",
      "Total loss:  -1.1826 | PDE Loss:  -2.676 | Function Loss:  -1.3508\n",
      "Total loss:  -1.1859 | PDE Loss:  -2.6805 | Function Loss:  -1.3535\n",
      "Total loss:  -1.1881 | PDE Loss:  -2.6859 | Function Loss:  -1.3542\n",
      "Total loss:  -1.19 | PDE Loss:  -2.6905 | Function Loss:  -1.3548\n",
      "Total loss:  -1.192 | PDE Loss:  -2.6938 | Function Loss:  -1.3563\n",
      "Total loss:  -1.1949 | PDE Loss:  -2.6977 | Function Loss:  -1.3587\n",
      "Total loss:  -1.1983 | PDE Loss:  -2.7057 | Function Loss:  -1.3601\n",
      "Total loss:  -1.2024 | PDE Loss:  -2.7054 | Function Loss:  -1.3661\n",
      "Total loss:  -1.2072 | PDE Loss:  -2.7113 | Function Loss:  -1.3704\n",
      "Total loss:  -1.2117 | PDE Loss:  -2.7134 | Function Loss:  -1.376\n",
      "Total loss:  -1.216 | PDE Loss:  -2.7168 | Function Loss:  -1.3808\n",
      "Total loss:  -1.2191 | PDE Loss:  -2.7211 | Function Loss:  -1.3833\n",
      "Total loss:  -1.2208 | PDE Loss:  -2.7313 | Function Loss:  -1.3812\n",
      "Total loss:  -1.2217 | PDE Loss:  -2.7324 | Function Loss:  -1.3819\n",
      "Total loss:  -1.2225 | PDE Loss:  -2.7348 | Function Loss:  -1.382\n",
      "Total loss:  -1.2235 | PDE Loss:  -2.7357 | Function Loss:  -1.3831\n",
      "Total loss:  -1.2247 | PDE Loss:  -2.7344 | Function Loss:  -1.3854\n",
      "Total loss:  -1.2258 | PDE Loss:  -2.7323 | Function Loss:  -1.3879\n",
      "Total loss:  -1.2272 | PDE Loss:  -2.7304 | Function Loss:  -1.3908\n",
      "Total loss:  -1.2291 | PDE Loss:  -2.7249 | Function Loss:  -1.3962\n",
      "Total loss:  -1.2318 | PDE Loss:  -2.7234 | Function Loss:  -1.4008\n",
      "Total loss:  -1.236 | PDE Loss:  -2.7205 | Function Loss:  -1.4084\n",
      "Total loss:  -1.2411 | PDE Loss:  -2.7244 | Function Loss:  -1.4141\n",
      "Total loss:  -1.2465 | PDE Loss:  -2.7279 | Function Loss:  -1.4204\n",
      "Total loss:  -1.2508 | PDE Loss:  -2.7373 | Function Loss:  -1.4222\n",
      "Total loss:  -1.2538 | PDE Loss:  -2.7476 | Function Loss:  -1.4217\n",
      "Total loss:  -1.2557 | PDE Loss:  -2.7567 | Function Loss:  -1.4203\n",
      "Total loss:  -1.2568 | PDE Loss:  -2.765 | Function Loss:  -1.4182\n",
      "Total loss:  -1.2575 | PDE Loss:  -2.7706 | Function Loss:  -1.4166\n",
      "Total loss:  -1.258 | PDE Loss:  -2.7746 | Function Loss:  -1.4156\n",
      "Total loss:  -1.2589 | PDE Loss:  -2.7804 | Function Loss:  -1.4144\n",
      "Total loss:  -1.2606 | PDE Loss:  -2.7879 | Function Loss:  -1.4136\n",
      "Total loss:  -1.2632 | PDE Loss:  -2.7982 | Function Loss:  -1.413\n",
      "Total loss:  -1.2665 | PDE Loss:  -2.8059 | Function Loss:  -1.4145\n",
      "Total loss:  -1.2703 | PDE Loss:  -2.8126 | Function Loss:  -1.4171\n",
      "Total loss:  -1.274 | PDE Loss:  -2.8135 | Function Loss:  -1.422\n",
      "Total loss:  -1.2781 | PDE Loss:  -2.8107 | Function Loss:  -1.4289\n",
      "Total loss:  -1.2818 | PDE Loss:  -2.8018 | Function Loss:  -1.4379\n",
      "Total loss:  -1.2845 | PDE Loss:  -2.7957 | Function Loss:  -1.4445\n",
      "Total loss:  -1.2862 | PDE Loss:  -2.7891 | Function Loss:  -1.45\n",
      "Total loss:  -1.2878 | PDE Loss:  -2.7833 | Function Loss:  -1.4549\n",
      "Total loss:  -1.2891 | PDE Loss:  -2.7803 | Function Loss:  -1.4582\n",
      "Total loss:  -1.2909 | PDE Loss:  -2.7757 | Function Loss:  -1.4632\n",
      "Total loss:  -1.2929 | PDE Loss:  -2.7766 | Function Loss:  -1.4657\n",
      "Total loss:  -1.2948 | PDE Loss:  -2.7738 | Function Loss:  -1.4699\n",
      "Total loss:  -1.2967 | PDE Loss:  -2.7766 | Function Loss:  -1.4715\n",
      "Total loss:  -1.2988 | PDE Loss:  -2.7777 | Function Loss:  -1.474\n",
      "Total loss:  -1.3008 | PDE Loss:  -2.7797 | Function Loss:  -1.476\n",
      "Total loss:  -1.3026 | PDE Loss:  -2.7852 | Function Loss:  -1.4761\n",
      "Total loss:  -1.3041 | PDE Loss:  -2.7836 | Function Loss:  -1.479\n",
      "Total loss:  -1.3051 | PDE Loss:  -2.7861 | Function Loss:  -1.4793\n",
      "Total loss:  -1.3062 | PDE Loss:  -2.7866 | Function Loss:  -1.4807\n",
      "Total loss:  -1.3073 | PDE Loss:  -2.788 | Function Loss:  -1.4816\n",
      "Total loss:  -1.3084 | PDE Loss:  -2.7879 | Function Loss:  -1.4834\n",
      "Total loss:  -1.3096 | PDE Loss:  -2.7874 | Function Loss:  -1.4854\n",
      "Total loss:  -1.3111 | PDE Loss:  -2.7884 | Function Loss:  -1.4871\n",
      "Total loss:  -1.3131 | PDE Loss:  -2.7883 | Function Loss:  -1.4901\n",
      "Total loss:  -1.3158 | PDE Loss:  -2.7908 | Function Loss:  -1.4929\n",
      "Total loss:  -1.3195 | PDE Loss:  -2.7934 | Function Loss:  -1.4972\n",
      "Total loss:  -1.3238 | PDE Loss:  -2.8009 | Function Loss:  -1.4998\n",
      "Total loss:  -1.3275 | PDE Loss:  -2.8084 | Function Loss:  -1.5017\n",
      "Total loss:  -1.3299 | PDE Loss:  -2.8132 | Function Loss:  -1.503\n",
      "Total loss:  -1.3321 | PDE Loss:  -2.8193 | Function Loss:  -1.5032\n",
      "Total loss:  -1.3336 | PDE Loss:  -2.8219 | Function Loss:  -1.5042\n",
      "Total loss:  -1.3347 | PDE Loss:  -2.8259 | Function Loss:  -1.5039\n",
      "Total loss:  -1.3355 | PDE Loss:  -2.8276 | Function Loss:  -1.5043\n",
      "Total loss:  -1.3366 | PDE Loss:  -2.8314 | Function Loss:  -1.5042\n",
      "Total loss:  -1.3392 | PDE Loss:  -2.836 | Function Loss:  -1.5058\n",
      "Total loss:  -1.3428 | PDE Loss:  -2.8485 | Function Loss:  -1.5052\n",
      "Total loss:  -1.3464 | PDE Loss:  -2.8584 | Function Loss:  -1.506\n",
      "Total loss:  -1.3517 | PDE Loss:  -2.8688 | Function Loss:  -1.5091\n",
      "Total loss:  -1.357 | PDE Loss:  -2.8843 | Function Loss:  -1.5101\n",
      "Total loss:  -1.3611 | PDE Loss:  -2.8877 | Function Loss:  -1.5144\n",
      "Total loss:  -1.3638 | PDE Loss:  -2.888 | Function Loss:  -1.5181\n",
      "Total loss:  -1.3662 | PDE Loss:  -2.8911 | Function Loss:  -1.5202\n",
      "Total loss:  -1.3676 | PDE Loss:  -2.8925 | Function Loss:  -1.5216\n",
      "Total loss:  -1.369 | PDE Loss:  -2.8893 | Function Loss:  -1.525\n",
      "Total loss:  -1.3699 | PDE Loss:  -2.8918 | Function Loss:  -1.5252\n",
      "Total loss:  -1.3709 | PDE Loss:  -2.8961 | Function Loss:  -1.5248\n",
      "Total loss:  -1.3716 | PDE Loss:  -2.8982 | Function Loss:  -1.5249\n",
      "Total loss:  -1.372 | PDE Loss:  -2.9005 | Function Loss:  -1.5245\n",
      "Total loss:  -1.3723 | PDE Loss:  -2.9024 | Function Loss:  -1.5242\n",
      "Total loss:  -1.3727 | PDE Loss:  -2.9037 | Function Loss:  -1.5241\n",
      "Total loss:  -1.3733 | PDE Loss:  -2.905 | Function Loss:  -1.5244\n",
      "Total loss:  -1.3744 | PDE Loss:  -2.9099 | Function Loss:  -1.524\n",
      "Total loss:  -1.3761 | PDE Loss:  -2.9099 | Function Loss:  -1.5264\n",
      "Total loss:  -1.3787 | PDE Loss:  -2.9185 | Function Loss:  -1.5265\n",
      "Total loss:  -1.383 | PDE Loss:  -2.9218 | Function Loss:  -1.5312\n",
      "Total loss:  -1.3889 | PDE Loss:  -2.9329 | Function Loss:  -1.535\n",
      "Total loss:  -1.3948 | PDE Loss:  -2.9416 | Function Loss:  -1.5398\n",
      "Total loss:  -1.4012 | PDE Loss:  -2.9369 | Function Loss:  -1.5508\n",
      "Total loss:  -1.4071 | PDE Loss:  -2.9473 | Function Loss:  -1.5548\n",
      "Total loss:  -1.411 | PDE Loss:  -2.9471 | Function Loss:  -1.5603\n",
      "Total loss:  -1.4132 | PDE Loss:  -2.9497 | Function Loss:  -1.5624\n",
      "Total loss:  -1.4143 | PDE Loss:  -2.9534 | Function Loss:  -1.5624\n",
      "Total loss:  -1.4147 | PDE Loss:  -2.9528 | Function Loss:  -1.5633\n",
      "Total loss:  -1.4152 | PDE Loss:  -2.9545 | Function Loss:  -1.5633\n",
      "Total loss:  -1.4157 | PDE Loss:  -2.9561 | Function Loss:  -1.5633\n",
      "Total loss:  -1.4162 | PDE Loss:  -2.9578 | Function Loss:  -1.5633\n",
      "Total loss:  -1.4166 | PDE Loss:  -2.9598 | Function Loss:  -1.5631\n",
      "Total loss:  -1.4175 | PDE Loss:  -2.9619 | Function Loss:  -1.5635\n",
      "Total loss:  -1.4192 | PDE Loss:  -2.9657 | Function Loss:  -1.5643\n",
      "Total loss:  -1.4213 | PDE Loss:  -2.9713 | Function Loss:  -1.565\n",
      "Total loss:  -1.424 | PDE Loss:  -2.9751 | Function Loss:  -1.5673\n",
      "Total loss:  -1.4268 | PDE Loss:  -2.9821 | Function Loss:  -1.5685\n",
      "Total loss:  -1.4294 | PDE Loss:  -2.9893 | Function Loss:  -1.5693\n",
      "Total loss:  -1.4314 | PDE Loss:  -2.9901 | Function Loss:  -1.5719\n",
      "Total loss:  -1.4336 | PDE Loss:  -2.994 | Function Loss:  -1.5734\n",
      "Total loss:  -1.4353 | PDE Loss:  -2.9968 | Function Loss:  -1.5747\n",
      "Total loss:  -1.4365 | PDE Loss:  -2.9962 | Function Loss:  -1.5765\n",
      "Total loss:  -1.4376 | PDE Loss:  -2.9984 | Function Loss:  -1.5772\n",
      "Total loss:  -1.4387 | PDE Loss:  -3.0008 | Function Loss:  -1.5778\n",
      "Total loss:  -1.4395 | PDE Loss:  -3.004 | Function Loss:  -1.5777\n",
      "Total loss:  -1.4402 | PDE Loss:  -3.0083 | Function Loss:  -1.577\n",
      "Total loss:  -1.4407 | PDE Loss:  -3.0131 | Function Loss:  -1.576\n",
      "Total loss:  -1.4412 | PDE Loss:  -3.0181 | Function Loss:  -1.5749\n",
      "Total loss:  -1.4418 | PDE Loss:  -3.0222 | Function Loss:  -1.5742\n",
      "Total loss:  -1.4424 | PDE Loss:  -3.0286 | Function Loss:  -1.5727\n",
      "Total loss:  -1.4431 | PDE Loss:  -3.0327 | Function Loss:  -1.5722\n",
      "Total loss:  -1.4441 | PDE Loss:  -3.039 | Function Loss:  -1.5714\n",
      "Total loss:  -1.4457 | PDE Loss:  -3.0475 | Function Loss:  -1.5707\n",
      "Total loss:  -1.4484 | PDE Loss:  -3.0556 | Function Loss:  -1.5717\n",
      "Total loss:  -1.453 | PDE Loss:  -3.0674 | Function Loss:  -1.5739\n",
      "Total loss:  -1.4591 | PDE Loss:  -3.0852 | Function Loss:  -1.5763\n",
      "Total loss:  -1.4636 | PDE Loss:  -3.0869 | Function Loss:  -1.5817\n",
      "Total loss:  -1.4678 | PDE Loss:  -3.0905 | Function Loss:  -1.5861\n",
      "Total loss:  -1.4706 | PDE Loss:  -3.0973 | Function Loss:  -1.5876\n",
      "Total loss:  -1.4724 | PDE Loss:  -3.0959 | Function Loss:  -1.5904\n",
      "Total loss:  -1.4735 | PDE Loss:  -3.0886 | Function Loss:  -1.5942\n",
      "Total loss:  -1.4743 | PDE Loss:  -3.088 | Function Loss:  -1.5954\n",
      "Total loss:  -1.475 | PDE Loss:  -3.0877 | Function Loss:  -1.5965\n",
      "Total loss:  -1.4756 | PDE Loss:  -3.0892 | Function Loss:  -1.5968\n",
      "Total loss:  -1.4761 | PDE Loss:  -3.0922 | Function Loss:  -1.5965\n",
      "Total loss:  -1.4767 | PDE Loss:  -3.0953 | Function Loss:  -1.5963\n",
      "Total loss:  -1.4774 | PDE Loss:  -3.0995 | Function Loss:  -1.5958\n",
      "Total loss:  -1.4782 | PDE Loss:  -3.1042 | Function Loss:  -1.5954\n",
      "Total loss:  -1.4792 | PDE Loss:  -3.1079 | Function Loss:  -1.5956\n",
      "Total loss:  -1.4806 | PDE Loss:  -3.1147 | Function Loss:  -1.5953\n",
      "Total loss:  -1.4821 | PDE Loss:  -3.1167 | Function Loss:  -1.5967\n",
      "Total loss:  -1.484 | PDE Loss:  -3.1243 | Function Loss:  -1.5969\n",
      "Total loss:  -1.4859 | PDE Loss:  -3.1259 | Function Loss:  -1.5988\n",
      "Total loss:  -1.4878 | PDE Loss:  -3.1257 | Function Loss:  -1.6014\n",
      "Total loss:  -1.49 | PDE Loss:  -3.1269 | Function Loss:  -1.6039\n",
      "Total loss:  -1.4923 | PDE Loss:  -3.1279 | Function Loss:  -1.6067\n",
      "Total loss:  -1.4953 | PDE Loss:  -3.1315 | Function Loss:  -1.6095\n",
      "Total loss:  -1.499 | PDE Loss:  -3.138 | Function Loss:  -1.6122\n",
      "Total loss:  -1.5025 | PDE Loss:  -3.1422 | Function Loss:  -1.6156\n",
      "Total loss:  -1.5056 | PDE Loss:  -3.1495 | Function Loss:  -1.6174\n",
      "Total loss:  -1.5087 | PDE Loss:  -3.152 | Function Loss:  -1.6207\n",
      "Total loss:  -1.5111 | PDE Loss:  -3.1578 | Function Loss:  -1.6221\n",
      "Total loss:  -1.5124 | PDE Loss:  -3.1609 | Function Loss:  -1.623\n",
      "Total loss:  -1.5133 | PDE Loss:  -3.1623 | Function Loss:  -1.6236\n",
      "Total loss:  -1.514 | PDE Loss:  -3.1646 | Function Loss:  -1.6239\n",
      "Total loss:  -1.5147 | PDE Loss:  -3.1668 | Function Loss:  -1.6242\n",
      "Total loss:  -1.5157 | PDE Loss:  -3.1706 | Function Loss:  -1.6244\n",
      "Total loss:  -1.517 | PDE Loss:  -3.1732 | Function Loss:  -1.6254\n",
      "Total loss:  -1.5189 | PDE Loss:  -3.1773 | Function Loss:  -1.6266\n",
      "Total loss:  -1.5213 | PDE Loss:  -3.178 | Function Loss:  -1.6294\n",
      "Total loss:  -1.5238 | PDE Loss:  -3.1788 | Function Loss:  -1.6325\n",
      "Total loss:  -1.5259 | PDE Loss:  -3.1743 | Function Loss:  -1.6364\n",
      "Total loss:  -1.5273 | PDE Loss:  -3.1742 | Function Loss:  -1.6382\n",
      "Total loss:  -1.5285 | PDE Loss:  -3.1739 | Function Loss:  -1.64\n",
      "Total loss:  -1.5299 | PDE Loss:  -3.1713 | Function Loss:  -1.6425\n",
      "Total loss:  -1.5316 | PDE Loss:  -3.1734 | Function Loss:  -1.6441\n",
      "Total loss:  -1.5335 | PDE Loss:  -3.1736 | Function Loss:  -1.6465\n",
      "Total loss:  -1.5357 | PDE Loss:  -3.1744 | Function Loss:  -1.6491\n",
      "Total loss:  -1.5384 | PDE Loss:  -3.1819 | Function Loss:  -1.6504\n",
      "Total loss:  -1.5418 | PDE Loss:  -3.1858 | Function Loss:  -1.6536\n",
      "Total loss:  -1.5447 | PDE Loss:  -3.1942 | Function Loss:  -1.6549\n",
      "Total loss:  -1.5471 | PDE Loss:  -3.2047 | Function Loss:  -1.6549\n",
      "Total loss:  -1.5488 | PDE Loss:  -3.2058 | Function Loss:  -1.6569\n",
      "Total loss:  -1.5497 | PDE Loss:  -3.2084 | Function Loss:  -1.6573\n",
      "Total loss:  -1.5505 | PDE Loss:  -3.2101 | Function Loss:  -1.6578\n",
      "Total loss:  -1.5513 | PDE Loss:  -3.2107 | Function Loss:  -1.6587\n",
      "Total loss:  -1.5519 | PDE Loss:  -3.2097 | Function Loss:  -1.6597\n",
      "Total loss:  -1.5525 | PDE Loss:  -3.2122 | Function Loss:  -1.6598\n",
      "Total loss:  -1.5532 | PDE Loss:  -3.2123 | Function Loss:  -1.6607\n",
      "Total loss:  -1.5541 | PDE Loss:  -3.2157 | Function Loss:  -1.6609\n",
      "Total loss:  -1.5554 | PDE Loss:  -3.2232 | Function Loss:  -1.6605\n",
      "Total loss:  -1.5568 | PDE Loss:  -3.2317 | Function Loss:  -1.66\n",
      "Total loss:  -1.5579 | PDE Loss:  -3.2391 | Function Loss:  -1.6594\n",
      "Total loss:  -1.5593 | PDE Loss:  -3.249 | Function Loss:  -1.6586\n",
      "Total loss:  -1.5606 | PDE Loss:  -3.2578 | Function Loss:  -1.6579\n",
      "Total loss:  -1.5614 | PDE Loss:  -3.2603 | Function Loss:  -1.6583\n",
      "Total loss:  -1.562 | PDE Loss:  -3.2641 | Function Loss:  -1.6581\n",
      "Total loss:  -1.5625 | PDE Loss:  -3.2628 | Function Loss:  -1.6591\n",
      "Total loss:  -1.563 | PDE Loss:  -3.2634 | Function Loss:  -1.6596\n",
      "Total loss:  -1.5636 | PDE Loss:  -3.2628 | Function Loss:  -1.6604\n",
      "Total loss:  -1.5641 | PDE Loss:  -3.2608 | Function Loss:  -1.6616\n",
      "Total loss:  -1.5647 | PDE Loss:  -3.2611 | Function Loss:  -1.6623\n",
      "Total loss:  -1.5654 | PDE Loss:  -3.2589 | Function Loss:  -1.6637\n",
      "Total loss:  -1.5663 | PDE Loss:  -3.2603 | Function Loss:  -1.6644\n",
      "Total loss:  -1.5672 | PDE Loss:  -3.261 | Function Loss:  -1.6654\n",
      "Total loss:  -1.5682 | PDE Loss:  -3.2622 | Function Loss:  -1.6664\n",
      "Total loss:  -1.5693 | PDE Loss:  -3.2649 | Function Loss:  -1.6671\n",
      "Total loss:  -1.5703 | PDE Loss:  -3.2678 | Function Loss:  -1.6676\n",
      "Total loss:  -1.5714 | PDE Loss:  -3.2695 | Function Loss:  -1.6685\n",
      "Total loss:  -1.5728 | PDE Loss:  -3.2742 | Function Loss:  -1.6691\n",
      "Total loss:  -1.5746 | PDE Loss:  -3.275 | Function Loss:  -1.6712\n",
      "Total loss:  -1.5773 | PDE Loss:  -3.2803 | Function Loss:  -1.6732\n",
      "Total loss:  -1.5803 | PDE Loss:  -3.2829 | Function Loss:  -1.6763\n",
      "Total loss:  -1.5833 | PDE Loss:  -3.2821 | Function Loss:  -1.6802\n",
      "Total loss:  -1.5858 | PDE Loss:  -3.2842 | Function Loss:  -1.6828\n",
      "Total loss:  -1.5874 | PDE Loss:  -3.2814 | Function Loss:  -1.6856\n",
      "Total loss:  -1.5886 | PDE Loss:  -3.282 | Function Loss:  -1.6868\n",
      "Total loss:  -1.5894 | PDE Loss:  -3.2825 | Function Loss:  -1.6878\n",
      "Total loss:  -1.5901 | PDE Loss:  -3.2837 | Function Loss:  -1.6884\n",
      "Total loss:  -1.5906 | PDE Loss:  -3.2861 | Function Loss:  -1.6883\n",
      "Total loss:  -1.5909 | PDE Loss:  -3.2885 | Function Loss:  -1.6882\n",
      "Total loss:  -1.5914 | PDE Loss:  -3.2902 | Function Loss:  -1.6883\n",
      "Total loss:  -1.5919 | PDE Loss:  -3.2935 | Function Loss:  -1.6882\n",
      "Total loss:  -1.5924 | PDE Loss:  -3.2947 | Function Loss:  -1.6885\n",
      "Total loss:  -1.5933 | PDE Loss:  -3.2971 | Function Loss:  -1.689\n",
      "Total loss:  -1.5946 | PDE Loss:  -3.2993 | Function Loss:  -1.6901\n",
      "Total loss:  -1.5961 | PDE Loss:  -3.2966 | Function Loss:  -1.6927\n",
      "Total loss:  -1.5986 | PDE Loss:  -3.2997 | Function Loss:  -1.695\n",
      "Total loss:  -1.6016 | PDE Loss:  -3.2961 | Function Loss:  -1.6996\n",
      "Total loss:  -1.6044 | PDE Loss:  -3.3024 | Function Loss:  -1.7016\n",
      "Total loss:  -1.6069 | PDE Loss:  -3.3044 | Function Loss:  -1.7042\n",
      "Total loss:  -1.6094 | PDE Loss:  -3.3138 | Function Loss:  -1.7049\n",
      "Total loss:  -1.6119 | PDE Loss:  -3.3232 | Function Loss:  -1.7057\n",
      "Total loss:  -1.614 | PDE Loss:  -3.3294 | Function Loss:  -1.7069\n",
      "Total loss:  -1.6159 | PDE Loss:  -3.3402 | Function Loss:  -1.7066\n",
      "Total loss:  -1.6175 | PDE Loss:  -3.3464 | Function Loss:  -1.7073\n",
      "Total loss:  -1.6191 | PDE Loss:  -3.3511 | Function Loss:  -1.7081\n",
      "Total loss:  -1.6203 | PDE Loss:  -3.3564 | Function Loss:  -1.7084\n",
      "Total loss:  -1.6213 | PDE Loss:  -3.3571 | Function Loss:  -1.7095\n",
      "Total loss:  -1.622 | PDE Loss:  -3.3548 | Function Loss:  -1.7109\n",
      "Total loss:  -1.6225 | PDE Loss:  -3.3532 | Function Loss:  -1.7118\n",
      "Total loss:  -1.6228 | PDE Loss:  -3.3527 | Function Loss:  -1.7123\n",
      "Total loss:  -1.623 | PDE Loss:  -3.3518 | Function Loss:  -1.7127\n",
      "Total loss:  -1.6232 | PDE Loss:  -3.3518 | Function Loss:  -1.713\n",
      "Total loss:  -1.6234 | PDE Loss:  -3.3528 | Function Loss:  -1.713\n",
      "Total loss:  -1.6236 | PDE Loss:  -3.3526 | Function Loss:  -1.7133\n",
      "Total loss:  -1.6238 | PDE Loss:  -3.354 | Function Loss:  -1.7133\n",
      "Total loss:  -1.624 | PDE Loss:  -3.3546 | Function Loss:  -1.7134\n",
      "Total loss:  -1.6244 | PDE Loss:  -3.3562 | Function Loss:  -1.7135\n",
      "Total loss:  -1.6248 | PDE Loss:  -3.3582 | Function Loss:  -1.7136\n",
      "Total loss:  -1.6255 | PDE Loss:  -3.3588 | Function Loss:  -1.7143\n",
      "Total loss:  -1.6265 | PDE Loss:  -3.3652 | Function Loss:  -1.714\n",
      "Total loss:  -1.6277 | PDE Loss:  -3.3651 | Function Loss:  -1.7155\n",
      "Total loss:  -1.6293 | PDE Loss:  -3.3691 | Function Loss:  -1.7165\n",
      "Total loss:  -1.6312 | PDE Loss:  -3.3739 | Function Loss:  -1.7178\n",
      "Total loss:  -1.633 | PDE Loss:  -3.3708 | Function Loss:  -1.7207\n",
      "Total loss:  -1.6348 | PDE Loss:  -3.3756 | Function Loss:  -1.7218\n",
      "Total loss:  -1.6365 | PDE Loss:  -3.3774 | Function Loss:  -1.7235\n",
      "Total loss:  -1.638 | PDE Loss:  -3.3769 | Function Loss:  -1.7254\n",
      "Total loss:  -1.6397 | PDE Loss:  -3.3842 | Function Loss:  -1.726\n",
      "Total loss:  -1.6412 | PDE Loss:  -3.3866 | Function Loss:  -1.7272\n",
      "Total loss:  -1.6421 | PDE Loss:  -3.3858 | Function Loss:  -1.7285\n",
      "Total loss:  -1.6431 | PDE Loss:  -3.3908 | Function Loss:  -1.7287\n",
      "Total loss:  -1.6442 | PDE Loss:  -3.3891 | Function Loss:  -1.7304\n",
      "Total loss:  -1.645 | PDE Loss:  -3.3914 | Function Loss:  -1.7309\n",
      "Total loss:  -1.6456 | PDE Loss:  -3.3956 | Function Loss:  -1.7307\n",
      "Total loss:  -1.646 | PDE Loss:  -3.395 | Function Loss:  -1.7313\n",
      "Total loss:  -1.6466 | PDE Loss:  -3.3965 | Function Loss:  -1.7316\n",
      "Total loss:  -1.6472 | PDE Loss:  -3.4009 | Function Loss:  -1.7314\n",
      "Total loss:  -1.6478 | PDE Loss:  -3.4042 | Function Loss:  -1.7314\n",
      "Total loss:  -1.6485 | PDE Loss:  -3.4082 | Function Loss:  -1.7315\n",
      "Total loss:  -1.6492 | PDE Loss:  -3.4119 | Function Loss:  -1.7316\n",
      "Total loss:  -1.6501 | PDE Loss:  -3.4162 | Function Loss:  -1.7317\n",
      "Total loss:  -1.6513 | PDE Loss:  -3.4166 | Function Loss:  -1.7331\n",
      "Total loss:  -1.6522 | PDE Loss:  -3.4338 | Function Loss:  -1.7307\n",
      "Total loss:  -1.6538 | PDE Loss:  -3.427 | Function Loss:  -1.734\n",
      "Total loss:  -1.6555 | PDE Loss:  -3.4201 | Function Loss:  -1.7375\n",
      "Total loss:  -1.6578 | PDE Loss:  -3.4166 | Function Loss:  -1.741\n",
      "Total loss:  -1.6601 | PDE Loss:  -3.412 | Function Loss:  -1.7447\n",
      "Total loss:  -1.662 | PDE Loss:  -3.4134 | Function Loss:  -1.7467\n",
      "Total loss:  -1.6637 | PDE Loss:  -3.4174 | Function Loss:  -1.7479\n",
      "Total loss:  -1.6655 | PDE Loss:  -3.4233 | Function Loss:  -1.7489\n",
      "Total loss:  -1.6676 | PDE Loss:  -3.4315 | Function Loss:  -1.7497\n",
      "Total loss:  -1.6691 | PDE Loss:  -3.4409 | Function Loss:  -1.7495\n",
      "Total loss:  -1.67 | PDE Loss:  -3.4414 | Function Loss:  -1.7506\n",
      "Total loss:  -1.6706 | PDE Loss:  -3.449 | Function Loss:  -1.7497\n",
      "Total loss:  -1.671 | PDE Loss:  -3.4489 | Function Loss:  -1.7502\n",
      "Total loss:  -1.6712 | PDE Loss:  -3.4482 | Function Loss:  -1.7506\n",
      "Total loss:  -1.6716 | PDE Loss:  -3.4493 | Function Loss:  -1.7509\n",
      "Total loss:  -1.6721 | PDE Loss:  -3.4485 | Function Loss:  -1.7517\n",
      "Total loss:  -1.6726 | PDE Loss:  -3.4507 | Function Loss:  -1.7518\n",
      "Total loss:  -1.6731 | PDE Loss:  -3.4533 | Function Loss:  -1.7518\n",
      "Total loss:  -1.6735 | PDE Loss:  -3.452 | Function Loss:  -1.7526\n",
      "Total loss:  -1.6742 | PDE Loss:  -3.4587 | Function Loss:  -1.7521\n",
      "Total loss:  -1.6747 | PDE Loss:  -3.4628 | Function Loss:  -1.7519\n",
      "Total loss:  -1.6757 | PDE Loss:  -3.4677 | Function Loss:  -1.7521\n",
      "Total loss:  -1.6769 | PDE Loss:  -3.4757 | Function Loss:  -1.752\n",
      "Total loss:  -1.6781 | PDE Loss:  -3.4812 | Function Loss:  -1.7524\n",
      "Total loss:  -1.6796 | PDE Loss:  -3.4897 | Function Loss:  -1.7527\n",
      "Total loss:  -1.6814 | PDE Loss:  -3.4973 | Function Loss:  -1.7534\n",
      "Total loss:  -1.683 | PDE Loss:  -3.5027 | Function Loss:  -1.7543\n",
      "Total loss:  -1.6843 | PDE Loss:  -3.5101 | Function Loss:  -1.7545\n",
      "Total loss:  -1.6854 | PDE Loss:  -3.5128 | Function Loss:  -1.7554\n",
      "Total loss:  -1.6868 | PDE Loss:  -3.5154 | Function Loss:  -1.7565\n",
      "Total loss:  -1.6879 | PDE Loss:  -3.5203 | Function Loss:  -1.757\n",
      "Total loss:  -1.6887 | PDE Loss:  -3.5169 | Function Loss:  -1.7585\n",
      "Total loss:  -1.6899 | PDE Loss:  -3.5209 | Function Loss:  -1.7592\n",
      "Total loss:  -1.6914 | PDE Loss:  -3.5173 | Function Loss:  -1.7616\n",
      "Total loss:  -1.693 | PDE Loss:  -3.5224 | Function Loss:  -1.7626\n",
      "Total loss:  -1.6944 | PDE Loss:  -3.5203 | Function Loss:  -1.7646\n",
      "Total loss:  -1.6958 | PDE Loss:  -3.5241 | Function Loss:  -1.7656\n",
      "Total loss:  -1.6972 | PDE Loss:  -3.5202 | Function Loss:  -1.7679\n",
      "Total loss:  -1.6983 | PDE Loss:  -3.5208 | Function Loss:  -1.7691\n",
      "Total loss:  -1.6993 | PDE Loss:  -3.5213 | Function Loss:  -1.7703\n",
      "Total loss:  -1.7004 | PDE Loss:  -3.5205 | Function Loss:  -1.7716\n",
      "Total loss:  -1.7013 | PDE Loss:  -3.5205 | Function Loss:  -1.7727\n",
      "Total loss:  -1.7019 | PDE Loss:  -3.5228 | Function Loss:  -1.773\n",
      "Total loss:  -1.7023 | PDE Loss:  -3.5248 | Function Loss:  -1.7731\n",
      "Total loss:  -1.7026 | PDE Loss:  -3.527 | Function Loss:  -1.7731\n",
      "Total loss:  -1.7029 | PDE Loss:  -3.5276 | Function Loss:  -1.7734\n",
      "Total loss:  -1.7034 | PDE Loss:  -3.5299 | Function Loss:  -1.7735\n",
      "Total loss:  -1.7039 | PDE Loss:  -3.5295 | Function Loss:  -1.7742\n",
      "Total loss:  -1.7046 | PDE Loss:  -3.5317 | Function Loss:  -1.7746\n",
      "Total loss:  -1.7052 | PDE Loss:  -3.5296 | Function Loss:  -1.7757\n",
      "Total loss:  -1.7059 | PDE Loss:  -3.5307 | Function Loss:  -1.7763\n",
      "Total loss:  -1.7066 | PDE Loss:  -3.5284 | Function Loss:  -1.7775\n",
      "Total loss:  -1.7073 | PDE Loss:  -3.5267 | Function Loss:  -1.7786\n",
      "Total loss:  -1.708 | PDE Loss:  -3.5252 | Function Loss:  -1.7797\n",
      "Total loss:  -1.7087 | PDE Loss:  -3.5232 | Function Loss:  -1.7809\n",
      "Total loss:  -1.7095 | PDE Loss:  -3.5214 | Function Loss:  -1.7822\n",
      "Total loss:  -1.7103 | PDE Loss:  -3.5251 | Function Loss:  -1.7825\n",
      "Total loss:  -1.7111 | PDE Loss:  -3.5247 | Function Loss:  -1.7835\n",
      "Total loss:  -1.7124 | PDE Loss:  -3.5283 | Function Loss:  -1.7844\n",
      "Total loss:  -1.7137 | PDE Loss:  -3.5275 | Function Loss:  -1.7861\n",
      "Total loss:  -1.7148 | PDE Loss:  -3.5305 | Function Loss:  -1.7869\n",
      "Total loss:  -1.7159 | PDE Loss:  -3.5359 | Function Loss:  -1.7871\n",
      "Total loss:  -1.7174 | PDE Loss:  -3.5428 | Function Loss:  -1.7877\n",
      "Total loss:  -1.7191 | PDE Loss:  -3.5489 | Function Loss:  -1.7887\n",
      "Total loss:  -1.7208 | PDE Loss:  -3.5589 | Function Loss:  -1.7889\n",
      "Total loss:  -1.7225 | PDE Loss:  -3.5643 | Function Loss:  -1.79\n",
      "Total loss:  -1.7241 | PDE Loss:  -3.5769 | Function Loss:  -1.7898\n",
      "Total loss:  -1.7251 | PDE Loss:  -3.5795 | Function Loss:  -1.7905\n",
      "Total loss:  -1.726 | PDE Loss:  -3.5826 | Function Loss:  -1.7911\n",
      "Total loss:  -1.7266 | PDE Loss:  -3.5835 | Function Loss:  -1.7916\n",
      "Total loss:  -1.727 | PDE Loss:  -3.5825 | Function Loss:  -1.7922\n",
      "Total loss:  -1.7273 | PDE Loss:  -3.5826 | Function Loss:  -1.7926\n",
      "Total loss:  -1.7277 | PDE Loss:  -3.5824 | Function Loss:  -1.7931\n",
      "Total loss:  -1.7283 | PDE Loss:  -3.581 | Function Loss:  -1.794\n",
      "Total loss:  -1.7291 | PDE Loss:  -3.5843 | Function Loss:  -1.7943\n",
      "Total loss:  -1.7297 | PDE Loss:  -3.5853 | Function Loss:  -1.7949\n",
      "Total loss:  -1.7305 | PDE Loss:  -3.588 | Function Loss:  -1.7954\n",
      "Total loss:  -1.7312 | PDE Loss:  -3.5896 | Function Loss:  -1.796\n",
      "Total loss:  -1.7321 | PDE Loss:  -3.5921 | Function Loss:  -1.7966\n",
      "Total loss:  -1.7329 | PDE Loss:  -3.5928 | Function Loss:  -1.7974\n",
      "Total loss:  -1.7337 | PDE Loss:  -3.5935 | Function Loss:  -1.7983\n",
      "Total loss:  -1.7345 | PDE Loss:  -3.5946 | Function Loss:  -1.799\n",
      "Total loss:  -1.7353 | PDE Loss:  -3.5938 | Function Loss:  -1.8001\n",
      "Total loss:  -1.7363 | PDE Loss:  -3.5977 | Function Loss:  -1.8006\n",
      "Total loss:  -1.7374 | PDE Loss:  -3.5981 | Function Loss:  -1.8019\n",
      "Total loss:  -1.7388 | PDE Loss:  -3.6053 | Function Loss:  -1.8022\n",
      "Total loss:  -1.7403 | PDE Loss:  -3.6115 | Function Loss:  -1.803\n",
      "Total loss:  -1.7419 | PDE Loss:  -3.6206 | Function Loss:  -1.8035\n",
      "Total loss:  -1.7436 | PDE Loss:  -3.6303 | Function Loss:  -1.804\n",
      "Total loss:  -1.7451 | PDE Loss:  -3.6372 | Function Loss:  -1.8047\n",
      "Total loss:  -1.7465 | PDE Loss:  -3.6432 | Function Loss:  -1.8055\n",
      "Total loss:  -1.7478 | PDE Loss:  -3.6475 | Function Loss:  -1.8063\n",
      "Total loss:  -1.749 | PDE Loss:  -3.6466 | Function Loss:  -1.8077\n",
      "Total loss:  -1.7503 | PDE Loss:  -3.6516 | Function Loss:  -1.8085\n",
      "Total loss:  -1.7517 | PDE Loss:  -3.6491 | Function Loss:  -1.8105\n",
      "Total loss:  -1.753 | PDE Loss:  -3.6514 | Function Loss:  -1.8117\n",
      "Total loss:  -1.7543 | PDE Loss:  -3.6519 | Function Loss:  -1.8131\n",
      "Total loss:  -1.7556 | PDE Loss:  -3.6546 | Function Loss:  -1.8142\n",
      "Total loss:  -1.7565 | PDE Loss:  -3.6538 | Function Loss:  -1.8153\n",
      "Total loss:  -1.7574 | PDE Loss:  -3.657 | Function Loss:  -1.8159\n",
      "Total loss:  -1.7583 | PDE Loss:  -3.6562 | Function Loss:  -1.8171\n",
      "Total loss:  -1.7589 | PDE Loss:  -3.6539 | Function Loss:  -1.818\n",
      "Total loss:  -1.7595 | PDE Loss:  -3.6603 | Function Loss:  -1.8178\n",
      "Total loss:  -1.76 | PDE Loss:  -3.6576 | Function Loss:  -1.8188\n",
      "Total loss:  -1.7604 | PDE Loss:  -3.6596 | Function Loss:  -1.819\n",
      "Total loss:  -1.7608 | PDE Loss:  -3.6606 | Function Loss:  -1.8192\n",
      "Total loss:  -1.7611 | PDE Loss:  -3.6608 | Function Loss:  -1.8195\n",
      "Total loss:  -1.7613 | PDE Loss:  -3.6597 | Function Loss:  -1.82\n",
      "Total loss:  -1.7616 | PDE Loss:  -3.6591 | Function Loss:  -1.8204\n",
      "Total loss:  -1.7621 | PDE Loss:  -3.6528 | Function Loss:  -1.8219\n",
      "Total loss:  -1.7625 | PDE Loss:  -3.6523 | Function Loss:  -1.8224\n",
      "Total loss:  -1.763 | PDE Loss:  -3.649 | Function Loss:  -1.8235\n",
      "Total loss:  -1.7636 | PDE Loss:  -3.6445 | Function Loss:  -1.8248\n",
      "Total loss:  -1.7643 | PDE Loss:  -3.6422 | Function Loss:  -1.826\n",
      "Total loss:  -1.7652 | PDE Loss:  -3.6364 | Function Loss:  -1.8279\n",
      "Total loss:  -1.766 | PDE Loss:  -3.6359 | Function Loss:  -1.829\n",
      "Total loss:  -1.7669 | PDE Loss:  -3.632 | Function Loss:  -1.8306\n",
      "Total loss:  -1.7679 | PDE Loss:  -3.6333 | Function Loss:  -1.8316\n",
      "Total loss:  -1.7693 | PDE Loss:  -3.6296 | Function Loss:  -1.8338\n",
      "Total loss:  -1.7709 | PDE Loss:  -3.6366 | Function Loss:  -1.8345\n",
      "Total loss:  -1.7724 | PDE Loss:  -3.6412 | Function Loss:  -1.8355\n",
      "Total loss:  -1.7742 | PDE Loss:  -3.6374 | Function Loss:  -1.8382\n",
      "Total loss:  -1.7761 | PDE Loss:  -3.6409 | Function Loss:  -1.8398\n",
      "Total loss:  -1.7774 | PDE Loss:  -3.648 | Function Loss:  -1.8403\n",
      "Total loss:  -1.7781 | PDE Loss:  -3.639 | Function Loss:  -1.8425\n",
      "Total loss:  -1.7785 | PDE Loss:  -3.641 | Function Loss:  -1.8427\n",
      "Total loss:  -1.7792 | PDE Loss:  -3.6396 | Function Loss:  -1.8436\n",
      "Total loss:  -1.7799 | PDE Loss:  -3.6379 | Function Loss:  -1.8447\n",
      "Total loss:  -1.7806 | PDE Loss:  -3.6388 | Function Loss:  -1.8454\n",
      "Total loss:  -1.7812 | PDE Loss:  -3.6342 | Function Loss:  -1.8468\n",
      "Total loss:  -1.7817 | PDE Loss:  -3.637 | Function Loss:  -1.847\n",
      "Total loss:  -1.7821 | PDE Loss:  -3.6394 | Function Loss:  -1.8471\n",
      "Total loss:  -1.7826 | PDE Loss:  -3.6433 | Function Loss:  -1.847\n",
      "Total loss:  -1.783 | PDE Loss:  -3.6457 | Function Loss:  -1.8471\n",
      "Total loss:  -1.7834 | PDE Loss:  -3.6488 | Function Loss:  -1.8471\n",
      "Total loss:  -1.784 | PDE Loss:  -3.6498 | Function Loss:  -1.8476\n",
      "Total loss:  -1.7845 | PDE Loss:  -3.6525 | Function Loss:  -1.8478\n",
      "Total loss:  -1.7851 | PDE Loss:  -3.6538 | Function Loss:  -1.8483\n",
      "Total loss:  -1.7858 | PDE Loss:  -3.6547 | Function Loss:  -1.8489\n",
      "Total loss:  -1.7866 | PDE Loss:  -3.6535 | Function Loss:  -1.85\n",
      "Total loss:  -1.7875 | PDE Loss:  -3.6528 | Function Loss:  -1.8512\n",
      "Total loss:  -1.7886 | PDE Loss:  -3.6474 | Function Loss:  -1.8533\n",
      "Total loss:  -1.7899 | PDE Loss:  -3.6425 | Function Loss:  -1.8556\n",
      "Total loss:  -1.7911 | PDE Loss:  -3.6432 | Function Loss:  -1.8569\n",
      "Total loss:  -1.7928 | PDE Loss:  -3.6349 | Function Loss:  -1.8603\n",
      "Total loss:  -1.7949 | PDE Loss:  -3.6416 | Function Loss:  -1.8616\n",
      "Total loss:  -1.7972 | PDE Loss:  -3.6381 | Function Loss:  -1.8648\n",
      "Total loss:  -1.799 | PDE Loss:  -3.6485 | Function Loss:  -1.8652\n",
      "Total loss:  -1.8012 | PDE Loss:  -3.6635 | Function Loss:  -1.8653\n",
      "Total loss:  -1.8035 | PDE Loss:  -3.6826 | Function Loss:  -1.865\n",
      "Total loss:  -1.8055 | PDE Loss:  -3.696 | Function Loss:  -1.8654\n",
      "Total loss:  -1.8071 | PDE Loss:  -3.7196 | Function Loss:  -1.8638\n",
      "Total loss:  -1.808 | PDE Loss:  -3.7257 | Function Loss:  -1.8639\n",
      "Total loss:  -1.8088 | PDE Loss:  -3.7302 | Function Loss:  -1.8642\n",
      "Total loss:  -1.8093 | PDE Loss:  -3.7284 | Function Loss:  -1.8651\n",
      "Total loss:  -1.8097 | PDE Loss:  -3.7292 | Function Loss:  -1.8654\n",
      "Total loss:  -1.81 | PDE Loss:  -3.7273 | Function Loss:  -1.866\n",
      "Total loss:  -1.8103 | PDE Loss:  -3.7276 | Function Loss:  -1.8663\n",
      "Total loss:  -1.8107 | PDE Loss:  -3.7292 | Function Loss:  -1.8665\n",
      "Total loss:  -1.8111 | PDE Loss:  -3.7303 | Function Loss:  -1.8668\n",
      "Total loss:  -1.8116 | PDE Loss:  -3.7343 | Function Loss:  -1.8668\n",
      "Total loss:  -1.8119 | PDE Loss:  -3.7373 | Function Loss:  -1.8668\n",
      "Total loss:  -1.8123 | PDE Loss:  -3.7389 | Function Loss:  -1.8671\n",
      "Total loss:  -1.8128 | PDE Loss:  -3.7403 | Function Loss:  -1.8674\n",
      "Total loss:  -1.8132 | PDE Loss:  -3.7409 | Function Loss:  -1.8678\n",
      "Total loss:  -1.8137 | PDE Loss:  -3.7403 | Function Loss:  -1.8685\n",
      "Total loss:  -1.8143 | PDE Loss:  -3.7418 | Function Loss:  -1.8689\n",
      "Total loss:  -1.815 | PDE Loss:  -3.7388 | Function Loss:  -1.8701\n",
      "Total loss:  -1.8157 | PDE Loss:  -3.7392 | Function Loss:  -1.8708\n",
      "Total loss:  -1.8164 | PDE Loss:  -3.7369 | Function Loss:  -1.8719\n",
      "Total loss:  -1.817 | PDE Loss:  -3.7349 | Function Loss:  -1.8729\n",
      "Total loss:  -1.8175 | PDE Loss:  -3.7341 | Function Loss:  -1.8736\n",
      "Total loss:  -1.8182 | PDE Loss:  -3.7326 | Function Loss:  -1.8746\n",
      "Total loss:  -1.819 | PDE Loss:  -3.7311 | Function Loss:  -1.8757\n",
      "Total loss:  -1.8197 | PDE Loss:  -3.7293 | Function Loss:  -1.8767\n",
      "Total loss:  -1.8203 | PDE Loss:  -3.7307 | Function Loss:  -1.8772\n",
      "Total loss:  -1.8209 | PDE Loss:  -3.7306 | Function Loss:  -1.8779\n",
      "Total loss:  -1.8217 | PDE Loss:  -3.7301 | Function Loss:  -1.8789\n",
      "Total loss:  -1.8225 | PDE Loss:  -3.733 | Function Loss:  -1.8794\n",
      "Total loss:  -1.8231 | PDE Loss:  -3.7309 | Function Loss:  -1.8805\n",
      "Total loss:  -1.8239 | PDE Loss:  -3.7302 | Function Loss:  -1.8814\n",
      "Total loss:  -1.8247 | PDE Loss:  -3.7284 | Function Loss:  -1.8826\n",
      "Total loss:  -1.8255 | PDE Loss:  -3.7166 | Function Loss:  -1.8853\n",
      "Total loss:  -1.8264 | PDE Loss:  -3.7238 | Function Loss:  -1.8853\n",
      "Total loss:  -1.8271 | PDE Loss:  -3.7178 | Function Loss:  -1.8868\n",
      "Total loss:  -1.8278 | PDE Loss:  -3.7131 | Function Loss:  -1.8884\n",
      "Total loss:  -1.8286 | PDE Loss:  -3.7092 | Function Loss:  -1.8899\n",
      "Total loss:  -1.8293 | PDE Loss:  -3.7077 | Function Loss:  -1.891\n",
      "Total loss:  -1.8302 | PDE Loss:  -3.7091 | Function Loss:  -1.8918\n",
      "Total loss:  -1.8315 | PDE Loss:  -3.7121 | Function Loss:  -1.8928\n",
      "Total loss:  -1.8329 | PDE Loss:  -3.7201 | Function Loss:  -1.8932\n",
      "Total loss:  -1.8342 | PDE Loss:  -3.7252 | Function Loss:  -1.894\n",
      "Total loss:  -1.8355 | PDE Loss:  -3.7357 | Function Loss:  -1.8939\n",
      "Total loss:  -1.8367 | PDE Loss:  -3.7418 | Function Loss:  -1.8944\n",
      "Total loss:  -1.8378 | PDE Loss:  -3.7449 | Function Loss:  -1.8953\n",
      "Total loss:  -1.8387 | PDE Loss:  -3.7568 | Function Loss:  -1.8946\n",
      "Total loss:  -1.8395 | PDE Loss:  -3.7542 | Function Loss:  -1.8959\n",
      "Total loss:  -1.8405 | PDE Loss:  -3.7555 | Function Loss:  -1.8968\n",
      "Total loss:  -1.8418 | PDE Loss:  -3.7495 | Function Loss:  -1.8992\n",
      "Total loss:  -1.8435 | PDE Loss:  -3.7458 | Function Loss:  -1.9016\n",
      "Total loss:  -1.8451 | PDE Loss:  -3.7433 | Function Loss:  -1.9038\n",
      "Total loss:  -1.8469 | PDE Loss:  -3.7317 | Function Loss:  -1.9075\n",
      "Total loss:  -1.8484 | PDE Loss:  -3.7345 | Function Loss:  -1.9089\n",
      "Total loss:  -1.8502 | PDE Loss:  -3.7324 | Function Loss:  -1.9113\n",
      "Total loss:  -1.8522 | PDE Loss:  -3.7377 | Function Loss:  -1.9127\n",
      "Total loss:  -1.8539 | PDE Loss:  -3.7349 | Function Loss:  -1.9151\n",
      "Total loss:  -1.8557 | PDE Loss:  -3.7335 | Function Loss:  -1.9175\n",
      "Total loss:  -1.858 | PDE Loss:  -3.7296 | Function Loss:  -1.9207\n",
      "Total loss:  -1.8604 | PDE Loss:  -3.7328 | Function Loss:  -1.923\n",
      "Total loss:  -1.8619 | PDE Loss:  -3.7238 | Function Loss:  -1.9261\n",
      "Total loss:  -1.8633 | PDE Loss:  -3.7288 | Function Loss:  -1.9269\n",
      "Total loss:  -1.8646 | PDE Loss:  -3.7333 | Function Loss:  -1.9277\n",
      "Total loss:  -1.8657 | PDE Loss:  -3.7367 | Function Loss:  -1.9285\n",
      "Total loss:  -1.8666 | PDE Loss:  -3.7429 | Function Loss:  -1.9286\n",
      "Total loss:  -1.8674 | PDE Loss:  -3.7466 | Function Loss:  -1.9289\n",
      "Total loss:  -1.8681 | PDE Loss:  -3.7441 | Function Loss:  -1.9301\n",
      "Total loss:  -1.869 | PDE Loss:  -3.7385 | Function Loss:  -1.932\n",
      "Total loss:  -1.8697 | PDE Loss:  -3.7429 | Function Loss:  -1.9322\n",
      "Total loss:  -1.8703 | PDE Loss:  -3.7381 | Function Loss:  -1.9335\n",
      "Total loss:  -1.8708 | PDE Loss:  -3.7381 | Function Loss:  -1.9342\n",
      "Total loss:  -1.8714 | PDE Loss:  -3.7362 | Function Loss:  -1.9351\n",
      "Total loss:  -1.8719 | PDE Loss:  -3.7379 | Function Loss:  -1.9354\n",
      "Total loss:  -1.8723 | PDE Loss:  -3.7392 | Function Loss:  -1.9357\n",
      "Total loss:  -1.8726 | PDE Loss:  -3.7328 | Function Loss:  -1.937\n",
      "Total loss:  -1.8733 | PDE Loss:  -3.7367 | Function Loss:  -1.9372\n",
      "Total loss:  -1.874 | PDE Loss:  -3.7374 | Function Loss:  -1.938\n",
      "Total loss:  -1.875 | PDE Loss:  -3.7401 | Function Loss:  -1.9386\n",
      "Total loss:  -1.8761 | PDE Loss:  -3.7363 | Function Loss:  -1.9405\n",
      "Total loss:  -1.8774 | PDE Loss:  -3.7393 | Function Loss:  -1.9416\n",
      "Total loss:  -1.8789 | PDE Loss:  -3.7346 | Function Loss:  -1.9441\n",
      "Total loss:  -1.8808 | PDE Loss:  -3.734 | Function Loss:  -1.9464\n",
      "Total loss:  -1.8832 | PDE Loss:  -3.7353 | Function Loss:  -1.949\n",
      "Total loss:  -1.8853 | PDE Loss:  -3.7312 | Function Loss:  -1.9521\n",
      "Total loss:  -1.8871 | PDE Loss:  -3.7342 | Function Loss:  -1.9537\n",
      "Total loss:  -1.8892 | PDE Loss:  -3.7381 | Function Loss:  -1.9555\n",
      "Total loss:  -1.8911 | PDE Loss:  -3.7438 | Function Loss:  -1.9568\n",
      "Total loss:  -1.8925 | PDE Loss:  -3.749 | Function Loss:  -1.9575\n",
      "Total loss:  -1.8934 | PDE Loss:  -3.7565 | Function Loss:  -1.9574\n",
      "Total loss:  -1.8943 | PDE Loss:  -3.7581 | Function Loss:  -1.9582\n",
      "Total loss:  -1.895 | PDE Loss:  -3.7617 | Function Loss:  -1.9585\n",
      "Total loss:  -1.8957 | PDE Loss:  -3.7602 | Function Loss:  -1.9595\n",
      "Total loss:  -1.8963 | PDE Loss:  -3.7623 | Function Loss:  -1.9598\n",
      "Total loss:  -1.8968 | PDE Loss:  -3.7547 | Function Loss:  -1.9617\n",
      "Total loss:  -1.8972 | PDE Loss:  -3.7534 | Function Loss:  -1.9624\n",
      "Total loss:  -1.8978 | PDE Loss:  -3.7498 | Function Loss:  -1.9636\n",
      "Total loss:  -1.8985 | PDE Loss:  -3.7457 | Function Loss:  -1.965\n",
      "Total loss:  -1.899 | PDE Loss:  -3.7436 | Function Loss:  -1.9661\n",
      "Total loss:  -1.8996 | PDE Loss:  -3.742 | Function Loss:  -1.967\n",
      "Total loss:  -1.9001 | PDE Loss:  -3.7411 | Function Loss:  -1.9678\n",
      "Total loss:  -1.9008 | PDE Loss:  -3.7423 | Function Loss:  -1.9683\n",
      "Total loss:  -1.9014 | PDE Loss:  -3.7424 | Function Loss:  -1.969\n",
      "Total loss:  -1.9019 | PDE Loss:  -3.7442 | Function Loss:  -1.9693\n",
      "Total loss:  -1.9024 | PDE Loss:  -3.7439 | Function Loss:  -1.97\n",
      "Total loss:  -1.9029 | PDE Loss:  -3.7477 | Function Loss:  -1.9699\n",
      "Total loss:  -1.9034 | PDE Loss:  -3.747 | Function Loss:  -1.9706\n",
      "Total loss:  -1.904 | PDE Loss:  -3.7505 | Function Loss:  -1.9707\n",
      "Total loss:  -1.9047 | PDE Loss:  -3.7535 | Function Loss:  -1.971\n",
      "Total loss:  -1.9055 | PDE Loss:  -3.7587 | Function Loss:  -1.9711\n",
      "Total loss:  -1.9063 | PDE Loss:  -3.7643 | Function Loss:  -1.9711\n",
      "Total loss:  -1.9068 | PDE Loss:  -3.7676 | Function Loss:  -1.9712\n",
      "Total loss:  -1.9073 | PDE Loss:  -3.7701 | Function Loss:  -1.9713\n",
      "Total loss:  -1.9077 | PDE Loss:  -3.7719 | Function Loss:  -1.9716\n",
      "Total loss:  -1.9082 | PDE Loss:  -3.7733 | Function Loss:  -1.972\n",
      "Total loss:  -1.9089 | PDE Loss:  -3.7752 | Function Loss:  -1.9724\n",
      "Total loss:  -1.9094 | PDE Loss:  -3.7771 | Function Loss:  -1.9727\n",
      "Total loss:  -1.9101 | PDE Loss:  -3.779 | Function Loss:  -1.9732\n",
      "Total loss:  -1.911 | PDE Loss:  -3.7795 | Function Loss:  -1.9742\n",
      "Total loss:  -1.9119 | PDE Loss:  -3.7824 | Function Loss:  -1.9748\n",
      "Total loss:  -1.9128 | PDE Loss:  -3.782 | Function Loss:  -1.9758\n",
      "Total loss:  -1.9138 | PDE Loss:  -3.7831 | Function Loss:  -1.9769\n",
      "Total loss:  -1.915 | PDE Loss:  -3.7826 | Function Loss:  -1.9783\n",
      "Total loss:  -1.9164 | PDE Loss:  -3.7834 | Function Loss:  -1.9798\n",
      "Total loss:  -1.9181 | PDE Loss:  -3.7849 | Function Loss:  -1.9815\n",
      "Total loss:  -1.9201 | PDE Loss:  -3.7879 | Function Loss:  -1.9834\n",
      "Total loss:  -1.9225 | PDE Loss:  -3.7962 | Function Loss:  -1.9849\n",
      "Total loss:  -1.925 | PDE Loss:  -3.7958 | Function Loss:  -1.9878\n",
      "Total loss:  -1.9274 | PDE Loss:  -3.8125 | Function Loss:  -1.988\n",
      "Total loss:  -1.9287 | PDE Loss:  -3.8166 | Function Loss:  -1.9889\n",
      "Total loss:  -1.9301 | PDE Loss:  -3.8081 | Function Loss:  -1.9918\n",
      "Total loss:  -1.9309 | PDE Loss:  -3.8089 | Function Loss:  -1.9927\n",
      "Total loss:  -1.9315 | PDE Loss:  -3.8136 | Function Loss:  -1.9926\n",
      "Total loss:  -1.9322 | PDE Loss:  -3.8098 | Function Loss:  -1.994\n",
      "Total loss:  -1.9329 | PDE Loss:  -3.8096 | Function Loss:  -1.9948\n",
      "Total loss:  -1.9337 | PDE Loss:  -3.8065 | Function Loss:  -1.9962\n",
      "Total loss:  -1.9345 | PDE Loss:  -3.8095 | Function Loss:  -1.9967\n",
      "Total loss:  -1.9354 | PDE Loss:  -3.8079 | Function Loss:  -1.9979\n",
      "Total loss:  -1.9364 | PDE Loss:  -3.8073 | Function Loss:  -1.9992\n",
      "Total loss:  -1.9374 | PDE Loss:  -3.8135 | Function Loss:  -1.9994\n",
      "Total loss:  -1.9383 | PDE Loss:  -3.8141 | Function Loss:  -2.0003\n",
      "Total loss:  -1.9391 | PDE Loss:  -3.8151 | Function Loss:  -2.0011\n",
      "Total loss:  -1.9397 | PDE Loss:  -3.822 | Function Loss:  -2.0007\n",
      "Total loss:  -1.94 | PDE Loss:  -3.8233 | Function Loss:  -2.0009\n",
      "Total loss:  -1.9403 | PDE Loss:  -3.8238 | Function Loss:  -2.0012\n",
      "Total loss:  -1.9407 | PDE Loss:  -3.8258 | Function Loss:  -2.0014\n",
      "Total loss:  -1.9411 | PDE Loss:  -3.8275 | Function Loss:  -2.0015\n",
      "Total loss:  -1.9414 | PDE Loss:  -3.8276 | Function Loss:  -2.0019\n",
      "Total loss:  -1.9418 | PDE Loss:  -3.8282 | Function Loss:  -2.0023\n",
      "Total loss:  -1.9425 | PDE Loss:  -3.829 | Function Loss:  -2.0029\n",
      "Total loss:  -1.9431 | PDE Loss:  -3.8281 | Function Loss:  -2.0037\n",
      "Total loss:  -1.9438 | PDE Loss:  -3.826 | Function Loss:  -2.0049\n",
      "Total loss:  -1.9446 | PDE Loss:  -3.8249 | Function Loss:  -2.006\n",
      "Total loss:  -1.9454 | PDE Loss:  -3.8205 | Function Loss:  -2.0075\n",
      "Total loss:  -1.9461 | PDE Loss:  -3.8188 | Function Loss:  -2.0086\n",
      "Total loss:  -1.9468 | PDE Loss:  -3.8162 | Function Loss:  -2.0098\n",
      "Total loss:  -1.9476 | PDE Loss:  -3.8139 | Function Loss:  -2.0111\n",
      "Total loss:  -1.9482 | PDE Loss:  -3.8138 | Function Loss:  -2.0118\n",
      "Total loss:  -1.9487 | PDE Loss:  -3.8104 | Function Loss:  -2.013\n",
      "Total loss:  -1.9491 | PDE Loss:  -3.8116 | Function Loss:  -2.0132\n",
      "Total loss:  -1.9494 | PDE Loss:  -3.8115 | Function Loss:  -2.0136\n",
      "Total loss:  -1.9498 | PDE Loss:  -3.8106 | Function Loss:  -2.0142\n",
      "Total loss:  -1.9503 | PDE Loss:  -3.8134 | Function Loss:  -2.0143\n",
      "Total loss:  -1.9508 | PDE Loss:  -3.813 | Function Loss:  -2.015\n",
      "Total loss:  -1.9515 | PDE Loss:  -3.8151 | Function Loss:  -2.0154\n",
      "Total loss:  -1.9522 | PDE Loss:  -3.8153 | Function Loss:  -2.0162\n",
      "Total loss:  -1.953 | PDE Loss:  -3.814 | Function Loss:  -2.0173\n",
      "Total loss:  -1.9538 | PDE Loss:  -3.8129 | Function Loss:  -2.0184\n",
      "Total loss:  -1.9545 | PDE Loss:  -3.81 | Function Loss:  -2.0198\n",
      "Total loss:  -1.9554 | PDE Loss:  -3.8086 | Function Loss:  -2.021\n",
      "Total loss:  -1.9565 | PDE Loss:  -3.8061 | Function Loss:  -2.0227\n",
      "Total loss:  -1.9577 | PDE Loss:  -3.805 | Function Loss:  -2.0242\n",
      "Total loss:  -1.9588 | PDE Loss:  -3.8059 | Function Loss:  -2.0254\n",
      "Total loss:  -1.9599 | PDE Loss:  -3.8051 | Function Loss:  -2.0268\n",
      "Total loss:  -1.9609 | PDE Loss:  -3.8089 | Function Loss:  -2.0274\n",
      "Total loss:  -1.9619 | PDE Loss:  -3.8086 | Function Loss:  -2.0285\n",
      "Total loss:  -1.9624 | PDE Loss:  -3.8111 | Function Loss:  -2.0287\n",
      "Total loss:  -1.9632 | PDE Loss:  -3.814 | Function Loss:  -2.0292\n",
      "Total loss:  -1.9641 | PDE Loss:  -3.8159 | Function Loss:  -2.0299\n",
      "Total loss:  -1.9647 | PDE Loss:  -3.8219 | Function Loss:  -2.0296\n",
      "Total loss:  -1.9655 | PDE Loss:  -3.8194 | Function Loss:  -2.031\n",
      "Total loss:  -1.9666 | PDE Loss:  -3.8244 | Function Loss:  -2.0315\n",
      "Total loss:  -1.9677 | PDE Loss:  -3.8213 | Function Loss:  -2.0333\n",
      "Total loss:  -1.9686 | PDE Loss:  -3.8201 | Function Loss:  -2.0345\n",
      "Total loss:  -1.9694 | PDE Loss:  -3.8182 | Function Loss:  -2.0357\n",
      "Total loss:  -1.97 | PDE Loss:  -3.8186 | Function Loss:  -2.0363\n",
      "Total loss:  -1.9704 | PDE Loss:  -3.8174 | Function Loss:  -2.037\n",
      "Total loss:  -1.971 | PDE Loss:  -3.818 | Function Loss:  -2.0377\n",
      "Total loss:  -1.9719 | PDE Loss:  -3.821 | Function Loss:  -2.0382\n",
      "Total loss:  -1.9728 | PDE Loss:  -3.823 | Function Loss:  -2.0389\n",
      "Total loss:  -1.9737 | PDE Loss:  -3.8288 | Function Loss:  -2.039\n",
      "Total loss:  -1.9746 | PDE Loss:  -3.831 | Function Loss:  -2.0397\n",
      "Total loss:  -1.9753 | PDE Loss:  -3.8362 | Function Loss:  -2.0397\n",
      "Total loss:  -1.9761 | PDE Loss:  -3.8375 | Function Loss:  -2.0404\n",
      "Total loss:  -1.9768 | PDE Loss:  -3.8378 | Function Loss:  -2.0411\n",
      "Total loss:  -1.9775 | PDE Loss:  -3.838 | Function Loss:  -2.0419\n",
      "Total loss:  -1.978 | PDE Loss:  -3.8347 | Function Loss:  -2.0431\n",
      "Total loss:  -1.9785 | PDE Loss:  -3.8327 | Function Loss:  -2.0439\n",
      "Total loss:  -1.979 | PDE Loss:  -3.8302 | Function Loss:  -2.0449\n",
      "Total loss:  -1.9795 | PDE Loss:  -3.8274 | Function Loss:  -2.046\n",
      "Total loss:  -1.9801 | PDE Loss:  -3.8269 | Function Loss:  -2.0468\n",
      "Total loss:  -1.9806 | PDE Loss:  -3.8266 | Function Loss:  -2.0474\n",
      "Total loss:  -1.9812 | PDE Loss:  -3.8285 | Function Loss:  -2.0478\n",
      "Total loss:  -1.9818 | PDE Loss:  -3.8307 | Function Loss:  -2.0481\n",
      "Total loss:  -1.9824 | PDE Loss:  -3.8343 | Function Loss:  -2.0482\n",
      "Total loss:  -1.9831 | PDE Loss:  -3.839 | Function Loss:  -2.0482\n",
      "Total loss:  -1.9838 | PDE Loss:  -3.8407 | Function Loss:  -2.0488\n",
      "Total loss:  -1.9846 | PDE Loss:  -3.8444 | Function Loss:  -2.0491\n",
      "Total loss:  -1.9856 | PDE Loss:  -3.8477 | Function Loss:  -2.0498\n",
      "Total loss:  -1.9871 | PDE Loss:  -3.8503 | Function Loss:  -2.0511\n",
      "Total loss:  -1.9886 | PDE Loss:  -3.8499 | Function Loss:  -2.0529\n",
      "Total loss:  -1.9898 | PDE Loss:  -3.8493 | Function Loss:  -2.0544\n",
      "Total loss:  -1.9908 | PDE Loss:  -3.8446 | Function Loss:  -2.0564\n",
      "Total loss:  -1.9915 | PDE Loss:  -3.8454 | Function Loss:  -2.057\n",
      "Total loss:  -1.9923 | PDE Loss:  -3.8439 | Function Loss:  -2.0582\n",
      "Total loss:  -1.9931 | PDE Loss:  -3.8426 | Function Loss:  -2.0593\n",
      "Total loss:  -1.9939 | PDE Loss:  -3.8388 | Function Loss:  -2.0609\n",
      "Total loss:  -1.9945 | PDE Loss:  -3.8361 | Function Loss:  -2.0621\n",
      "Total loss:  -1.9952 | PDE Loss:  -3.8344 | Function Loss:  -2.0631\n",
      "Total loss:  -1.9959 | PDE Loss:  -3.8336 | Function Loss:  -2.064\n",
      "Total loss:  -1.9965 | PDE Loss:  -3.8341 | Function Loss:  -2.0647\n",
      "Total loss:  -1.9973 | PDE Loss:  -3.8382 | Function Loss:  -2.0649\n",
      "Total loss:  -1.9982 | PDE Loss:  -3.8416 | Function Loss:  -2.0654\n",
      "Total loss:  -1.9991 | PDE Loss:  -3.8484 | Function Loss:  -2.0653\n",
      "Total loss:  -2.0009 | PDE Loss:  -3.8643 | Function Loss:  -2.0648\n",
      "Total loss:  -2.0025 | PDE Loss:  -3.878 | Function Loss:  -2.0646\n",
      "Total loss:  -2.0042 | PDE Loss:  -3.8944 | Function Loss:  -2.064\n",
      "Total loss:  -2.0058 | PDE Loss:  -3.9073 | Function Loss:  -2.064\n",
      "Total loss:  -2.0073 | PDE Loss:  -3.92 | Function Loss:  -2.0639\n",
      "Total loss:  -2.0087 | PDE Loss:  -3.9368 | Function Loss:  -2.0633\n",
      "Total loss:  -2.0098 | PDE Loss:  -3.9432 | Function Loss:  -2.0637\n",
      "Total loss:  -2.0109 | PDE Loss:  -3.9494 | Function Loss:  -2.0641\n",
      "Total loss:  -2.0121 | PDE Loss:  -3.9563 | Function Loss:  -2.0645\n",
      "Total loss:  -2.0128 | PDE Loss:  -3.958 | Function Loss:  -2.0651\n",
      "Total loss:  -2.0133 | PDE Loss:  -3.9616 | Function Loss:  -2.0651\n",
      "Total loss:  -2.0136 | PDE Loss:  -3.9629 | Function Loss:  -2.0654\n",
      "Total loss:  -2.014 | PDE Loss:  -3.965 | Function Loss:  -2.0655\n",
      "Total loss:  -2.0144 | PDE Loss:  -3.9664 | Function Loss:  -2.0659\n",
      "Total loss:  -2.0149 | PDE Loss:  -3.967 | Function Loss:  -2.0663\n",
      "Total loss:  -2.0152 | PDE Loss:  -3.9678 | Function Loss:  -2.0666\n",
      "Total loss:  -2.0156 | PDE Loss:  -3.9688 | Function Loss:  -2.0669\n",
      "Total loss:  -2.016 | PDE Loss:  -3.9671 | Function Loss:  -2.0675\n",
      "Total loss:  -2.0163 | PDE Loss:  -3.9686 | Function Loss:  -2.0676\n",
      "Total loss:  -2.0165 | PDE Loss:  -3.967 | Function Loss:  -2.0682\n",
      "Total loss:  -2.0169 | PDE Loss:  -3.9657 | Function Loss:  -2.0687\n",
      "Total loss:  -2.0173 | PDE Loss:  -3.9644 | Function Loss:  -2.0694\n",
      "Total loss:  -2.0178 | PDE Loss:  -3.9612 | Function Loss:  -2.0704\n",
      "Total loss:  -2.0183 | PDE Loss:  -3.9617 | Function Loss:  -2.0708\n",
      "Total loss:  -2.0188 | PDE Loss:  -3.9627 | Function Loss:  -2.0713\n",
      "Total loss:  -2.0192 | PDE Loss:  -3.9627 | Function Loss:  -2.0718\n",
      "Total loss:  -2.0199 | PDE Loss:  -3.9691 | Function Loss:  -2.0717\n",
      "Total loss:  -2.0207 | PDE Loss:  -3.9716 | Function Loss:  -2.0722\n",
      "Total loss:  -2.0215 | PDE Loss:  -3.9801 | Function Loss:  -2.0721\n",
      "Total loss:  -2.0224 | PDE Loss:  -3.9877 | Function Loss:  -2.0722\n",
      "Total loss:  -2.0236 | PDE Loss:  -3.9981 | Function Loss:  -2.0723\n",
      "Total loss:  -2.0249 | PDE Loss:  -4.0125 | Function Loss:  -2.072\n",
      "Total loss:  -2.026 | PDE Loss:  -4.0223 | Function Loss:  -2.0721\n",
      "Total loss:  -2.0269 | PDE Loss:  -4.0318 | Function Loss:  -2.0721\n",
      "Total loss:  -2.0276 | PDE Loss:  -4.0395 | Function Loss:  -2.072\n",
      "Total loss:  -2.0281 | PDE Loss:  -4.0393 | Function Loss:  -2.0726\n",
      "Total loss:  -2.0284 | PDE Loss:  -4.0427 | Function Loss:  -2.0726\n",
      "Total loss:  -2.0287 | PDE Loss:  -4.0427 | Function Loss:  -2.073\n",
      "Total loss:  -2.0293 | PDE Loss:  -4.0422 | Function Loss:  -2.0737\n",
      "Total loss:  -2.0302 | PDE Loss:  -4.0413 | Function Loss:  -2.0747\n",
      "Total loss:  -2.0313 | PDE Loss:  -4.0407 | Function Loss:  -2.076\n",
      "Total loss:  -2.0323 | PDE Loss:  -4.0412 | Function Loss:  -2.0771\n",
      "Total loss:  -2.0334 | PDE Loss:  -4.0384 | Function Loss:  -2.0787\n",
      "Total loss:  -2.0346 | PDE Loss:  -4.0372 | Function Loss:  -2.0801\n",
      "Total loss:  -2.0357 | PDE Loss:  -4.0382 | Function Loss:  -2.0811\n",
      "Total loss:  -2.0366 | PDE Loss:  -4.0341 | Function Loss:  -2.0827\n",
      "Total loss:  -2.0374 | PDE Loss:  -4.0407 | Function Loss:  -2.0828\n",
      "Total loss:  -2.038 | PDE Loss:  -4.0411 | Function Loss:  -2.0835\n",
      "Total loss:  -2.0385 | PDE Loss:  -4.0481 | Function Loss:  -2.0832\n",
      "Total loss:  -2.0388 | PDE Loss:  -4.0522 | Function Loss:  -2.0831\n",
      "Total loss:  -2.039 | PDE Loss:  -4.0537 | Function Loss:  -2.0832\n",
      "Total loss:  -2.0392 | PDE Loss:  -4.0555 | Function Loss:  -2.0831\n",
      "Total loss:  -2.0393 | PDE Loss:  -4.0573 | Function Loss:  -2.0831\n",
      "Total loss:  -2.0394 | PDE Loss:  -4.0563 | Function Loss:  -2.0833\n",
      "Total loss:  -2.0396 | PDE Loss:  -4.0558 | Function Loss:  -2.0836\n",
      "Total loss:  -2.0397 | PDE Loss:  -4.053 | Function Loss:  -2.0841\n",
      "Total loss:  -2.04 | PDE Loss:  -4.0503 | Function Loss:  -2.0846\n",
      "Total loss:  -2.0403 | PDE Loss:  -4.0448 | Function Loss:  -2.0856\n",
      "Total loss:  -2.0406 | PDE Loss:  -4.0436 | Function Loss:  -2.0861\n",
      "Total loss:  -2.0409 | PDE Loss:  -4.039 | Function Loss:  -2.0869\n",
      "Total loss:  -2.0412 | PDE Loss:  -4.0384 | Function Loss:  -2.0873\n",
      "Total loss:  -2.0416 | PDE Loss:  -4.0337 | Function Loss:  -2.0882\n",
      "Total loss:  -2.0419 | PDE Loss:  -4.0371 | Function Loss:  -2.0882\n",
      "Total loss:  -2.0421 | PDE Loss:  -4.0369 | Function Loss:  -2.0884\n",
      "Total loss:  -2.0423 | PDE Loss:  -4.038 | Function Loss:  -2.0886\n",
      "Total loss:  -2.0426 | PDE Loss:  -4.0391 | Function Loss:  -2.0888\n",
      "Total loss:  -2.043 | PDE Loss:  -4.0404 | Function Loss:  -2.0891\n",
      "Total loss:  -2.0435 | PDE Loss:  -4.0419 | Function Loss:  -2.0894\n",
      "Total loss:  -2.044 | PDE Loss:  -4.0433 | Function Loss:  -2.0899\n",
      "Total loss:  -2.0447 | PDE Loss:  -4.0455 | Function Loss:  -2.0904\n",
      "Total loss:  -2.0453 | PDE Loss:  -4.0453 | Function Loss:  -2.0911\n",
      "Total loss:  -2.0459 | PDE Loss:  -4.0466 | Function Loss:  -2.0916\n",
      "Total loss:  -2.0464 | PDE Loss:  -4.045 | Function Loss:  -2.0923\n",
      "Total loss:  -2.0469 | PDE Loss:  -4.0462 | Function Loss:  -2.0927\n",
      "Total loss:  -2.0473 | PDE Loss:  -4.0452 | Function Loss:  -2.0933\n",
      "Total loss:  -2.0477 | PDE Loss:  -4.0449 | Function Loss:  -2.0937\n",
      "Total loss:  -2.0482 | PDE Loss:  -4.0471 | Function Loss:  -2.0941\n",
      "Total loss:  -2.0488 | PDE Loss:  -4.0466 | Function Loss:  -2.0948\n",
      "Total loss:  -2.0494 | PDE Loss:  -4.0485 | Function Loss:  -2.0953\n",
      "Total loss:  -2.0501 | PDE Loss:  -4.0513 | Function Loss:  -2.0958\n",
      "Total loss:  -2.0509 | PDE Loss:  -4.0512 | Function Loss:  -2.0966\n",
      "Total loss:  -2.0517 | PDE Loss:  -4.0538 | Function Loss:  -2.0973\n",
      "Total loss:  -2.0526 | PDE Loss:  -4.0525 | Function Loss:  -2.0983\n",
      "Total loss:  -2.0534 | PDE Loss:  -4.0527 | Function Loss:  -2.0992\n",
      "Total loss:  -2.0541 | PDE Loss:  -4.0541 | Function Loss:  -2.0998\n",
      "Total loss:  -2.0547 | PDE Loss:  -4.0525 | Function Loss:  -2.1008\n",
      "Total loss:  -2.0555 | PDE Loss:  -4.0525 | Function Loss:  -2.1016\n",
      "Total loss:  -2.0563 | PDE Loss:  -4.0483 | Function Loss:  -2.103\n",
      "Total loss:  -2.0571 | PDE Loss:  -4.047 | Function Loss:  -2.104\n",
      "Total loss:  -2.0577 | PDE Loss:  -4.0423 | Function Loss:  -2.1052\n",
      "Total loss:  -2.0582 | PDE Loss:  -4.0407 | Function Loss:  -2.106\n",
      "Total loss:  -2.0588 | PDE Loss:  -4.0396 | Function Loss:  -2.1068\n",
      "Total loss:  -2.0596 | PDE Loss:  -4.0383 | Function Loss:  -2.1078\n",
      "Total loss:  -2.0605 | PDE Loss:  -4.0408 | Function Loss:  -2.1085\n",
      "Total loss:  -2.0612 | PDE Loss:  -4.0429 | Function Loss:  -2.1091\n",
      "Total loss:  -2.0624 | PDE Loss:  -4.0467 | Function Loss:  -2.1099\n",
      "Total loss:  -2.0635 | PDE Loss:  -4.0498 | Function Loss:  -2.1108\n",
      "Total loss:  -2.0645 | PDE Loss:  -4.0547 | Function Loss:  -2.1114\n",
      "Total loss:  -2.0654 | PDE Loss:  -4.0535 | Function Loss:  -2.1125\n",
      "Total loss:  -2.0661 | PDE Loss:  -4.0574 | Function Loss:  -2.1128\n",
      "Total loss:  -2.0668 | PDE Loss:  -4.0568 | Function Loss:  -2.1137\n",
      "Total loss:  -2.0675 | PDE Loss:  -4.0559 | Function Loss:  -2.1146\n",
      "Total loss:  -2.0682 | PDE Loss:  -4.0547 | Function Loss:  -2.1154\n",
      "Total loss:  -2.0689 | PDE Loss:  -4.0509 | Function Loss:  -2.1167\n",
      "Total loss:  -2.0698 | PDE Loss:  -4.0466 | Function Loss:  -2.1182\n",
      "Total loss:  -2.0708 | PDE Loss:  -4.0343 | Function Loss:  -2.1208\n",
      "Total loss:  -2.072 | PDE Loss:  -4.0316 | Function Loss:  -2.1225\n",
      "Total loss:  -2.0728 | PDE Loss:  -4.0276 | Function Loss:  -2.1239\n",
      "Total loss:  -2.0738 | PDE Loss:  -4.0286 | Function Loss:  -2.1249\n",
      "Total loss:  -2.0746 | PDE Loss:  -4.0294 | Function Loss:  -2.1257\n",
      "Total loss:  -2.0752 | PDE Loss:  -4.0335 | Function Loss:  -2.1259\n",
      "Total loss:  -2.0757 | PDE Loss:  -4.035 | Function Loss:  -2.1263\n",
      "Total loss:  -2.0762 | PDE Loss:  -4.0429 | Function Loss:  -2.1259\n",
      "Total loss:  -2.0767 | PDE Loss:  -4.0448 | Function Loss:  -2.1261\n",
      "Total loss:  -2.0772 | PDE Loss:  -4.0505 | Function Loss:  -2.126\n",
      "Total loss:  -2.0776 | PDE Loss:  -4.0535 | Function Loss:  -2.1262\n",
      "Total loss:  -2.078 | PDE Loss:  -4.0561 | Function Loss:  -2.1263\n",
      "Total loss:  -2.0783 | PDE Loss:  -4.06 | Function Loss:  -2.1262\n",
      "Total loss:  -2.0787 | PDE Loss:  -4.0604 | Function Loss:  -2.1266\n",
      "Total loss:  -2.0792 | PDE Loss:  -4.062 | Function Loss:  -2.1269\n",
      "Total loss:  -2.0797 | PDE Loss:  -4.0647 | Function Loss:  -2.1272\n",
      "Total loss:  -2.0803 | PDE Loss:  -4.0598 | Function Loss:  -2.1284\n",
      "Total loss:  -2.0807 | PDE Loss:  -4.059 | Function Loss:  -2.1289\n",
      "Total loss:  -2.0811 | PDE Loss:  -4.0584 | Function Loss:  -2.1295\n",
      "Total loss:  -2.0815 | PDE Loss:  -4.0544 | Function Loss:  -2.1304\n",
      "Total loss:  -2.0817 | PDE Loss:  -4.0537 | Function Loss:  -2.1307\n",
      "Total loss:  -2.082 | PDE Loss:  -4.0531 | Function Loss:  -2.1311\n",
      "Total loss:  -2.0824 | PDE Loss:  -4.0549 | Function Loss:  -2.1313\n",
      "Total loss:  -2.0827 | PDE Loss:  -4.0566 | Function Loss:  -2.1314\n",
      "Total loss:  -2.083 | PDE Loss:  -4.0603 | Function Loss:  -2.1313\n",
      "Total loss:  -2.0833 | PDE Loss:  -4.0649 | Function Loss:  -2.1311\n",
      "Total loss:  -2.0837 | PDE Loss:  -4.0706 | Function Loss:  -2.131\n",
      "Total loss:  -2.0843 | PDE Loss:  -4.0793 | Function Loss:  -2.1306\n",
      "Total loss:  -2.0849 | PDE Loss:  -4.0877 | Function Loss:  -2.1303\n",
      "Total loss:  -2.0856 | PDE Loss:  -4.0969 | Function Loss:  -2.1301\n",
      "Total loss:  -2.0864 | PDE Loss:  -4.108 | Function Loss:  -2.1298\n",
      "Total loss:  -2.0873 | PDE Loss:  -4.1129 | Function Loss:  -2.1303\n",
      "Total loss:  -2.0884 | PDE Loss:  -4.1318 | Function Loss:  -2.1296\n",
      "Total loss:  -2.0894 | PDE Loss:  -4.1314 | Function Loss:  -2.1307\n",
      "Total loss:  -2.0907 | PDE Loss:  -4.1337 | Function Loss:  -2.132\n",
      "Total loss:  -2.0921 | PDE Loss:  -4.1287 | Function Loss:  -2.1339\n",
      "Total loss:  -2.0932 | PDE Loss:  -4.1281 | Function Loss:  -2.1353\n",
      "Total loss:  -2.0942 | PDE Loss:  -4.1246 | Function Loss:  -2.1367\n",
      "Total loss:  -2.0949 | PDE Loss:  -4.1232 | Function Loss:  -2.1376\n",
      "Total loss:  -2.0957 | PDE Loss:  -4.1249 | Function Loss:  -2.1384\n",
      "Total loss:  -2.0966 | PDE Loss:  -4.1232 | Function Loss:  -2.1395\n",
      "Total loss:  -2.0974 | PDE Loss:  -4.1281 | Function Loss:  -2.1399\n",
      "Total loss:  -2.098 | PDE Loss:  -4.129 | Function Loss:  -2.1404\n",
      "Total loss:  -2.0988 | PDE Loss:  -4.1294 | Function Loss:  -2.1413\n",
      "Total loss:  -2.0998 | PDE Loss:  -4.1276 | Function Loss:  -2.1426\n",
      "Total loss:  -2.1007 | PDE Loss:  -4.1247 | Function Loss:  -2.1439\n",
      "Total loss:  -2.1013 | PDE Loss:  -4.1183 | Function Loss:  -2.1452\n",
      "Total loss:  -2.1018 | PDE Loss:  -4.1154 | Function Loss:  -2.146\n",
      "Total loss:  -2.1022 | PDE Loss:  -4.1086 | Function Loss:  -2.1473\n",
      "Total loss:  -2.1027 | PDE Loss:  -4.1041 | Function Loss:  -2.1483\n",
      "Total loss:  -2.1033 | PDE Loss:  -4.0996 | Function Loss:  -2.1495\n",
      "Total loss:  -2.1041 | PDE Loss:  -4.0955 | Function Loss:  -2.1508\n",
      "Total loss:  -2.1051 | PDE Loss:  -4.0954 | Function Loss:  -2.1519\n",
      "Total loss:  -2.1059 | PDE Loss:  -4.0937 | Function Loss:  -2.1531\n",
      "Total loss:  -2.107 | PDE Loss:  -4.0972 | Function Loss:  -2.1538\n",
      "Total loss:  -2.1082 | PDE Loss:  -4.0954 | Function Loss:  -2.1554\n",
      "Total loss:  -2.1092 | PDE Loss:  -4.0999 | Function Loss:  -2.156\n",
      "Total loss:  -2.1103 | PDE Loss:  -4.1019 | Function Loss:  -2.157\n",
      "Total loss:  -2.1113 | PDE Loss:  -4.1094 | Function Loss:  -2.1573\n",
      "Total loss:  -2.1122 | PDE Loss:  -4.1061 | Function Loss:  -2.1586\n",
      "Total loss:  -2.1127 | PDE Loss:  -4.111 | Function Loss:  -2.1587\n",
      "Total loss:  -2.1131 | PDE Loss:  -4.1098 | Function Loss:  -2.1593\n",
      "Total loss:  -2.1135 | PDE Loss:  -4.1099 | Function Loss:  -2.1597\n",
      "Total loss:  -2.1138 | PDE Loss:  -4.1102 | Function Loss:  -2.16\n",
      "Total loss:  -2.114 | PDE Loss:  -4.1075 | Function Loss:  -2.1605\n",
      "Total loss:  -2.1141 | PDE Loss:  -4.1097 | Function Loss:  -2.1604\n",
      "Total loss:  -2.1143 | PDE Loss:  -4.109 | Function Loss:  -2.1607\n",
      "Total loss:  -2.1145 | PDE Loss:  -4.1119 | Function Loss:  -2.1606\n",
      "Total loss:  -2.1148 | PDE Loss:  -4.1131 | Function Loss:  -2.1607\n",
      "Total loss:  -2.1151 | PDE Loss:  -4.1137 | Function Loss:  -2.161\n",
      "Total loss:  -2.1154 | PDE Loss:  -4.1152 | Function Loss:  -2.1612\n",
      "Total loss:  -2.1158 | PDE Loss:  -4.1177 | Function Loss:  -2.1613\n",
      "Total loss:  -2.1161 | PDE Loss:  -4.1194 | Function Loss:  -2.1615\n",
      "Total loss:  -2.1166 | PDE Loss:  -4.119 | Function Loss:  -2.1621\n",
      "Total loss:  -2.1171 | PDE Loss:  -4.12 | Function Loss:  -2.1625\n",
      "Total loss:  -2.1176 | PDE Loss:  -4.1197 | Function Loss:  -2.1631\n",
      "Total loss:  -2.1182 | PDE Loss:  -4.1222 | Function Loss:  -2.1635\n",
      "Total loss:  -2.1188 | PDE Loss:  -4.1227 | Function Loss:  -2.1642\n",
      "Total loss:  -2.1195 | PDE Loss:  -4.1243 | Function Loss:  -2.1647\n",
      "Total loss:  -2.1202 | PDE Loss:  -4.1271 | Function Loss:  -2.1652\n",
      "Total loss:  -2.121 | PDE Loss:  -4.129 | Function Loss:  -2.1658\n",
      "Total loss:  -2.1217 | PDE Loss:  -4.133 | Function Loss:  -2.1662\n",
      "Total loss:  -2.1224 | PDE Loss:  -4.1342 | Function Loss:  -2.1669\n",
      "Total loss:  -2.1232 | PDE Loss:  -4.1389 | Function Loss:  -2.1672\n",
      "Total loss:  -2.1242 | PDE Loss:  -4.1402 | Function Loss:  -2.1682\n",
      "Total loss:  -2.1249 | PDE Loss:  -4.1444 | Function Loss:  -2.1686\n",
      "Total loss:  -2.1257 | PDE Loss:  -4.1468 | Function Loss:  -2.1692\n",
      "Total loss:  -2.1266 | PDE Loss:  -4.1474 | Function Loss:  -2.1701\n",
      "Total loss:  -2.1272 | PDE Loss:  -4.1487 | Function Loss:  -2.1707\n",
      "Total loss:  -2.1279 | PDE Loss:  -4.1476 | Function Loss:  -2.1715\n",
      "Total loss:  -2.1287 | PDE Loss:  -4.1459 | Function Loss:  -2.1726\n",
      "Total loss:  -2.1298 | PDE Loss:  -4.1438 | Function Loss:  -2.174\n",
      "Total loss:  -2.1311 | PDE Loss:  -4.138 | Function Loss:  -2.1761\n",
      "Total loss:  -2.1324 | PDE Loss:  -4.1329 | Function Loss:  -2.1781\n",
      "Total loss:  -2.1336 | PDE Loss:  -4.1267 | Function Loss:  -2.1801\n",
      "Total loss:  -2.1345 | PDE Loss:  -4.1174 | Function Loss:  -2.1822\n",
      "Total loss:  -2.1353 | PDE Loss:  -4.1209 | Function Loss:  -2.1827\n",
      "Total loss:  -2.1358 | PDE Loss:  -4.1141 | Function Loss:  -2.184\n",
      "Total loss:  -2.1362 | PDE Loss:  -4.1123 | Function Loss:  -2.1847\n",
      "Total loss:  -2.1367 | PDE Loss:  -4.1125 | Function Loss:  -2.1853\n",
      "Total loss:  -2.1371 | PDE Loss:  -4.1076 | Function Loss:  -2.1863\n",
      "Total loss:  -2.1375 | PDE Loss:  -4.1031 | Function Loss:  -2.1872\n",
      "Total loss:  -2.1381 | PDE Loss:  -4.0962 | Function Loss:  -2.1887\n",
      "Total loss:  -2.1387 | PDE Loss:  -4.0883 | Function Loss:  -2.1905\n",
      "Total loss:  -2.1396 | PDE Loss:  -4.0815 | Function Loss:  -2.1923\n",
      "Total loss:  -2.1404 | PDE Loss:  -4.0715 | Function Loss:  -2.1946\n",
      "Total loss:  -2.1411 | PDE Loss:  -4.068 | Function Loss:  -2.1958\n",
      "Total loss:  -2.1419 | PDE Loss:  -4.0642 | Function Loss:  -2.1972\n",
      "Total loss:  -2.1427 | PDE Loss:  -4.0653 | Function Loss:  -2.1979\n",
      "Total loss:  -2.1434 | PDE Loss:  -4.0638 | Function Loss:  -2.199\n",
      "Total loss:  -2.1442 | PDE Loss:  -4.0685 | Function Loss:  -2.1992\n",
      "Total loss:  -2.1448 | PDE Loss:  -4.0742 | Function Loss:  -2.1992\n",
      "Total loss:  -2.1456 | PDE Loss:  -4.0824 | Function Loss:  -2.1989\n",
      "Total loss:  -2.1463 | PDE Loss:  -4.0917 | Function Loss:  -2.1985\n",
      "Total loss:  -2.1468 | PDE Loss:  -4.1034 | Function Loss:  -2.1977\n",
      "Total loss:  -2.1474 | PDE Loss:  -4.1105 | Function Loss:  -2.1974\n",
      "Total loss:  -2.1478 | PDE Loss:  -4.1175 | Function Loss:  -2.197\n",
      "Total loss:  -2.1482 | PDE Loss:  -4.1208 | Function Loss:  -2.1972\n",
      "Total loss:  -2.1489 | PDE Loss:  -4.1207 | Function Loss:  -2.1979\n",
      "Total loss:  -2.1494 | PDE Loss:  -4.1179 | Function Loss:  -2.1988\n",
      "Total loss:  -2.1499 | PDE Loss:  -4.1218 | Function Loss:  -2.1989\n",
      "Total loss:  -2.1503 | PDE Loss:  -4.12 | Function Loss:  -2.1996\n",
      "Total loss:  -2.1508 | PDE Loss:  -4.1157 | Function Loss:  -2.2006\n",
      "Total loss:  -2.1512 | PDE Loss:  -4.1105 | Function Loss:  -2.2017\n",
      "Total loss:  -2.1516 | PDE Loss:  -4.1054 | Function Loss:  -2.2029\n",
      "Total loss:  -2.152 | PDE Loss:  -4.1015 | Function Loss:  -2.2037\n",
      "Total loss:  -2.1524 | PDE Loss:  -4.0974 | Function Loss:  -2.2047\n",
      "Total loss:  -2.1528 | PDE Loss:  -4.0937 | Function Loss:  -2.2057\n",
      "Total loss:  -2.1533 | PDE Loss:  -4.0914 | Function Loss:  -2.2065\n",
      "Total loss:  -2.1538 | PDE Loss:  -4.0885 | Function Loss:  -2.2074\n",
      "Total loss:  -2.1545 | PDE Loss:  -4.0875 | Function Loss:  -2.2083\n",
      "Total loss:  -2.155 | PDE Loss:  -4.0773 | Function Loss:  -2.2103\n",
      "Total loss:  -2.1557 | PDE Loss:  -4.0804 | Function Loss:  -2.2107\n",
      "Total loss:  -2.1565 | PDE Loss:  -4.0826 | Function Loss:  -2.2113\n",
      "Total loss:  -2.1572 | PDE Loss:  -4.085 | Function Loss:  -2.2118\n",
      "Total loss:  -2.1579 | PDE Loss:  -4.0844 | Function Loss:  -2.2126\n",
      "Total loss:  -2.1584 | PDE Loss:  -4.0867 | Function Loss:  -2.2129\n",
      "Total loss:  -2.1588 | PDE Loss:  -4.0816 | Function Loss:  -2.2141\n",
      "Total loss:  -2.1593 | PDE Loss:  -4.0821 | Function Loss:  -2.2146\n",
      "Total loss:  -2.16 | PDE Loss:  -4.0745 | Function Loss:  -2.2164\n",
      "Total loss:  -2.1607 | PDE Loss:  -4.0699 | Function Loss:  -2.2178\n",
      "Total loss:  -2.1614 | PDE Loss:  -4.0662 | Function Loss:  -2.2192\n",
      "Total loss:  -2.1624 | PDE Loss:  -4.0622 | Function Loss:  -2.2209\n",
      "Total loss:  -2.1632 | PDE Loss:  -4.0627 | Function Loss:  -2.2217\n",
      "Total loss:  -2.1638 | PDE Loss:  -4.0669 | Function Loss:  -2.2218\n",
      "Total loss:  -2.1642 | PDE Loss:  -4.07 | Function Loss:  -2.2218\n",
      "Total loss:  -2.1647 | PDE Loss:  -4.0718 | Function Loss:  -2.2221\n",
      "Total loss:  -2.1651 | PDE Loss:  -4.0743 | Function Loss:  -2.2222\n",
      "Total loss:  -2.1655 | PDE Loss:  -4.0774 | Function Loss:  -2.2222\n",
      "Total loss:  -2.166 | PDE Loss:  -4.077 | Function Loss:  -2.2228\n",
      "Total loss:  -2.1665 | PDE Loss:  -4.0778 | Function Loss:  -2.2234\n",
      "Total loss:  -2.1672 | PDE Loss:  -4.0798 | Function Loss:  -2.2238\n",
      "Total loss:  -2.1679 | PDE Loss:  -4.0771 | Function Loss:  -2.225\n",
      "Total loss:  -2.1687 | PDE Loss:  -4.0763 | Function Loss:  -2.2261\n",
      "Total loss:  -2.1696 | PDE Loss:  -4.0762 | Function Loss:  -2.2271\n",
      "Total loss:  -2.1704 | PDE Loss:  -4.0747 | Function Loss:  -2.2282\n",
      "Total loss:  -2.1709 | PDE Loss:  -4.0727 | Function Loss:  -2.229\n",
      "Total loss:  -2.1713 | PDE Loss:  -4.0745 | Function Loss:  -2.2292\n",
      "Total loss:  -2.1716 | PDE Loss:  -4.0744 | Function Loss:  -2.2297\n",
      "Total loss:  -2.1722 | PDE Loss:  -4.076 | Function Loss:  -2.2301\n",
      "Total loss:  -2.173 | PDE Loss:  -4.0827 | Function Loss:  -2.2301\n",
      "Total loss:  -2.1737 | PDE Loss:  -4.0829 | Function Loss:  -2.2309\n",
      "Total loss:  -2.1746 | PDE Loss:  -4.084 | Function Loss:  -2.2317\n",
      "Total loss:  -2.1755 | PDE Loss:  -4.0829 | Function Loss:  -2.2329\n",
      "Total loss:  -2.1764 | PDE Loss:  -4.0822 | Function Loss:  -2.234\n",
      "Total loss:  -2.1771 | PDE Loss:  -4.0796 | Function Loss:  -2.2352\n",
      "Total loss:  -2.1777 | PDE Loss:  -4.0774 | Function Loss:  -2.2362\n",
      "Total loss:  -2.1782 | PDE Loss:  -4.0759 | Function Loss:  -2.237\n",
      "Total loss:  -2.1787 | PDE Loss:  -4.0722 | Function Loss:  -2.238\n",
      "Total loss:  -2.1792 | PDE Loss:  -4.0719 | Function Loss:  -2.2387\n",
      "Total loss:  -2.1799 | PDE Loss:  -4.0675 | Function Loss:  -2.2401\n",
      "Total loss:  -2.1805 | PDE Loss:  -4.0687 | Function Loss:  -2.2406\n",
      "Total loss:  -2.181 | PDE Loss:  -4.0675 | Function Loss:  -2.2414\n",
      "Total loss:  -2.1814 | PDE Loss:  -4.0673 | Function Loss:  -2.242\n",
      "Total loss:  -2.1821 | PDE Loss:  -4.0683 | Function Loss:  -2.2425\n",
      "Total loss:  -2.1828 | PDE Loss:  -4.0674 | Function Loss:  -2.2435\n",
      "Total loss:  -2.1834 | PDE Loss:  -4.0655 | Function Loss:  -2.2444\n",
      "Total loss:  -2.1837 | PDE Loss:  -4.0646 | Function Loss:  -2.245\n",
      "Total loss:  -2.1841 | PDE Loss:  -4.0615 | Function Loss:  -2.2459\n",
      "Total loss:  -2.1846 | PDE Loss:  -4.0583 | Function Loss:  -2.2469\n",
      "Total loss:  -2.1849 | PDE Loss:  -4.0551 | Function Loss:  -2.2479\n",
      "Total loss:  -2.1853 | PDE Loss:  -4.0513 | Function Loss:  -2.2489\n",
      "Total loss:  -2.1857 | PDE Loss:  -4.0492 | Function Loss:  -2.2496\n",
      "Total loss:  -2.1862 | PDE Loss:  -4.0481 | Function Loss:  -2.2504\n",
      "Total loss:  -2.187 | PDE Loss:  -4.0465 | Function Loss:  -2.2515\n",
      "Total loss:  -2.1879 | PDE Loss:  -4.0478 | Function Loss:  -2.2524\n",
      "Total loss:  -2.189 | PDE Loss:  -4.0477 | Function Loss:  -2.2537\n",
      "Total loss:  -2.1901 | PDE Loss:  -4.0513 | Function Loss:  -2.2544\n",
      "Total loss:  -2.1912 | PDE Loss:  -4.0539 | Function Loss:  -2.2552\n",
      "Total loss:  -2.1922 | PDE Loss:  -4.0561 | Function Loss:  -2.2561\n",
      "Total loss:  -2.1931 | PDE Loss:  -4.0595 | Function Loss:  -2.2566\n",
      "Total loss:  -2.1939 | PDE Loss:  -4.0584 | Function Loss:  -2.2577\n",
      "Total loss:  -2.1947 | PDE Loss:  -4.0603 | Function Loss:  -2.2583\n",
      "Total loss:  -2.1953 | PDE Loss:  -4.0595 | Function Loss:  -2.2592\n",
      "Total loss:  -2.1962 | PDE Loss:  -4.0568 | Function Loss:  -2.2606\n",
      "Total loss:  -2.1971 | PDE Loss:  -4.0535 | Function Loss:  -2.2622\n",
      "Total loss:  -2.198 | PDE Loss:  -4.0467 | Function Loss:  -2.2643\n",
      "Total loss:  -2.1988 | PDE Loss:  -4.0441 | Function Loss:  -2.2657\n",
      "Total loss:  -2.1995 | PDE Loss:  -4.0384 | Function Loss:  -2.2674\n",
      "Total loss:  -2.2001 | PDE Loss:  -4.0377 | Function Loss:  -2.2683\n",
      "Total loss:  -2.2007 | PDE Loss:  -4.0382 | Function Loss:  -2.2689\n",
      "Total loss:  -2.2014 | PDE Loss:  -4.041 | Function Loss:  -2.2693\n",
      "Total loss:  -2.2023 | PDE Loss:  -4.0464 | Function Loss:  -2.2694\n",
      "Total loss:  -2.2032 | PDE Loss:  -4.0543 | Function Loss:  -2.2692\n",
      "Total loss:  -2.2041 | PDE Loss:  -4.0641 | Function Loss:  -2.2686\n",
      "Total loss:  -2.2049 | PDE Loss:  -4.0733 | Function Loss:  -2.2681\n",
      "Total loss:  -2.2057 | PDE Loss:  -4.0843 | Function Loss:  -2.2673\n",
      "Total loss:  -2.2066 | PDE Loss:  -4.0909 | Function Loss:  -2.2673\n",
      "Total loss:  -2.2073 | PDE Loss:  -4.1006 | Function Loss:  -2.2667\n",
      "Total loss:  -2.2078 | PDE Loss:  -4.1013 | Function Loss:  -2.2672\n",
      "Total loss:  -2.2082 | PDE Loss:  -4.1018 | Function Loss:  -2.2675\n",
      "Total loss:  -2.2086 | PDE Loss:  -4.1001 | Function Loss:  -2.2683\n",
      "Total loss:  -2.2093 | PDE Loss:  -4.0979 | Function Loss:  -2.2695\n",
      "Total loss:  -2.2102 | PDE Loss:  -4.0956 | Function Loss:  -2.2708\n",
      "Total loss:  -2.2112 | PDE Loss:  -4.0937 | Function Loss:  -2.2722\n",
      "Total loss:  -2.2124 | PDE Loss:  -4.091 | Function Loss:  -2.274\n",
      "Total loss:  -2.2136 | PDE Loss:  -4.0932 | Function Loss:  -2.2751\n",
      "Total loss:  -2.2143 | PDE Loss:  -4.0915 | Function Loss:  -2.2762\n",
      "Total loss:  -2.2153 | PDE Loss:  -4.0913 | Function Loss:  -2.2773\n",
      "Total loss:  -2.2162 | PDE Loss:  -4.0922 | Function Loss:  -2.2782\n",
      "Total loss:  -2.2171 | PDE Loss:  -4.0922 | Function Loss:  -2.2792\n",
      "Total loss:  -2.2179 | PDE Loss:  -4.0943 | Function Loss:  -2.2798\n",
      "Total loss:  -2.2187 | PDE Loss:  -4.0963 | Function Loss:  -2.2805\n",
      "Total loss:  -2.2194 | PDE Loss:  -4.0988 | Function Loss:  -2.2809\n",
      "Total loss:  -2.2201 | PDE Loss:  -4.1026 | Function Loss:  -2.2811\n",
      "Total loss:  -2.2206 | PDE Loss:  -4.1061 | Function Loss:  -2.2812\n",
      "Total loss:  -2.2211 | PDE Loss:  -4.1102 | Function Loss:  -2.2811\n",
      "Total loss:  -2.2215 | PDE Loss:  -4.1137 | Function Loss:  -2.2811\n",
      "Total loss:  -2.2219 | PDE Loss:  -4.1175 | Function Loss:  -2.281\n",
      "Total loss:  -2.2223 | PDE Loss:  -4.12 | Function Loss:  -2.2811\n",
      "Total loss:  -2.2228 | PDE Loss:  -4.1228 | Function Loss:  -2.2812\n",
      "Total loss:  -2.2232 | PDE Loss:  -4.1236 | Function Loss:  -2.2816\n",
      "Total loss:  -2.2237 | PDE Loss:  -4.1241 | Function Loss:  -2.282\n",
      "Total loss:  -2.2242 | PDE Loss:  -4.1239 | Function Loss:  -2.2826\n",
      "Total loss:  -2.2247 | PDE Loss:  -4.1219 | Function Loss:  -2.2835\n",
      "Total loss:  -2.2253 | PDE Loss:  -4.1212 | Function Loss:  -2.2843\n",
      "Total loss:  -2.2259 | PDE Loss:  -4.118 | Function Loss:  -2.2854\n",
      "Total loss:  -2.2266 | PDE Loss:  -4.1171 | Function Loss:  -2.2864\n",
      "Total loss:  -2.2274 | PDE Loss:  -4.1149 | Function Loss:  -2.2877\n",
      "Total loss:  -2.2283 | PDE Loss:  -4.1172 | Function Loss:  -2.2884\n",
      "Total loss:  -2.2292 | PDE Loss:  -4.1159 | Function Loss:  -2.2896\n",
      "Total loss:  -2.2298 | PDE Loss:  -4.1166 | Function Loss:  -2.2902\n",
      "Total loss:  -2.2306 | PDE Loss:  -4.1179 | Function Loss:  -2.2909\n",
      "Total loss:  -2.2314 | PDE Loss:  -4.1205 | Function Loss:  -2.2914\n",
      "Total loss:  -2.2322 | PDE Loss:  -4.1235 | Function Loss:  -2.2919\n",
      "Total loss:  -2.2329 | PDE Loss:  -4.126 | Function Loss:  -2.2924\n",
      "Total loss:  -2.2337 | PDE Loss:  -4.1295 | Function Loss:  -2.2928\n",
      "Total loss:  -2.2345 | PDE Loss:  -4.1303 | Function Loss:  -2.2935\n",
      "Total loss:  -2.2352 | PDE Loss:  -4.1316 | Function Loss:  -2.2942\n",
      "Total loss:  -2.2359 | PDE Loss:  -4.1329 | Function Loss:  -2.2947\n",
      "Total loss:  -2.2366 | PDE Loss:  -4.1325 | Function Loss:  -2.2956\n",
      "Total loss:  -2.2373 | PDE Loss:  -4.1333 | Function Loss:  -2.2963\n",
      "Total loss:  -2.237 | PDE Loss:  -4.1364 | Function Loss:  -2.2955\n",
      "Total loss:  -2.2377 | PDE Loss:  -4.137 | Function Loss:  -2.2963\n",
      "Total loss:  -2.2384 | PDE Loss:  -4.1367 | Function Loss:  -2.2971\n",
      "Total loss:  -2.2392 | PDE Loss:  -4.1373 | Function Loss:  -2.2979\n",
      "Total loss:  -2.2403 | PDE Loss:  -4.1426 | Function Loss:  -2.2984\n",
      "Total loss:  -2.2414 | PDE Loss:  -4.1447 | Function Loss:  -2.2994\n",
      "Total loss:  -2.2424 | PDE Loss:  -4.1484 | Function Loss:  -2.2999\n",
      "Total loss:  -2.2432 | PDE Loss:  -4.153 | Function Loss:  -2.3002\n",
      "Total loss:  -2.2438 | PDE Loss:  -4.1537 | Function Loss:  -2.3008\n",
      "Total loss:  -2.2442 | PDE Loss:  -4.1539 | Function Loss:  -2.3012\n",
      "Total loss:  -2.2445 | PDE Loss:  -4.1558 | Function Loss:  -2.3013\n",
      "Total loss:  -2.2447 | PDE Loss:  -4.1544 | Function Loss:  -2.3018\n",
      "Total loss:  -2.2451 | PDE Loss:  -4.1537 | Function Loss:  -2.3023\n",
      "Total loss:  -2.2457 | PDE Loss:  -4.152 | Function Loss:  -2.3032\n",
      "Total loss:  -2.2463 | PDE Loss:  -4.1485 | Function Loss:  -2.3044\n",
      "Total loss:  -2.2468 | PDE Loss:  -4.1426 | Function Loss:  -2.3058\n",
      "Total loss:  -2.2475 | PDE Loss:  -4.143 | Function Loss:  -2.3066\n",
      "Total loss:  -2.2482 | PDE Loss:  -4.1416 | Function Loss:  -2.3076\n",
      "Total loss:  -2.2489 | PDE Loss:  -4.1435 | Function Loss:  -2.3081\n",
      "Total loss:  -2.2496 | PDE Loss:  -4.1453 | Function Loss:  -2.3087\n",
      "Total loss:  -2.2504 | PDE Loss:  -4.1485 | Function Loss:  -2.3091\n",
      "Total loss:  -2.2511 | PDE Loss:  -4.1538 | Function Loss:  -2.3091\n",
      "Total loss:  -2.2518 | PDE Loss:  -4.1558 | Function Loss:  -2.3097\n",
      "Total loss:  -2.2524 | PDE Loss:  -4.1606 | Function Loss:  -2.3097\n",
      "Total loss:  -2.253 | PDE Loss:  -4.1624 | Function Loss:  -2.3101\n",
      "Total loss:  -2.2539 | PDE Loss:  -4.1626 | Function Loss:  -2.3111\n",
      "Total loss:  -2.2548 | PDE Loss:  -4.1631 | Function Loss:  -2.3121\n",
      "Total loss:  -2.2556 | PDE Loss:  -4.1624 | Function Loss:  -2.3131\n",
      "Total loss:  -2.2566 | PDE Loss:  -4.1628 | Function Loss:  -2.3142\n",
      "Total loss:  -2.2576 | PDE Loss:  -4.1548 | Function Loss:  -2.3164\n",
      "Total loss:  -2.2584 | PDE Loss:  -4.1598 | Function Loss:  -2.3166\n",
      "Total loss:  -2.2594 | PDE Loss:  -4.1611 | Function Loss:  -2.3176\n",
      "Total loss:  -2.2605 | PDE Loss:  -4.1635 | Function Loss:  -2.3185\n",
      "Total loss:  -2.2614 | PDE Loss:  -4.1647 | Function Loss:  -2.3194\n",
      "Total loss:  -2.2623 | PDE Loss:  -4.1656 | Function Loss:  -2.3203\n",
      "Total loss:  -2.2632 | PDE Loss:  -4.1659 | Function Loss:  -2.3212\n",
      "Total loss:  -2.2639 | PDE Loss:  -4.1672 | Function Loss:  -2.3219\n",
      "Total loss:  -2.2645 | PDE Loss:  -4.1671 | Function Loss:  -2.3226\n",
      "Total loss:  -2.2653 | PDE Loss:  -4.1704 | Function Loss:  -2.323\n",
      "Total loss:  -2.266 | PDE Loss:  -4.1706 | Function Loss:  -2.3237\n",
      "Total loss:  -2.2667 | PDE Loss:  -4.1774 | Function Loss:  -2.3236\n",
      "Total loss:  -2.2674 | PDE Loss:  -4.1796 | Function Loss:  -2.3241\n",
      "Total loss:  -2.268 | PDE Loss:  -4.1859 | Function Loss:  -2.3239\n",
      "Total loss:  -2.2685 | PDE Loss:  -4.1896 | Function Loss:  -2.324\n",
      "Total loss:  -2.269 | PDE Loss:  -4.1924 | Function Loss:  -2.3241\n",
      "Total loss:  -2.2694 | PDE Loss:  -4.1952 | Function Loss:  -2.3242\n",
      "Total loss:  -2.2699 | PDE Loss:  -4.1979 | Function Loss:  -2.3245\n",
      "Total loss:  -2.2706 | PDE Loss:  -4.2002 | Function Loss:  -2.3249\n",
      "Total loss:  -2.2714 | PDE Loss:  -4.2043 | Function Loss:  -2.3253\n",
      "Total loss:  -2.2722 | PDE Loss:  -4.2055 | Function Loss:  -2.3261\n",
      "Total loss:  -2.2733 | PDE Loss:  -4.2019 | Function Loss:  -2.3277\n",
      "Total loss:  -2.2746 | PDE Loss:  -4.2046 | Function Loss:  -2.3289\n",
      "Total loss:  -2.2759 | PDE Loss:  -4.1997 | Function Loss:  -2.331\n",
      "Total loss:  -2.2772 | PDE Loss:  -4.197 | Function Loss:  -2.3329\n",
      "Total loss:  -2.2788 | PDE Loss:  -4.1913 | Function Loss:  -2.3355\n",
      "Total loss:  -2.2802 | PDE Loss:  -4.1881 | Function Loss:  -2.3375\n",
      "Total loss:  -2.2814 | PDE Loss:  -4.1862 | Function Loss:  -2.3392\n",
      "Total loss:  -2.2826 | PDE Loss:  -4.1861 | Function Loss:  -2.3405\n",
      "Total loss:  -2.284 | PDE Loss:  -4.1878 | Function Loss:  -2.3419\n",
      "Total loss:  -2.2855 | PDE Loss:  -4.1957 | Function Loss:  -2.3425\n",
      "Total loss:  -2.2867 | PDE Loss:  -4.2021 | Function Loss:  -2.343\n",
      "Total loss:  -2.2876 | PDE Loss:  -4.2066 | Function Loss:  -2.3434\n",
      "Total loss:  -2.2883 | PDE Loss:  -4.2164 | Function Loss:  -2.3428\n",
      "Total loss:  -2.2888 | PDE Loss:  -4.2196 | Function Loss:  -2.3429\n",
      "Total loss:  -2.2892 | PDE Loss:  -4.2235 | Function Loss:  -2.3429\n",
      "Total loss:  -2.2897 | PDE Loss:  -4.2286 | Function Loss:  -2.3428\n",
      "Total loss:  -2.2903 | PDE Loss:  -4.2313 | Function Loss:  -2.3432\n",
      "Total loss:  -2.2907 | PDE Loss:  -4.2331 | Function Loss:  -2.3433\n",
      "Total loss:  -2.2918 | PDE Loss:  -4.2366 | Function Loss:  -2.3441\n",
      "Total loss:  -2.2926 | PDE Loss:  -4.2414 | Function Loss:  -2.3444\n",
      "Total loss:  -2.2933 | PDE Loss:  -4.2426 | Function Loss:  -2.3451\n",
      "Total loss:  -2.2945 | PDE Loss:  -4.2435 | Function Loss:  -2.3463\n",
      "Total loss:  -2.2957 | PDE Loss:  -4.2478 | Function Loss:  -2.3471\n",
      "Total loss:  -2.2969 | PDE Loss:  -4.2453 | Function Loss:  -2.3488\n",
      "Total loss:  -2.2977 | PDE Loss:  -4.2473 | Function Loss:  -2.3494\n",
      "Total loss:  -2.2992 | PDE Loss:  -4.2462 | Function Loss:  -2.3513\n",
      "Total loss:  -2.3006 | PDE Loss:  -4.2531 | Function Loss:  -2.352\n",
      "Total loss:  -2.3015 | PDE Loss:  -4.253 | Function Loss:  -2.353\n",
      "Total loss:  -2.3023 | PDE Loss:  -4.2569 | Function Loss:  -2.3533\n",
      "Total loss:  -2.3031 | PDE Loss:  -4.2553 | Function Loss:  -2.3544\n",
      "Total loss:  -2.304 | PDE Loss:  -4.2595 | Function Loss:  -2.355\n",
      "Total loss:  -2.3049 | PDE Loss:  -4.2562 | Function Loss:  -2.3565\n",
      "Total loss:  -2.3059 | PDE Loss:  -4.26 | Function Loss:  -2.3571\n",
      "Total loss:  -2.3068 | PDE Loss:  -4.2582 | Function Loss:  -2.3584\n",
      "Total loss:  -2.3076 | PDE Loss:  -4.2626 | Function Loss:  -2.3587\n",
      "Total loss:  -2.3083 | PDE Loss:  -4.2607 | Function Loss:  -2.3597\n",
      "Total loss:  -2.3088 | PDE Loss:  -4.2648 | Function Loss:  -2.3598\n",
      "Total loss:  -2.3092 | PDE Loss:  -4.2644 | Function Loss:  -2.3603\n",
      "Total loss:  -2.3096 | PDE Loss:  -4.2648 | Function Loss:  -2.3606\n",
      "Total loss:  -2.3099 | PDE Loss:  -4.2668 | Function Loss:  -2.3607\n",
      "Total loss:  -2.3101 | PDE Loss:  -4.2672 | Function Loss:  -2.3609\n",
      "Total loss:  -2.3104 | PDE Loss:  -4.2696 | Function Loss:  -2.3609\n",
      "Total loss:  -2.3108 | PDE Loss:  -4.2711 | Function Loss:  -2.3612\n",
      "Total loss:  -2.3116 | PDE Loss:  -4.2754 | Function Loss:  -2.3615\n",
      "Total loss:  -2.3126 | PDE Loss:  -4.2805 | Function Loss:  -2.3621\n",
      "Total loss:  -2.3137 | PDE Loss:  -4.285 | Function Loss:  -2.3628\n",
      "Total loss:  -2.3151 | PDE Loss:  -4.2916 | Function Loss:  -2.3635\n",
      "Total loss:  -2.3166 | PDE Loss:  -4.2966 | Function Loss:  -2.3646\n",
      "Total loss:  -2.3179 | PDE Loss:  -4.2982 | Function Loss:  -2.3659\n",
      "Total loss:  -2.3187 | PDE Loss:  -4.302 | Function Loss:  -2.3664\n",
      "Total loss:  -2.3195 | PDE Loss:  -4.3017 | Function Loss:  -2.3672\n",
      "Total loss:  -2.32 | PDE Loss:  -4.3023 | Function Loss:  -2.3678\n",
      "Total loss:  -2.3205 | PDE Loss:  -4.3046 | Function Loss:  -2.3681\n",
      "Total loss:  -2.3213 | PDE Loss:  -4.3052 | Function Loss:  -2.3688\n",
      "Total loss:  -2.3221 | PDE Loss:  -4.3076 | Function Loss:  -2.3695\n",
      "Total loss:  -2.3229 | PDE Loss:  -4.3072 | Function Loss:  -2.3705\n",
      "Total loss:  -2.3238 | PDE Loss:  -4.3063 | Function Loss:  -2.3716\n",
      "Total loss:  -2.3248 | PDE Loss:  -4.3017 | Function Loss:  -2.3732\n",
      "Total loss:  -2.3259 | PDE Loss:  -4.2997 | Function Loss:  -2.3746\n",
      "Total loss:  -2.3268 | PDE Loss:  -4.2926 | Function Loss:  -2.3766\n",
      "Total loss:  -2.3277 | PDE Loss:  -4.2917 | Function Loss:  -2.3777\n",
      "Total loss:  -2.3286 | PDE Loss:  -4.286 | Function Loss:  -2.3794\n",
      "Total loss:  -2.3295 | PDE Loss:  -4.2833 | Function Loss:  -2.3807\n",
      "Total loss:  -2.3302 | PDE Loss:  -4.284 | Function Loss:  -2.3814\n",
      "Total loss:  -2.3311 | PDE Loss:  -4.2802 | Function Loss:  -2.3829\n",
      "Total loss:  -2.3323 | PDE Loss:  -4.2845 | Function Loss:  -2.3837\n",
      "Total loss:  -2.3333 | PDE Loss:  -4.2807 | Function Loss:  -2.3853\n",
      "Total loss:  -2.3343 | PDE Loss:  -4.2839 | Function Loss:  -2.386\n",
      "Total loss:  -2.3352 | PDE Loss:  -4.2868 | Function Loss:  -2.3867\n",
      "Total loss:  -2.3362 | PDE Loss:  -4.2877 | Function Loss:  -2.3877\n",
      "Total loss:  -2.3372 | PDE Loss:  -4.2905 | Function Loss:  -2.3884\n",
      "Total loss:  -2.3383 | PDE Loss:  -4.2913 | Function Loss:  -2.3896\n",
      "Total loss:  -2.3397 | PDE Loss:  -4.2933 | Function Loss:  -2.3909\n",
      "Total loss:  -2.3413 | PDE Loss:  -4.296 | Function Loss:  -2.3924\n",
      "Total loss:  -2.3425 | PDE Loss:  -4.2935 | Function Loss:  -2.394\n",
      "Total loss:  -2.3435 | PDE Loss:  -4.2923 | Function Loss:  -2.3953\n",
      "Total loss:  -2.3445 | PDE Loss:  -4.2949 | Function Loss:  -2.3961\n",
      "Total loss:  -2.3453 | PDE Loss:  -4.2936 | Function Loss:  -2.3971\n",
      "Total loss:  -2.3459 | PDE Loss:  -4.2998 | Function Loss:  -2.3971\n",
      "Total loss:  -2.3465 | PDE Loss:  -4.3011 | Function Loss:  -2.3976\n",
      "Total loss:  -2.3471 | PDE Loss:  -4.3073 | Function Loss:  -2.3975\n",
      "Total loss:  -2.3481 | PDE Loss:  -4.3134 | Function Loss:  -2.3979\n",
      "Total loss:  -2.3495 | PDE Loss:  -4.3241 | Function Loss:  -2.3982\n",
      "Total loss:  -2.3509 | PDE Loss:  -4.3289 | Function Loss:  -2.3992\n",
      "Total loss:  -2.3523 | PDE Loss:  -4.3325 | Function Loss:  -2.4003\n",
      "Total loss:  -2.3536 | PDE Loss:  -4.334 | Function Loss:  -2.4016\n",
      "Total loss:  -2.3551 | PDE Loss:  -4.3303 | Function Loss:  -2.4037\n",
      "Total loss:  -2.3564 | PDE Loss:  -4.3299 | Function Loss:  -2.4052\n",
      "Total loss:  -2.3577 | PDE Loss:  -4.3223 | Function Loss:  -2.4076\n",
      "Total loss:  -2.3587 | PDE Loss:  -4.3164 | Function Loss:  -2.4095\n",
      "Total loss:  -2.3599 | PDE Loss:  -4.3133 | Function Loss:  -2.4111\n",
      "Total loss:  -2.361 | PDE Loss:  -4.3072 | Function Loss:  -2.4132\n",
      "Total loss:  -2.3624 | PDE Loss:  -4.3066 | Function Loss:  -2.4148\n",
      "Total loss:  -2.3639 | PDE Loss:  -4.3075 | Function Loss:  -2.4164\n",
      "Total loss:  -2.3653 | PDE Loss:  -4.3127 | Function Loss:  -2.4173\n",
      "Total loss:  -2.3664 | PDE Loss:  -4.3149 | Function Loss:  -2.4183\n",
      "Total loss:  -2.3676 | PDE Loss:  -4.3219 | Function Loss:  -2.4187\n",
      "Total loss:  -2.3686 | PDE Loss:  -4.3224 | Function Loss:  -2.4199\n",
      "Total loss:  -2.3697 | PDE Loss:  -4.3244 | Function Loss:  -2.4207\n",
      "Total loss:  -2.3706 | PDE Loss:  -4.3252 | Function Loss:  -2.4217\n",
      "Total loss:  -2.3716 | PDE Loss:  -4.3235 | Function Loss:  -2.4231\n",
      "Total loss:  -2.3729 | PDE Loss:  -4.324 | Function Loss:  -2.4245\n",
      "Total loss:  -2.3741 | PDE Loss:  -4.3224 | Function Loss:  -2.426\n",
      "Total loss:  -2.3749 | PDE Loss:  -4.322 | Function Loss:  -2.427\n",
      "Total loss:  -2.3759 | PDE Loss:  -4.3247 | Function Loss:  -2.4277\n",
      "Total loss:  -2.3771 | PDE Loss:  -4.3228 | Function Loss:  -2.4293\n",
      "Total loss:  -2.3782 | PDE Loss:  -4.3261 | Function Loss:  -2.4301\n",
      "Total loss:  -2.3792 | PDE Loss:  -4.327 | Function Loss:  -2.4312\n",
      "Total loss:  -2.3804 | PDE Loss:  -4.3299 | Function Loss:  -2.4322\n",
      "Total loss:  -2.3815 | PDE Loss:  -4.3352 | Function Loss:  -2.4327\n",
      "Total loss:  -2.3825 | PDE Loss:  -4.3372 | Function Loss:  -2.4336\n",
      "Total loss:  -2.3836 | PDE Loss:  -4.3419 | Function Loss:  -2.4342\n",
      "Total loss:  -2.3846 | PDE Loss:  -4.337 | Function Loss:  -2.436\n",
      "Total loss:  -2.3855 | PDE Loss:  -4.3383 | Function Loss:  -2.4368\n",
      "Total loss:  -2.3864 | PDE Loss:  -4.3333 | Function Loss:  -2.4385\n",
      "Total loss:  -2.3875 | PDE Loss:  -4.3296 | Function Loss:  -2.4401\n",
      "Total loss:  -2.3885 | PDE Loss:  -4.3209 | Function Loss:  -2.4424\n",
      "Total loss:  -2.3892 | PDE Loss:  -4.3185 | Function Loss:  -2.4436\n",
      "Total loss:  -2.3898 | PDE Loss:  -4.3125 | Function Loss:  -2.4451\n",
      "Total loss:  -2.3904 | PDE Loss:  -4.3112 | Function Loss:  -2.4459\n",
      "Total loss:  -2.391 | PDE Loss:  -4.31 | Function Loss:  -2.4467\n",
      "Total loss:  -2.3915 | PDE Loss:  -4.3083 | Function Loss:  -2.4476\n",
      "Total loss:  -2.3923 | PDE Loss:  -4.3127 | Function Loss:  -2.4479\n",
      "Total loss:  -2.3932 | PDE Loss:  -4.3148 | Function Loss:  -2.4486\n",
      "Total loss:  -2.3942 | PDE Loss:  -4.3193 | Function Loss:  -2.4491\n",
      "Total loss:  -2.3951 | PDE Loss:  -4.3206 | Function Loss:  -2.4499\n",
      "Total loss:  -2.3959 | PDE Loss:  -4.3202 | Function Loss:  -2.451\n",
      "Total loss:  -2.3971 | PDE Loss:  -4.3142 | Function Loss:  -2.4531\n",
      "Total loss:  -2.3981 | PDE Loss:  -4.3132 | Function Loss:  -2.4544\n",
      "Total loss:  -2.399 | PDE Loss:  -4.313 | Function Loss:  -2.4554\n",
      "Total loss:  -2.3997 | PDE Loss:  -4.3075 | Function Loss:  -2.4571\n",
      "Total loss:  -2.4004 | PDE Loss:  -4.3098 | Function Loss:  -2.4575\n",
      "Total loss:  -2.4009 | PDE Loss:  -4.3083 | Function Loss:  -2.4583\n",
      "Total loss:  -2.4017 | PDE Loss:  -4.3073 | Function Loss:  -2.4593\n",
      "Total loss:  -2.4028 | PDE Loss:  -4.3104 | Function Loss:  -2.4602\n",
      "Total loss:  -2.404 | PDE Loss:  -4.3076 | Function Loss:  -2.4619\n",
      "Total loss:  -2.4049 | PDE Loss:  -4.3132 | Function Loss:  -2.4621\n",
      "Total loss:  -2.4059 | PDE Loss:  -4.315 | Function Loss:  -2.4631\n",
      "Total loss:  -2.4068 | PDE Loss:  -4.3235 | Function Loss:  -2.4629\n",
      "Total loss:  -2.4077 | PDE Loss:  -4.3256 | Function Loss:  -2.4636\n",
      "Total loss:  -2.4084 | PDE Loss:  -4.3289 | Function Loss:  -2.464\n",
      "Total loss:  -2.4091 | PDE Loss:  -4.3317 | Function Loss:  -2.4644\n",
      "Total loss:  -2.4099 | PDE Loss:  -4.3287 | Function Loss:  -2.4657\n",
      "Total loss:  -2.4106 | PDE Loss:  -4.3281 | Function Loss:  -2.4666\n",
      "Total loss:  -2.4111 | PDE Loss:  -4.3262 | Function Loss:  -2.4674\n",
      "Total loss:  -2.4117 | PDE Loss:  -4.3272 | Function Loss:  -2.4679\n",
      "Total loss:  -2.4122 | PDE Loss:  -4.3251 | Function Loss:  -2.4688\n",
      "Total loss:  -2.4128 | PDE Loss:  -4.3261 | Function Loss:  -2.4694\n",
      "Total loss:  -2.4138 | PDE Loss:  -4.3287 | Function Loss:  -2.4701\n",
      "Total loss:  -2.415 | PDE Loss:  -4.3279 | Function Loss:  -2.4717\n",
      "Total loss:  -2.4164 | PDE Loss:  -4.3377 | Function Loss:  -2.4718\n",
      "Total loss:  -2.4177 | PDE Loss:  -4.3436 | Function Loss:  -2.4726\n",
      "Total loss:  -2.4201 | PDE Loss:  -4.3518 | Function Loss:  -2.4742\n",
      "Total loss:  -2.4225 | PDE Loss:  -4.3673 | Function Loss:  -2.4749\n",
      "Total loss:  -2.4242 | PDE Loss:  -4.3665 | Function Loss:  -2.4768\n",
      "Total loss:  -2.426 | PDE Loss:  -4.3693 | Function Loss:  -2.4785\n",
      "Total loss:  -2.4276 | PDE Loss:  -4.3631 | Function Loss:  -2.4812\n",
      "Total loss:  -2.4291 | PDE Loss:  -4.3625 | Function Loss:  -2.483\n",
      "Total loss:  -2.4304 | PDE Loss:  -4.3578 | Function Loss:  -2.485\n",
      "Total loss:  -2.4317 | PDE Loss:  -4.3553 | Function Loss:  -2.4869\n",
      "Total loss:  -2.4333 | PDE Loss:  -4.3503 | Function Loss:  -2.4894\n",
      "Total loss:  -2.435 | PDE Loss:  -4.3527 | Function Loss:  -2.4909\n",
      "Total loss:  -2.4365 | PDE Loss:  -4.3488 | Function Loss:  -2.4932\n",
      "Total loss:  -2.4378 | PDE Loss:  -4.3547 | Function Loss:  -2.4939\n",
      "Total loss:  -2.4387 | PDE Loss:  -4.3569 | Function Loss:  -2.4946\n",
      "Total loss:  -2.4396 | PDE Loss:  -4.3645 | Function Loss:  -2.4945\n",
      "Total loss:  -2.4401 | PDE Loss:  -4.3657 | Function Loss:  -2.495\n",
      "Total loss:  -2.4406 | PDE Loss:  -4.3704 | Function Loss:  -2.4949\n",
      "Total loss:  -2.4414 | PDE Loss:  -4.3716 | Function Loss:  -2.4956\n",
      "Total loss:  -2.4424 | PDE Loss:  -4.3753 | Function Loss:  -2.4963\n",
      "Total loss:  -2.4435 | PDE Loss:  -4.3739 | Function Loss:  -2.4977\n",
      "Total loss:  -2.4445 | PDE Loss:  -4.3754 | Function Loss:  -2.4987\n",
      "Total loss:  -2.4452 | PDE Loss:  -4.3728 | Function Loss:  -2.4997\n",
      "Total loss:  -2.4461 | PDE Loss:  -4.3734 | Function Loss:  -2.5007\n",
      "Total loss:  -2.4469 | PDE Loss:  -4.3698 | Function Loss:  -2.5021\n",
      "Total loss:  -2.4476 | PDE Loss:  -4.3694 | Function Loss:  -2.503\n",
      "Total loss:  -2.4484 | PDE Loss:  -4.3704 | Function Loss:  -2.5038\n",
      "Total loss:  -2.4495 | PDE Loss:  -4.3687 | Function Loss:  -2.5052\n",
      "Total loss:  -2.4507 | PDE Loss:  -4.3721 | Function Loss:  -2.5062\n",
      "Total loss:  -2.4518 | PDE Loss:  -4.3721 | Function Loss:  -2.5074\n",
      "Total loss:  -2.4525 | PDE Loss:  -4.3722 | Function Loss:  -2.5082\n",
      "Total loss:  -2.4531 | PDE Loss:  -4.3736 | Function Loss:  -2.5087\n",
      "Total loss:  -2.4537 | PDE Loss:  -4.3748 | Function Loss:  -2.5092\n",
      "Total loss:  -2.4542 | PDE Loss:  -4.374 | Function Loss:  -2.5099\n",
      "Total loss:  -2.4547 | PDE Loss:  -4.3746 | Function Loss:  -2.5104\n",
      "Total loss:  -2.4553 | PDE Loss:  -4.3733 | Function Loss:  -2.5112\n",
      "Total loss:  -2.4562 | PDE Loss:  -4.3717 | Function Loss:  -2.5125\n",
      "Total loss:  -2.4573 | PDE Loss:  -4.373 | Function Loss:  -2.5136\n",
      "Total loss:  -2.4582 | PDE Loss:  -4.3704 | Function Loss:  -2.5149\n",
      "Total loss:  -2.459 | PDE Loss:  -4.3734 | Function Loss:  -2.5154\n",
      "Total loss:  -2.4599 | PDE Loss:  -4.3719 | Function Loss:  -2.5166\n",
      "Total loss:  -2.4606 | PDE Loss:  -4.3769 | Function Loss:  -2.5168\n",
      "Total loss:  -2.4615 | PDE Loss:  -4.3791 | Function Loss:  -2.5175\n",
      "Total loss:  -2.4625 | PDE Loss:  -4.3871 | Function Loss:  -2.5175\n",
      "Total loss:  -2.4633 | PDE Loss:  -4.3906 | Function Loss:  -2.518\n",
      "Total loss:  -2.4641 | PDE Loss:  -4.3969 | Function Loss:  -2.518\n",
      "Total loss:  -2.4648 | PDE Loss:  -4.3986 | Function Loss:  -2.5186\n",
      "Total loss:  -2.4657 | PDE Loss:  -4.4023 | Function Loss:  -2.5191\n",
      "Total loss:  -2.4665 | PDE Loss:  -4.3991 | Function Loss:  -2.5204\n",
      "Total loss:  -2.4672 | PDE Loss:  -4.4001 | Function Loss:  -2.5211\n",
      "Total loss:  -2.4678 | PDE Loss:  -4.399 | Function Loss:  -2.522\n",
      "Total loss:  -2.4687 | PDE Loss:  -4.3987 | Function Loss:  -2.523\n",
      "Total loss:  -2.4696 | PDE Loss:  -4.4006 | Function Loss:  -2.5238\n",
      "Total loss:  -2.4705 | PDE Loss:  -4.4026 | Function Loss:  -2.5245\n",
      "Total loss:  -2.4713 | PDE Loss:  -4.4072 | Function Loss:  -2.5248\n",
      "Total loss:  -2.4719 | PDE Loss:  -4.4112 | Function Loss:  -2.525\n",
      "Total loss:  -2.4727 | PDE Loss:  -4.4149 | Function Loss:  -2.5254\n",
      "Total loss:  -2.4737 | PDE Loss:  -4.4217 | Function Loss:  -2.5257\n",
      "Total loss:  -2.4748 | PDE Loss:  -4.4251 | Function Loss:  -2.5264\n",
      "Total loss:  -2.4758 | PDE Loss:  -4.4303 | Function Loss:  -2.5269\n",
      "Total loss:  -2.4769 | PDE Loss:  -4.4314 | Function Loss:  -2.528\n",
      "Total loss:  -2.4779 | PDE Loss:  -4.4324 | Function Loss:  -2.529\n",
      "Total loss:  -2.4791 | PDE Loss:  -4.4328 | Function Loss:  -2.5303\n",
      "Total loss:  -2.4802 | PDE Loss:  -4.4294 | Function Loss:  -2.532\n",
      "Total loss:  -2.4811 | PDE Loss:  -4.4295 | Function Loss:  -2.533\n",
      "Total loss:  -2.4823 | PDE Loss:  -4.4305 | Function Loss:  -2.5343\n",
      "Total loss:  -2.4835 | PDE Loss:  -4.433 | Function Loss:  -2.5353\n",
      "Total loss:  -2.4847 | PDE Loss:  -4.4364 | Function Loss:  -2.5361\n",
      "Total loss:  -2.486 | PDE Loss:  -4.4402 | Function Loss:  -2.5371\n",
      "Total loss:  -2.4872 | PDE Loss:  -4.4484 | Function Loss:  -2.5375\n",
      "Total loss:  -2.4886 | PDE Loss:  -4.4456 | Function Loss:  -2.5394\n",
      "Total loss:  -2.4893 | PDE Loss:  -4.4492 | Function Loss:  -2.5398\n",
      "Total loss:  -2.4909 | PDE Loss:  -4.4523 | Function Loss:  -2.5412\n",
      "Total loss:  -2.4919 | PDE Loss:  -4.4538 | Function Loss:  -2.5421\n",
      "Total loss:  -2.4926 | PDE Loss:  -4.4519 | Function Loss:  -2.5431\n",
      "Total loss:  -2.4934 | PDE Loss:  -4.4484 | Function Loss:  -2.5444\n",
      "Total loss:  -2.4944 | PDE Loss:  -4.4432 | Function Loss:  -2.5462\n",
      "Total loss:  -2.4954 | PDE Loss:  -4.4365 | Function Loss:  -2.5483\n",
      "Total loss:  -2.4965 | PDE Loss:  -4.4336 | Function Loss:  -2.5498\n",
      "Total loss:  -2.4975 | PDE Loss:  -4.4199 | Function Loss:  -2.5528\n",
      "Total loss:  -2.4984 | PDE Loss:  -4.4208 | Function Loss:  -2.5537\n",
      "Total loss:  -2.4991 | PDE Loss:  -4.4181 | Function Loss:  -2.5549\n",
      "Total loss:  -2.5002 | PDE Loss:  -4.4209 | Function Loss:  -2.5557\n",
      "Total loss:  -2.5012 | PDE Loss:  -4.4201 | Function Loss:  -2.5569\n",
      "Total loss:  -2.5022 | PDE Loss:  -4.4199 | Function Loss:  -2.5582\n",
      "Total loss:  -2.5034 | PDE Loss:  -4.4214 | Function Loss:  -2.5593\n",
      "Total loss:  -2.5045 | PDE Loss:  -4.4178 | Function Loss:  -2.5611\n",
      "Total loss:  -2.5055 | PDE Loss:  -4.4188 | Function Loss:  -2.5621\n",
      "Total loss:  -2.5062 | PDE Loss:  -4.4159 | Function Loss:  -2.5632\n",
      "Total loss:  -2.5067 | PDE Loss:  -4.4143 | Function Loss:  -2.564\n",
      "Total loss:  -2.5072 | PDE Loss:  -4.413 | Function Loss:  -2.5648\n",
      "Total loss:  -2.5077 | PDE Loss:  -4.4113 | Function Loss:  -2.5656\n",
      "Total loss:  -2.5083 | PDE Loss:  -4.4107 | Function Loss:  -2.5664\n",
      "Total loss:  -2.5089 | PDE Loss:  -4.4096 | Function Loss:  -2.5672\n",
      "Total loss:  -2.5096 | PDE Loss:  -4.4099 | Function Loss:  -2.5679\n",
      "Total loss:  -2.5104 | PDE Loss:  -4.4139 | Function Loss:  -2.5683\n",
      "Total loss:  -2.5112 | PDE Loss:  -4.4098 | Function Loss:  -2.5698\n",
      "Total loss:  -2.512 | PDE Loss:  -4.4172 | Function Loss:  -2.5696\n",
      "Total loss:  -2.5124 | PDE Loss:  -4.4234 | Function Loss:  -2.5693\n",
      "Total loss:  -2.5131 | PDE Loss:  -4.4267 | Function Loss:  -2.5696\n",
      "Total loss:  -2.5136 | PDE Loss:  -4.4288 | Function Loss:  -2.5699\n",
      "Total loss:  -2.514 | PDE Loss:  -4.4288 | Function Loss:  -2.5704\n",
      "Total loss:  -2.5145 | PDE Loss:  -4.4289 | Function Loss:  -2.5709\n",
      "Total loss:  -2.5152 | PDE Loss:  -4.4283 | Function Loss:  -2.5717\n",
      "Total loss:  -2.5158 | PDE Loss:  -4.4274 | Function Loss:  -2.5726\n",
      "Total loss:  -2.5164 | PDE Loss:  -4.4278 | Function Loss:  -2.5733\n",
      "Total loss:  -2.5171 | PDE Loss:  -4.4264 | Function Loss:  -2.5742\n",
      "Total loss:  -2.5178 | PDE Loss:  -4.4284 | Function Loss:  -2.5747\n",
      "Total loss:  -2.5185 | PDE Loss:  -4.4248 | Function Loss:  -2.576\n",
      "Total loss:  -2.5192 | PDE Loss:  -4.4231 | Function Loss:  -2.5771\n",
      "Total loss:  -2.5201 | PDE Loss:  -4.421 | Function Loss:  -2.5784\n",
      "Total loss:  -2.5209 | PDE Loss:  -4.4172 | Function Loss:  -2.5799\n",
      "Total loss:  -2.5217 | PDE Loss:  -4.4173 | Function Loss:  -2.5807\n",
      "Total loss:  -2.5224 | PDE Loss:  -4.4148 | Function Loss:  -2.5819\n",
      "Total loss:  -2.523 | PDE Loss:  -4.4171 | Function Loss:  -2.5823\n",
      "Total loss:  -2.5235 | PDE Loss:  -4.4174 | Function Loss:  -2.5829\n",
      "Total loss:  -2.524 | PDE Loss:  -4.4192 | Function Loss:  -2.5831\n",
      "Total loss:  -2.5245 | PDE Loss:  -4.4213 | Function Loss:  -2.5833\n",
      "Total loss:  -2.525 | PDE Loss:  -4.4243 | Function Loss:  -2.5835\n",
      "Total loss:  -2.5254 | PDE Loss:  -4.4251 | Function Loss:  -2.5839\n",
      "Total loss:  -2.5258 | PDE Loss:  -4.4286 | Function Loss:  -2.5838\n",
      "Total loss:  -2.5262 | PDE Loss:  -4.4278 | Function Loss:  -2.5844\n",
      "Total loss:  -2.5266 | PDE Loss:  -4.4294 | Function Loss:  -2.5847\n",
      "Total loss:  -2.5273 | PDE Loss:  -4.4304 | Function Loss:  -2.5853\n",
      "Total loss:  -2.5281 | PDE Loss:  -4.4322 | Function Loss:  -2.586\n",
      "Total loss:  -2.5289 | PDE Loss:  -4.439 | Function Loss:  -2.5859\n",
      "Total loss:  -2.5296 | PDE Loss:  -4.4388 | Function Loss:  -2.5868\n",
      "Total loss:  -2.5302 | PDE Loss:  -4.4417 | Function Loss:  -2.587\n",
      "Total loss:  -2.5309 | PDE Loss:  -4.4452 | Function Loss:  -2.5874\n",
      "Total loss:  -2.5316 | PDE Loss:  -4.4505 | Function Loss:  -2.5873\n",
      "Total loss:  -2.5321 | PDE Loss:  -4.4552 | Function Loss:  -2.5873\n",
      "Total loss:  -2.5326 | PDE Loss:  -4.4601 | Function Loss:  -2.5873\n",
      "Total loss:  -2.5331 | PDE Loss:  -4.4657 | Function Loss:  -2.587\n",
      "Total loss:  -2.5335 | PDE Loss:  -4.4679 | Function Loss:  -2.5873\n",
      "Total loss:  -2.534 | PDE Loss:  -4.4729 | Function Loss:  -2.5871\n",
      "Total loss:  -2.5344 | PDE Loss:  -4.4741 | Function Loss:  -2.5874\n",
      "Total loss:  -2.535 | PDE Loss:  -4.4759 | Function Loss:  -2.5879\n",
      "Total loss:  -2.5358 | PDE Loss:  -4.4752 | Function Loss:  -2.5888\n",
      "Total loss:  -2.5365 | PDE Loss:  -4.4743 | Function Loss:  -2.5898\n",
      "Total loss:  -2.5376 | PDE Loss:  -4.474 | Function Loss:  -2.591\n",
      "Total loss:  -2.5388 | PDE Loss:  -4.473 | Function Loss:  -2.5925\n",
      "Total loss:  -2.54 | PDE Loss:  -4.4749 | Function Loss:  -2.5937\n",
      "Total loss:  -2.5413 | PDE Loss:  -4.4782 | Function Loss:  -2.5947\n",
      "Total loss:  -2.5426 | PDE Loss:  -4.4818 | Function Loss:  -2.5956\n",
      "Total loss:  -2.5437 | PDE Loss:  -4.4876 | Function Loss:  -2.5961\n",
      "Total loss:  -2.5447 | PDE Loss:  -4.4905 | Function Loss:  -2.5969\n",
      "Total loss:  -2.5456 | PDE Loss:  -4.4942 | Function Loss:  -2.5974\n",
      "Total loss:  -2.5465 | PDE Loss:  -4.4967 | Function Loss:  -2.5981\n",
      "Total loss:  -2.5476 | PDE Loss:  -4.4989 | Function Loss:  -2.5991\n",
      "Total loss:  -2.549 | PDE Loss:  -4.5009 | Function Loss:  -2.6004\n",
      "Total loss:  -2.5504 | PDE Loss:  -4.5011 | Function Loss:  -2.602\n",
      "Total loss:  -2.5516 | PDE Loss:  -4.5062 | Function Loss:  -2.6027\n",
      "Total loss:  -2.5529 | PDE Loss:  -4.5027 | Function Loss:  -2.6047\n",
      "Total loss:  -2.5538 | PDE Loss:  -4.5023 | Function Loss:  -2.6057\n",
      "Total loss:  -2.5546 | PDE Loss:  -4.5047 | Function Loss:  -2.6062\n",
      "Total loss:  -2.5551 | PDE Loss:  -4.5043 | Function Loss:  -2.6068\n",
      "Total loss:  -2.5558 | PDE Loss:  -4.5044 | Function Loss:  -2.6076\n",
      "Total loss:  -2.5567 | PDE Loss:  -4.505 | Function Loss:  -2.6086\n",
      "Total loss:  -2.5574 | PDE Loss:  -4.5047 | Function Loss:  -2.6095\n",
      "Total loss:  -2.558 | PDE Loss:  -4.5058 | Function Loss:  -2.6099\n",
      "Total loss:  -2.5584 | PDE Loss:  -4.506 | Function Loss:  -2.6104\n",
      "Total loss:  -2.5591 | PDE Loss:  -4.5084 | Function Loss:  -2.6109\n",
      "Total loss:  -2.5599 | PDE Loss:  -4.5123 | Function Loss:  -2.6113\n",
      "Total loss:  -2.5607 | PDE Loss:  -4.5138 | Function Loss:  -2.6121\n",
      "Total loss:  -2.5616 | PDE Loss:  -4.5186 | Function Loss:  -2.6124\n",
      "Total loss:  -2.5623 | PDE Loss:  -4.519 | Function Loss:  -2.6132\n",
      "Total loss:  -2.563 | PDE Loss:  -4.5195 | Function Loss:  -2.6139\n",
      "Total loss:  -2.5636 | PDE Loss:  -4.5177 | Function Loss:  -2.6148\n",
      "Total loss:  -2.5643 | PDE Loss:  -4.5193 | Function Loss:  -2.6154\n",
      "Total loss:  -2.5649 | PDE Loss:  -4.516 | Function Loss:  -2.6164\n",
      "Total loss:  -2.5654 | PDE Loss:  -4.5159 | Function Loss:  -2.617\n",
      "Total loss:  -2.5662 | PDE Loss:  -4.5153 | Function Loss:  -2.6179\n",
      "Total loss:  -2.567 | PDE Loss:  -4.5154 | Function Loss:  -2.6189\n",
      "Total loss:  -2.5679 | PDE Loss:  -4.5177 | Function Loss:  -2.6196\n",
      "Total loss:  -2.5689 | PDE Loss:  -4.5209 | Function Loss:  -2.6203\n",
      "Total loss:  -2.5699 | PDE Loss:  -4.5231 | Function Loss:  -2.6212\n",
      "Total loss:  -2.5711 | PDE Loss:  -4.5281 | Function Loss:  -2.6219\n",
      "Total loss:  -2.5725 | PDE Loss:  -4.5317 | Function Loss:  -2.623\n",
      "Total loss:  -2.5737 | PDE Loss:  -4.5361 | Function Loss:  -2.6239\n",
      "Total loss:  -2.5747 | PDE Loss:  -4.5381 | Function Loss:  -2.6247\n",
      "Total loss:  -2.5755 | PDE Loss:  -4.5388 | Function Loss:  -2.6255\n",
      "Total loss:  -2.5761 | PDE Loss:  -4.54 | Function Loss:  -2.6261\n",
      "Total loss:  -2.5765 | PDE Loss:  -4.5387 | Function Loss:  -2.6267\n",
      "Total loss:  -2.577 | PDE Loss:  -4.5383 | Function Loss:  -2.6273\n",
      "Total loss:  -2.5776 | PDE Loss:  -4.5365 | Function Loss:  -2.6282\n",
      "Total loss:  -2.5785 | PDE Loss:  -4.5349 | Function Loss:  -2.6294\n",
      "Total loss:  -2.5795 | PDE Loss:  -4.5329 | Function Loss:  -2.6307\n",
      "Total loss:  -2.5805 | PDE Loss:  -4.5298 | Function Loss:  -2.6323\n",
      "Total loss:  -2.5816 | PDE Loss:  -4.5281 | Function Loss:  -2.6338\n",
      "Total loss:  -2.5827 | PDE Loss:  -4.5248 | Function Loss:  -2.6354\n",
      "Total loss:  -2.5836 | PDE Loss:  -4.5236 | Function Loss:  -2.6365\n",
      "Total loss:  -2.5841 | PDE Loss:  -4.5203 | Function Loss:  -2.6376\n",
      "Total loss:  -2.5846 | PDE Loss:  -4.5193 | Function Loss:  -2.6382\n",
      "Total loss:  -2.585 | PDE Loss:  -4.5183 | Function Loss:  -2.6389\n",
      "Total loss:  -2.5855 | PDE Loss:  -4.5155 | Function Loss:  -2.6398\n",
      "Total loss:  -2.5861 | PDE Loss:  -4.516 | Function Loss:  -2.6404\n",
      "Total loss:  -2.5866 | PDE Loss:  -4.5139 | Function Loss:  -2.6413\n",
      "Total loss:  -2.5873 | PDE Loss:  -4.5111 | Function Loss:  -2.6424\n",
      "Total loss:  -2.5878 | PDE Loss:  -4.5114 | Function Loss:  -2.6429\n",
      "Total loss:  -2.5884 | PDE Loss:  -4.5107 | Function Loss:  -2.6437\n",
      "Total loss:  -2.5886 | PDE Loss:  -4.5091 | Function Loss:  -2.6442\n",
      "Total loss:  -2.5892 | PDE Loss:  -4.509 | Function Loss:  -2.6448\n",
      "Total loss:  -2.5896 | PDE Loss:  -4.5086 | Function Loss:  -2.6453\n",
      "Total loss:  -2.59 | PDE Loss:  -4.5077 | Function Loss:  -2.6459\n",
      "Total loss:  -2.5904 | PDE Loss:  -4.5068 | Function Loss:  -2.6465\n",
      "Total loss:  -2.5907 | PDE Loss:  -4.5054 | Function Loss:  -2.6471\n",
      "Total loss:  -2.5912 | PDE Loss:  -4.505 | Function Loss:  -2.6477\n",
      "Total loss:  -2.5918 | PDE Loss:  -4.5035 | Function Loss:  -2.6485\n",
      "Total loss:  -2.5925 | PDE Loss:  -4.5026 | Function Loss:  -2.6495\n",
      "Total loss:  -2.5934 | PDE Loss:  -4.4999 | Function Loss:  -2.6509\n",
      "Total loss:  -2.5941 | PDE Loss:  -4.4984 | Function Loss:  -2.6519\n",
      "Total loss:  -2.5953 | PDE Loss:  -4.4979 | Function Loss:  -2.6534\n",
      "Total loss:  -2.5966 | PDE Loss:  -4.4958 | Function Loss:  -2.6551\n",
      "Total loss:  -2.5976 | PDE Loss:  -4.4957 | Function Loss:  -2.6563\n",
      "Total loss:  -2.5983 | PDE Loss:  -4.494 | Function Loss:  -2.6574\n",
      "Total loss:  -2.5991 | PDE Loss:  -4.4926 | Function Loss:  -2.6585\n",
      "Total loss:  -2.5998 | PDE Loss:  -4.4911 | Function Loss:  -2.6595\n",
      "Total loss:  -2.6007 | PDE Loss:  -4.4877 | Function Loss:  -2.661\n",
      "Total loss:  -2.6015 | PDE Loss:  -4.4861 | Function Loss:  -2.6622\n",
      "Total loss:  -2.6022 | PDE Loss:  -4.4842 | Function Loss:  -2.6633\n",
      "Total loss:  -2.6026 | PDE Loss:  -4.4838 | Function Loss:  -2.6638\n",
      "Total loss:  -2.603 | PDE Loss:  -4.4852 | Function Loss:  -2.6641\n",
      "Total loss:  -2.6034 | PDE Loss:  -4.4864 | Function Loss:  -2.6643\n",
      "Total loss:  -2.6037 | PDE Loss:  -4.4877 | Function Loss:  -2.6645\n",
      "Total loss:  -2.6041 | PDE Loss:  -4.4881 | Function Loss:  -2.6649\n",
      "Total loss:  -2.6046 | PDE Loss:  -4.4872 | Function Loss:  -2.6656\n",
      "Total loss:  -2.6053 | PDE Loss:  -4.4852 | Function Loss:  -2.6667\n",
      "Total loss:  -2.606 | PDE Loss:  -4.4814 | Function Loss:  -2.6681\n",
      "Total loss:  -2.6068 | PDE Loss:  -4.4794 | Function Loss:  -2.6693\n",
      "Total loss:  -2.6073 | PDE Loss:  -4.4772 | Function Loss:  -2.6703\n",
      "Total loss:  -2.6081 | PDE Loss:  -4.4777 | Function Loss:  -2.6711\n",
      "Total loss:  -2.6088 | PDE Loss:  -4.4746 | Function Loss:  -2.6724\n",
      "Total loss:  -2.6093 | PDE Loss:  -4.4773 | Function Loss:  -2.6725\n",
      "Total loss:  -2.6098 | PDE Loss:  -4.4793 | Function Loss:  -2.6728\n",
      "Total loss:  -2.6103 | PDE Loss:  -4.4829 | Function Loss:  -2.6729\n",
      "Total loss:  -2.6109 | PDE Loss:  -4.4859 | Function Loss:  -2.673\n",
      "Total loss:  -2.6114 | PDE Loss:  -4.4882 | Function Loss:  -2.6733\n",
      "Total loss:  -2.6118 | PDE Loss:  -4.4895 | Function Loss:  -2.6736\n",
      "Total loss:  -2.6122 | PDE Loss:  -4.49 | Function Loss:  -2.674\n",
      "Total loss:  -2.6127 | PDE Loss:  -4.489 | Function Loss:  -2.6747\n",
      "Total loss:  -2.6132 | PDE Loss:  -4.4892 | Function Loss:  -2.6752\n",
      "Total loss:  -2.6135 | PDE Loss:  -4.4864 | Function Loss:  -2.676\n",
      "Total loss:  -2.6138 | PDE Loss:  -4.4855 | Function Loss:  -2.6764\n",
      "Total loss:  -2.614 | PDE Loss:  -4.4843 | Function Loss:  -2.6769\n",
      "Total loss:  -2.6142 | PDE Loss:  -4.4843 | Function Loss:  -2.6771\n",
      "Total loss:  -2.6144 | PDE Loss:  -4.485 | Function Loss:  -2.6772\n",
      "Total loss:  -2.6145 | PDE Loss:  -4.4853 | Function Loss:  -2.6774\n",
      "Total loss:  -2.6147 | PDE Loss:  -4.4877 | Function Loss:  -2.6772\n",
      "Total loss:  -2.615 | PDE Loss:  -4.4892 | Function Loss:  -2.6772\n",
      "Total loss:  -2.6153 | PDE Loss:  -4.4929 | Function Loss:  -2.6771\n",
      "Total loss:  -2.6158 | PDE Loss:  -4.4959 | Function Loss:  -2.6772\n",
      "Total loss:  -2.6164 | PDE Loss:  -4.4986 | Function Loss:  -2.6775\n",
      "Total loss:  -2.617 | PDE Loss:  -4.5 | Function Loss:  -2.678\n",
      "Total loss:  -2.6175 | PDE Loss:  -4.4998 | Function Loss:  -2.6785\n",
      "Total loss:  -2.618 | PDE Loss:  -4.4972 | Function Loss:  -2.6795\n",
      "Total loss:  -2.6185 | PDE Loss:  -4.4945 | Function Loss:  -2.6805\n",
      "Total loss:  -2.619 | PDE Loss:  -4.4895 | Function Loss:  -2.6819\n",
      "Total loss:  -2.6194 | PDE Loss:  -4.4862 | Function Loss:  -2.6829\n",
      "Total loss:  -2.62 | PDE Loss:  -4.4816 | Function Loss:  -2.6842\n",
      "Total loss:  -2.6205 | PDE Loss:  -4.4794 | Function Loss:  -2.6851\n",
      "Total loss:  -2.6209 | PDE Loss:  -4.476 | Function Loss:  -2.6862\n",
      "Total loss:  -2.6213 | PDE Loss:  -4.4762 | Function Loss:  -2.6866\n",
      "Total loss:  -2.6215 | PDE Loss:  -4.4756 | Function Loss:  -2.687\n",
      "Total loss:  -2.6218 | PDE Loss:  -4.4771 | Function Loss:  -2.6871\n",
      "Total loss:  -2.6221 | PDE Loss:  -4.4775 | Function Loss:  -2.6874\n",
      "Total loss:  -2.6224 | PDE Loss:  -4.4791 | Function Loss:  -2.6874\n",
      "Total loss:  -2.6227 | PDE Loss:  -4.4792 | Function Loss:  -2.6878\n",
      "Total loss:  -2.6232 | PDE Loss:  -4.4788 | Function Loss:  -2.6884\n",
      "Total loss:  -2.6237 | PDE Loss:  -4.4771 | Function Loss:  -2.6893\n",
      "Total loss:  -2.6244 | PDE Loss:  -4.4727 | Function Loss:  -2.6908\n",
      "Total loss:  -2.6251 | PDE Loss:  -4.4683 | Function Loss:  -2.6924\n",
      "Total loss:  -2.6259 | PDE Loss:  -4.4632 | Function Loss:  -2.6942\n",
      "Total loss:  -2.6269 | PDE Loss:  -4.4568 | Function Loss:  -2.6964\n",
      "Total loss:  -2.6281 | PDE Loss:  -4.4523 | Function Loss:  -2.6987\n",
      "Total loss:  -2.6291 | PDE Loss:  -4.4481 | Function Loss:  -2.7005\n",
      "Total loss:  -2.6301 | PDE Loss:  -4.4471 | Function Loss:  -2.7019\n",
      "Total loss:  -2.6311 | PDE Loss:  -4.4473 | Function Loss:  -2.7031\n",
      "Total loss:  -2.632 | PDE Loss:  -4.4482 | Function Loss:  -2.704\n",
      "Total loss:  -2.6328 | PDE Loss:  -4.451 | Function Loss:  -2.7044\n",
      "Total loss:  -2.6334 | PDE Loss:  -4.45 | Function Loss:  -2.7053\n",
      "Total loss:  -2.6338 | PDE Loss:  -4.4539 | Function Loss:  -2.705\n",
      "Total loss:  -2.634 | PDE Loss:  -4.4549 | Function Loss:  -2.7051\n",
      "Total loss:  -2.6343 | PDE Loss:  -4.4557 | Function Loss:  -2.7053\n",
      "Total loss:  -2.6345 | PDE Loss:  -4.4545 | Function Loss:  -2.7058\n",
      "Total loss:  -2.6348 | PDE Loss:  -4.4536 | Function Loss:  -2.7063\n",
      "Total loss:  -2.6351 | PDE Loss:  -4.4518 | Function Loss:  -2.707\n",
      "Total loss:  -2.6355 | PDE Loss:  -4.4503 | Function Loss:  -2.7077\n",
      "Total loss:  -2.6359 | PDE Loss:  -4.4498 | Function Loss:  -2.7083\n",
      "Total loss:  -2.6364 | PDE Loss:  -4.4501 | Function Loss:  -2.7088\n",
      "Total loss:  -2.6368 | PDE Loss:  -4.451 | Function Loss:  -2.7091\n",
      "Total loss:  -2.6373 | PDE Loss:  -4.4545 | Function Loss:  -2.7091\n",
      "Total loss:  -2.6377 | PDE Loss:  -4.4566 | Function Loss:  -2.7092\n",
      "Total loss:  -2.6382 | PDE Loss:  -4.4601 | Function Loss:  -2.7091\n",
      "Total loss:  -2.6386 | PDE Loss:  -4.4621 | Function Loss:  -2.7093\n",
      "Total loss:  -2.6391 | PDE Loss:  -4.4636 | Function Loss:  -2.7096\n",
      "Total loss:  -2.6397 | PDE Loss:  -4.4639 | Function Loss:  -2.7102\n",
      "Total loss:  -2.6401 | PDE Loss:  -4.4633 | Function Loss:  -2.7108\n",
      "Total loss:  -2.6405 | PDE Loss:  -4.4608 | Function Loss:  -2.7117\n",
      "Total loss:  -2.6409 | PDE Loss:  -4.4604 | Function Loss:  -2.7122\n",
      "Total loss:  -2.6413 | PDE Loss:  -4.4586 | Function Loss:  -2.713\n",
      "Total loss:  -2.6417 | PDE Loss:  -4.4569 | Function Loss:  -2.7138\n",
      "Total loss:  -2.6422 | PDE Loss:  -4.454 | Function Loss:  -2.7149\n",
      "Total loss:  -2.6426 | PDE Loss:  -4.4523 | Function Loss:  -2.7158\n",
      "Total loss:  -2.6432 | PDE Loss:  -4.4477 | Function Loss:  -2.7173\n",
      "Total loss:  -2.6437 | PDE Loss:  -4.4487 | Function Loss:  -2.7177\n",
      "Total loss:  -2.6441 | PDE Loss:  -4.4487 | Function Loss:  -2.7182\n",
      "Total loss:  -2.6446 | PDE Loss:  -4.4506 | Function Loss:  -2.7184\n",
      "Total loss:  -2.6449 | PDE Loss:  -4.4526 | Function Loss:  -2.7184\n",
      "Total loss:  -2.6452 | PDE Loss:  -4.4537 | Function Loss:  -2.7186\n",
      "Total loss:  -2.6456 | PDE Loss:  -4.4559 | Function Loss:  -2.7186\n",
      "Total loss:  -2.6459 | PDE Loss:  -4.4565 | Function Loss:  -2.7189\n",
      "Total loss:  -2.6463 | PDE Loss:  -4.457 | Function Loss:  -2.7192\n",
      "Total loss:  -2.6467 | PDE Loss:  -4.4553 | Function Loss:  -2.72\n",
      "Total loss:  -2.647 | PDE Loss:  -4.4534 | Function Loss:  -2.7207\n",
      "Total loss:  -2.6472 | PDE Loss:  -4.4513 | Function Loss:  -2.7214\n",
      "Total loss:  -2.6475 | PDE Loss:  -4.4495 | Function Loss:  -2.7221\n",
      "Total loss:  -2.6478 | PDE Loss:  -4.4479 | Function Loss:  -2.7227\n",
      "Total loss:  -2.6481 | PDE Loss:  -4.4462 | Function Loss:  -2.7234\n",
      "Total loss:  -2.6484 | PDE Loss:  -4.4461 | Function Loss:  -2.7237\n",
      "Total loss:  -2.6487 | PDE Loss:  -4.4453 | Function Loss:  -2.7243\n",
      "Total loss:  -2.6491 | PDE Loss:  -4.4458 | Function Loss:  -2.7247\n",
      "Total loss:  -2.6497 | PDE Loss:  -4.4465 | Function Loss:  -2.7253\n",
      "Total loss:  -2.6504 | PDE Loss:  -4.4484 | Function Loss:  -2.7257\n",
      "Total loss:  -2.651 | PDE Loss:  -4.4518 | Function Loss:  -2.7258\n",
      "Total loss:  -2.6519 | PDE Loss:  -4.4556 | Function Loss:  -2.7262\n",
      "Total loss:  -2.6528 | PDE Loss:  -4.4607 | Function Loss:  -2.7263\n",
      "Total loss:  -2.6537 | PDE Loss:  -4.4634 | Function Loss:  -2.7268\n",
      "Total loss:  -2.655 | PDE Loss:  -4.4656 | Function Loss:  -2.728\n",
      "Total loss:  -2.6569 | PDE Loss:  -4.4658 | Function Loss:  -2.7301\n",
      "Total loss:  -2.6589 | PDE Loss:  -4.4611 | Function Loss:  -2.7334\n",
      "Total loss:  -2.6611 | PDE Loss:  -4.4565 | Function Loss:  -2.7369\n",
      "Total loss:  -2.6625 | PDE Loss:  -4.4522 | Function Loss:  -2.7394\n",
      "Total loss:  -2.6647 | PDE Loss:  -4.4479 | Function Loss:  -2.7429\n",
      "Total loss:  -2.6664 | PDE Loss:  -4.4435 | Function Loss:  -2.7458\n",
      "Total loss:  -2.6677 | PDE Loss:  -4.4414 | Function Loss:  -2.7477\n",
      "Total loss:  -2.6687 | PDE Loss:  -4.4432 | Function Loss:  -2.7486\n",
      "Total loss:  -2.6699 | PDE Loss:  -4.4442 | Function Loss:  -2.7499\n",
      "Total loss:  -2.6715 | PDE Loss:  -4.4487 | Function Loss:  -2.7509\n",
      "Total loss:  -2.673 | PDE Loss:  -4.4521 | Function Loss:  -2.752\n",
      "Total loss:  -2.6741 | PDE Loss:  -4.4543 | Function Loss:  -2.7529\n",
      "Total loss:  -2.6752 | PDE Loss:  -4.4568 | Function Loss:  -2.7537\n",
      "Total loss:  -2.6761 | PDE Loss:  -4.4567 | Function Loss:  -2.7548\n",
      "Total loss:  -2.6766 | PDE Loss:  -4.4563 | Function Loss:  -2.7555\n",
      "Total loss:  -2.6771 | PDE Loss:  -4.4557 | Function Loss:  -2.7562\n",
      "Total loss:  -2.6775 | PDE Loss:  -4.4556 | Function Loss:  -2.7566\n",
      "Total loss:  -2.6777 | PDE Loss:  -4.4544 | Function Loss:  -2.7572\n",
      "Total loss:  -2.6779 | PDE Loss:  -4.4559 | Function Loss:  -2.7571\n",
      "Total loss:  -2.678 | PDE Loss:  -4.4564 | Function Loss:  -2.7572\n",
      "Total loss:  -2.6782 | PDE Loss:  -4.4578 | Function Loss:  -2.7571\n",
      "Total loss:  -2.6785 | PDE Loss:  -4.4598 | Function Loss:  -2.757\n",
      "Total loss:  -2.6788 | PDE Loss:  -4.4622 | Function Loss:  -2.7569\n",
      "Total loss:  -2.6791 | PDE Loss:  -4.4656 | Function Loss:  -2.7566\n",
      "Total loss:  -2.6793 | PDE Loss:  -4.4679 | Function Loss:  -2.7565\n",
      "Total loss:  -2.6796 | PDE Loss:  -4.47 | Function Loss:  -2.7563\n",
      "Total loss:  -2.6799 | PDE Loss:  -4.4723 | Function Loss:  -2.7563\n",
      "Total loss:  -2.6803 | PDE Loss:  -4.4744 | Function Loss:  -2.7563\n",
      "Total loss:  -2.6806 | PDE Loss:  -4.4768 | Function Loss:  -2.7562\n",
      "Total loss:  -2.6809 | PDE Loss:  -4.4782 | Function Loss:  -2.7564\n",
      "Total loss:  -2.6814 | PDE Loss:  -4.4808 | Function Loss:  -2.7565\n",
      "Total loss:  -2.6821 | PDE Loss:  -4.4822 | Function Loss:  -2.757\n",
      "Total loss:  -2.6828 | PDE Loss:  -4.4882 | Function Loss:  -2.7567\n",
      "Total loss:  -2.6833 | PDE Loss:  -4.4918 | Function Loss:  -2.7567\n",
      "Total loss:  -2.6843 | PDE Loss:  -4.4984 | Function Loss:  -2.7566\n",
      "Total loss:  -2.6854 | PDE Loss:  -4.5053 | Function Loss:  -2.7566\n",
      "Total loss:  -2.6862 | PDE Loss:  -4.5106 | Function Loss:  -2.7567\n",
      "Total loss:  -2.687 | PDE Loss:  -4.5151 | Function Loss:  -2.7569\n",
      "Total loss:  -2.6878 | PDE Loss:  -4.5185 | Function Loss:  -2.7572\n",
      "Total loss:  -2.6885 | PDE Loss:  -4.5209 | Function Loss:  -2.7576\n",
      "Total loss:  -2.689 | PDE Loss:  -4.5224 | Function Loss:  -2.758\n",
      "Total loss:  -2.6896 | PDE Loss:  -4.5229 | Function Loss:  -2.7585\n",
      "Total loss:  -2.6901 | PDE Loss:  -4.5236 | Function Loss:  -2.759\n",
      "Total loss:  -2.6907 | PDE Loss:  -4.5238 | Function Loss:  -2.7596\n",
      "Total loss:  -2.6913 | PDE Loss:  -4.5235 | Function Loss:  -2.7605\n",
      "Total loss:  -2.6921 | PDE Loss:  -4.524 | Function Loss:  -2.7613\n",
      "Total loss:  -2.693 | PDE Loss:  -4.524 | Function Loss:  -2.7623\n",
      "Total loss:  -2.6938 | PDE Loss:  -4.5255 | Function Loss:  -2.7631\n",
      "Total loss:  -2.6947 | PDE Loss:  -4.5261 | Function Loss:  -2.764\n",
      "Total loss:  -2.6953 | PDE Loss:  -4.5284 | Function Loss:  -2.7643\n",
      "Total loss:  -2.696 | PDE Loss:  -4.5306 | Function Loss:  -2.7647\n",
      "Total loss:  -2.6966 | PDE Loss:  -4.5349 | Function Loss:  -2.7646\n",
      "Total loss:  -2.697 | PDE Loss:  -4.5366 | Function Loss:  -2.7649\n",
      "Total loss:  -2.6975 | PDE Loss:  -4.5392 | Function Loss:  -2.765\n",
      "Total loss:  -2.698 | PDE Loss:  -4.5409 | Function Loss:  -2.7653\n",
      "Total loss:  -2.6987 | PDE Loss:  -4.5445 | Function Loss:  -2.7655\n",
      "Total loss:  -2.6993 | PDE Loss:  -4.5464 | Function Loss:  -2.7659\n",
      "Total loss:  -2.6998 | PDE Loss:  -4.5478 | Function Loss:  -2.7663\n",
      "Total loss:  -2.7005 | PDE Loss:  -4.5491 | Function Loss:  -2.7669\n",
      "Total loss:  -2.7012 | PDE Loss:  -4.5503 | Function Loss:  -2.7675\n",
      "Total loss:  -2.7018 | PDE Loss:  -4.5514 | Function Loss:  -2.768\n",
      "Total loss:  -2.7023 | PDE Loss:  -4.5523 | Function Loss:  -2.7685\n",
      "Total loss:  -2.7028 | PDE Loss:  -4.5521 | Function Loss:  -2.769\n",
      "Total loss:  -2.7032 | PDE Loss:  -4.5523 | Function Loss:  -2.7695\n",
      "Total loss:  -2.7037 | PDE Loss:  -4.5519 | Function Loss:  -2.7701\n",
      "Total loss:  -2.7042 | PDE Loss:  -4.5518 | Function Loss:  -2.7707\n",
      "Total loss:  -2.7047 | PDE Loss:  -4.5511 | Function Loss:  -2.7714\n",
      "Total loss:  -2.7051 | PDE Loss:  -4.5516 | Function Loss:  -2.7718\n",
      "Total loss:  -2.7055 | PDE Loss:  -4.5535 | Function Loss:  -2.7719\n",
      "Total loss:  -2.7059 | PDE Loss:  -4.5537 | Function Loss:  -2.7724\n",
      "Total loss:  -2.7063 | PDE Loss:  -4.5552 | Function Loss:  -2.7726\n",
      "Total loss:  -2.7068 | PDE Loss:  -4.5564 | Function Loss:  -2.773\n",
      "Total loss:  -2.7073 | PDE Loss:  -4.558 | Function Loss:  -2.7733\n",
      "Total loss:  -2.7076 | PDE Loss:  -4.5595 | Function Loss:  -2.7735\n",
      "Total loss:  -2.708 | PDE Loss:  -4.5602 | Function Loss:  -2.7737\n",
      "Total loss:  -2.7082 | PDE Loss:  -4.5611 | Function Loss:  -2.7739\n",
      "Total loss:  -2.7085 | PDE Loss:  -4.5622 | Function Loss:  -2.774\n",
      "Total loss:  -2.7087 | PDE Loss:  -4.5631 | Function Loss:  -2.7741\n",
      "Total loss:  -2.7089 | PDE Loss:  -4.5642 | Function Loss:  -2.7742\n",
      "Total loss:  -2.7092 | PDE Loss:  -4.5657 | Function Loss:  -2.7743\n",
      "Total loss:  -2.7096 | PDE Loss:  -4.5675 | Function Loss:  -2.7744\n",
      "Total loss:  -2.71 | PDE Loss:  -4.5694 | Function Loss:  -2.7746\n",
      "Total loss:  -2.7105 | PDE Loss:  -4.5706 | Function Loss:  -2.775\n",
      "Total loss:  -2.7109 | PDE Loss:  -4.5716 | Function Loss:  -2.7753\n",
      "Total loss:  -2.7113 | PDE Loss:  -4.5719 | Function Loss:  -2.7757\n",
      "Total loss:  -2.7119 | PDE Loss:  -4.5716 | Function Loss:  -2.7764\n",
      "Total loss:  -2.7124 | PDE Loss:  -4.5717 | Function Loss:  -2.777\n",
      "Total loss:  -2.7128 | PDE Loss:  -4.5706 | Function Loss:  -2.7776\n",
      "Total loss:  -2.7133 | PDE Loss:  -4.5704 | Function Loss:  -2.7783\n",
      "Total loss:  -2.7138 | PDE Loss:  -4.5686 | Function Loss:  -2.7791\n",
      "Total loss:  -2.7142 | PDE Loss:  -4.5725 | Function Loss:  -2.779\n",
      "Total loss:  -2.7147 | PDE Loss:  -4.5697 | Function Loss:  -2.78\n",
      "Total loss:  -2.715 | PDE Loss:  -4.5691 | Function Loss:  -2.7805\n",
      "Total loss:  -2.7156 | PDE Loss:  -4.5684 | Function Loss:  -2.7813\n",
      "Total loss:  -2.7163 | PDE Loss:  -4.5671 | Function Loss:  -2.7824\n",
      "Total loss:  -2.7172 | PDE Loss:  -4.5667 | Function Loss:  -2.7834\n",
      "Total loss:  -2.7181 | PDE Loss:  -4.5637 | Function Loss:  -2.7849\n",
      "Total loss:  -2.719 | PDE Loss:  -4.5616 | Function Loss:  -2.7863\n",
      "Total loss:  -2.7197 | PDE Loss:  -4.559 | Function Loss:  -2.7876\n",
      "Total loss:  -2.7203 | PDE Loss:  -4.5589 | Function Loss:  -2.7883\n",
      "Total loss:  -2.7214 | PDE Loss:  -4.5592 | Function Loss:  -2.7896\n",
      "Total loss:  -2.7225 | PDE Loss:  -4.5599 | Function Loss:  -2.7908\n",
      "Total loss:  -2.7234 | PDE Loss:  -4.5602 | Function Loss:  -2.7917\n",
      "Total loss:  -2.7242 | PDE Loss:  -4.5609 | Function Loss:  -2.7925\n",
      "Total loss:  -2.725 | PDE Loss:  -4.5627 | Function Loss:  -2.7932\n",
      "Total loss:  -2.7258 | PDE Loss:  -4.5631 | Function Loss:  -2.794\n",
      "Total loss:  -2.7265 | PDE Loss:  -4.5638 | Function Loss:  -2.7947\n",
      "Total loss:  -2.7272 | PDE Loss:  -4.5659 | Function Loss:  -2.7952\n",
      "Total loss:  -2.7279 | PDE Loss:  -4.568 | Function Loss:  -2.7957\n",
      "Total loss:  -2.7289 | PDE Loss:  -4.5712 | Function Loss:  -2.7963\n",
      "Total loss:  -2.7298 | PDE Loss:  -4.5743 | Function Loss:  -2.7969\n",
      "Total loss:  -2.7305 | PDE Loss:  -4.577 | Function Loss:  -2.7972\n",
      "Total loss:  -2.7313 | PDE Loss:  -4.5784 | Function Loss:  -2.7979\n",
      "Total loss:  -2.732 | PDE Loss:  -4.5797 | Function Loss:  -2.7985\n",
      "Total loss:  -2.7326 | PDE Loss:  -4.5796 | Function Loss:  -2.7992\n",
      "Total loss:  -2.7331 | PDE Loss:  -4.5781 | Function Loss:  -2.8\n",
      "Total loss:  -2.7336 | PDE Loss:  -4.576 | Function Loss:  -2.801\n",
      "Total loss:  -2.734 | PDE Loss:  -4.5738 | Function Loss:  -2.8019\n",
      "Total loss:  -2.7344 | PDE Loss:  -4.5735 | Function Loss:  -2.8024\n",
      "Total loss:  -2.7348 | PDE Loss:  -4.5716 | Function Loss:  -2.8032\n",
      "Total loss:  -2.7351 | PDE Loss:  -4.5724 | Function Loss:  -2.8034\n",
      "Total loss:  -2.7354 | PDE Loss:  -4.5728 | Function Loss:  -2.8036\n",
      "Total loss:  -2.7357 | PDE Loss:  -4.5738 | Function Loss:  -2.8038\n",
      "Total loss:  -2.7361 | PDE Loss:  -4.5747 | Function Loss:  -2.8041\n",
      "Total loss:  -2.7365 | PDE Loss:  -4.5757 | Function Loss:  -2.8044\n",
      "Total loss:  -2.737 | PDE Loss:  -4.5764 | Function Loss:  -2.8049\n",
      "Total loss:  -2.7377 | PDE Loss:  -4.575 | Function Loss:  -2.8059\n",
      "Total loss:  -2.7383 | PDE Loss:  -4.5758 | Function Loss:  -2.8065\n",
      "Total loss:  -2.7392 | PDE Loss:  -4.5757 | Function Loss:  -2.8076\n",
      "Total loss:  -2.7403 | PDE Loss:  -4.5757 | Function Loss:  -2.8088\n",
      "Total loss:  -2.7412 | PDE Loss:  -4.5743 | Function Loss:  -2.8102\n",
      "Total loss:  -2.7421 | PDE Loss:  -4.5739 | Function Loss:  -2.8113\n",
      "Total loss:  -2.7428 | PDE Loss:  -4.5735 | Function Loss:  -2.8122\n",
      "Total loss:  -2.7435 | PDE Loss:  -4.5745 | Function Loss:  -2.8128\n",
      "Total loss:  -2.7442 | PDE Loss:  -4.5758 | Function Loss:  -2.8134\n",
      "Total loss:  -2.745 | PDE Loss:  -4.5787 | Function Loss:  -2.8138\n",
      "Total loss:  -2.7459 | PDE Loss:  -4.5799 | Function Loss:  -2.8147\n",
      "Total loss:  -2.7466 | PDE Loss:  -4.5827 | Function Loss:  -2.8151\n",
      "Total loss:  -2.7474 | PDE Loss:  -4.5845 | Function Loss:  -2.8157\n",
      "Total loss:  -2.7482 | PDE Loss:  -4.5862 | Function Loss:  -2.8164\n",
      "Total loss:  -2.749 | PDE Loss:  -4.5854 | Function Loss:  -2.8174\n",
      "Total loss:  -2.7498 | PDE Loss:  -4.5854 | Function Loss:  -2.8183\n",
      "Total loss:  -2.7505 | PDE Loss:  -4.5835 | Function Loss:  -2.8195\n",
      "Total loss:  -2.7512 | PDE Loss:  -4.5816 | Function Loss:  -2.8206\n",
      "Total loss:  -2.752 | PDE Loss:  -4.5817 | Function Loss:  -2.8215\n",
      "Total loss:  -2.7527 | PDE Loss:  -4.5795 | Function Loss:  -2.8228\n",
      "Total loss:  -2.7535 | PDE Loss:  -4.5795 | Function Loss:  -2.8237\n",
      "Total loss:  -2.7544 | PDE Loss:  -4.5771 | Function Loss:  -2.8252\n",
      "Total loss:  -2.7554 | PDE Loss:  -4.5785 | Function Loss:  -2.8261\n",
      "Total loss:  -2.7562 | PDE Loss:  -4.5774 | Function Loss:  -2.8273\n",
      "Total loss:  -2.757 | PDE Loss:  -4.5794 | Function Loss:  -2.8278\n",
      "Total loss:  -2.7575 | PDE Loss:  -4.5794 | Function Loss:  -2.8284\n",
      "Total loss:  -2.758 | PDE Loss:  -4.5805 | Function Loss:  -2.8288\n",
      "Total loss:  -2.7584 | PDE Loss:  -4.5802 | Function Loss:  -2.8294\n",
      "Total loss:  -2.7588 | PDE Loss:  -4.5802 | Function Loss:  -2.8298\n",
      "Total loss:  -2.7591 | PDE Loss:  -4.5794 | Function Loss:  -2.8303\n",
      "Total loss:  -2.7595 | PDE Loss:  -4.5781 | Function Loss:  -2.831\n",
      "Total loss:  -2.7599 | PDE Loss:  -4.5769 | Function Loss:  -2.8317\n",
      "Total loss:  -2.7602 | PDE Loss:  -4.5756 | Function Loss:  -2.8323\n",
      "Total loss:  -2.7606 | PDE Loss:  -4.5746 | Function Loss:  -2.8329\n",
      "Total loss:  -2.7609 | PDE Loss:  -4.5745 | Function Loss:  -2.8333\n",
      "Total loss:  -2.7612 | PDE Loss:  -4.5732 | Function Loss:  -2.8339\n",
      "Total loss:  -2.7615 | PDE Loss:  -4.5734 | Function Loss:  -2.8342\n",
      "Total loss:  -2.7617 | PDE Loss:  -4.574 | Function Loss:  -2.8344\n",
      "Total loss:  -2.7621 | PDE Loss:  -4.5742 | Function Loss:  -2.8348\n",
      "Total loss:  -2.7624 | PDE Loss:  -4.5759 | Function Loss:  -2.8349\n",
      "Total loss:  -2.7628 | PDE Loss:  -4.5758 | Function Loss:  -2.8353\n",
      "Total loss:  -2.7631 | PDE Loss:  -4.5767 | Function Loss:  -2.8355\n",
      "Total loss:  -2.7634 | PDE Loss:  -4.5759 | Function Loss:  -2.836\n",
      "Total loss:  -2.7637 | PDE Loss:  -4.5749 | Function Loss:  -2.8365\n",
      "Total loss:  -2.764 | PDE Loss:  -4.5736 | Function Loss:  -2.8372\n",
      "Total loss:  -2.7645 | PDE Loss:  -4.5697 | Function Loss:  -2.8384\n",
      "Total loss:  -2.7649 | PDE Loss:  -4.5687 | Function Loss:  -2.8391\n",
      "Total loss:  -2.7653 | PDE Loss:  -4.567 | Function Loss:  -2.84\n",
      "Total loss:  -2.7659 | PDE Loss:  -4.5673 | Function Loss:  -2.8406\n",
      "Total loss:  -2.7665 | PDE Loss:  -4.567 | Function Loss:  -2.8414\n",
      "Total loss:  -2.767 | PDE Loss:  -4.5686 | Function Loss:  -2.8416\n",
      "Total loss:  -2.7675 | PDE Loss:  -4.5708 | Function Loss:  -2.8418\n",
      "Total loss:  -2.7681 | PDE Loss:  -4.573 | Function Loss:  -2.8422\n",
      "Total loss:  -2.7689 | PDE Loss:  -4.5766 | Function Loss:  -2.8424\n",
      "Total loss:  -2.7691 | PDE Loss:  -4.5743 | Function Loss:  -2.8431\n",
      "Total loss:  -2.7698 | PDE Loss:  -4.5777 | Function Loss:  -2.8433\n",
      "Total loss:  -2.7704 | PDE Loss:  -4.5807 | Function Loss:  -2.8434\n",
      "Total loss:  -2.7709 | PDE Loss:  -4.5819 | Function Loss:  -2.8438\n",
      "Total loss:  -2.7713 | PDE Loss:  -4.5825 | Function Loss:  -2.8442\n",
      "Total loss:  -2.7717 | PDE Loss:  -4.5824 | Function Loss:  -2.8446\n",
      "Total loss:  -2.7721 | PDE Loss:  -4.5809 | Function Loss:  -2.8453\n",
      "Total loss:  -2.7724 | PDE Loss:  -4.5807 | Function Loss:  -2.8458\n",
      "Total loss:  -2.7727 | PDE Loss:  -4.5781 | Function Loss:  -2.8467\n",
      "Total loss:  -2.7731 | PDE Loss:  -4.5773 | Function Loss:  -2.8472\n",
      "Total loss:  -2.7735 | PDE Loss:  -4.5753 | Function Loss:  -2.848\n",
      "Total loss:  -2.7738 | PDE Loss:  -4.5727 | Function Loss:  -2.8489\n",
      "Total loss:  -2.7741 | PDE Loss:  -4.572 | Function Loss:  -2.8495\n",
      "Total loss:  -2.7744 | PDE Loss:  -4.5716 | Function Loss:  -2.8499\n",
      "Total loss:  -2.7747 | PDE Loss:  -4.5702 | Function Loss:  -2.8505\n",
      "Total loss:  -2.775 | PDE Loss:  -4.5701 | Function Loss:  -2.8509\n",
      "Total loss:  -2.7753 | PDE Loss:  -4.5693 | Function Loss:  -2.8514\n",
      "Total loss:  -2.7756 | PDE Loss:  -4.569 | Function Loss:  -2.8518\n",
      "Total loss:  -2.7759 | PDE Loss:  -4.5678 | Function Loss:  -2.8524\n",
      "Total loss:  -2.7762 | PDE Loss:  -4.5667 | Function Loss:  -2.853\n",
      "Total loss:  -2.7767 | PDE Loss:  -4.565 | Function Loss:  -2.8538\n",
      "Total loss:  -2.7771 | PDE Loss:  -4.5635 | Function Loss:  -2.8546\n",
      "Total loss:  -2.7775 | PDE Loss:  -4.5623 | Function Loss:  -2.8553\n",
      "Total loss:  -2.7779 | PDE Loss:  -4.5618 | Function Loss:  -2.8559\n",
      "Total loss:  -2.7783 | PDE Loss:  -4.5614 | Function Loss:  -2.8566\n",
      "Total loss:  -2.7787 | PDE Loss:  -4.5611 | Function Loss:  -2.857\n",
      "Total loss:  -2.779 | PDE Loss:  -4.5627 | Function Loss:  -2.8571\n",
      "Total loss:  -2.7795 | PDE Loss:  -4.5644 | Function Loss:  -2.8573\n",
      "Total loss:  -2.78 | PDE Loss:  -4.5668 | Function Loss:  -2.8575\n",
      "Total loss:  -2.7806 | PDE Loss:  -4.5687 | Function Loss:  -2.8578\n",
      "Total loss:  -2.7812 | PDE Loss:  -4.5703 | Function Loss:  -2.8583\n",
      "Total loss:  -2.782 | PDE Loss:  -4.5704 | Function Loss:  -2.8592\n",
      "Total loss:  -2.7828 | PDE Loss:  -4.5695 | Function Loss:  -2.8603\n",
      "Total loss:  -2.7835 | PDE Loss:  -4.5683 | Function Loss:  -2.8613\n",
      "Total loss:  -2.7841 | PDE Loss:  -4.5663 | Function Loss:  -2.8624\n",
      "Total loss:  -2.7846 | PDE Loss:  -4.5648 | Function Loss:  -2.8634\n",
      "Total loss:  -2.7851 | PDE Loss:  -4.5642 | Function Loss:  -2.8641\n",
      "Total loss:  -2.7856 | PDE Loss:  -4.5634 | Function Loss:  -2.8649\n",
      "Total loss:  -2.7861 | PDE Loss:  -4.5645 | Function Loss:  -2.8653\n",
      "Total loss:  -2.7867 | PDE Loss:  -4.5651 | Function Loss:  -2.8658\n",
      "Total loss:  -2.7873 | PDE Loss:  -4.567 | Function Loss:  -2.8662\n",
      "Total loss:  -2.788 | PDE Loss:  -4.5697 | Function Loss:  -2.8664\n",
      "Total loss:  -2.7885 | PDE Loss:  -4.5715 | Function Loss:  -2.8668\n",
      "Total loss:  -2.7892 | PDE Loss:  -4.5744 | Function Loss:  -2.867\n",
      "Total loss:  -2.7897 | PDE Loss:  -4.5756 | Function Loss:  -2.8674\n",
      "Total loss:  -2.7901 | PDE Loss:  -4.577 | Function Loss:  -2.8675\n",
      "Total loss:  -2.7904 | PDE Loss:  -4.5774 | Function Loss:  -2.8678\n",
      "Total loss:  -2.7906 | PDE Loss:  -4.5774 | Function Loss:  -2.8681\n",
      "Total loss:  -2.7909 | PDE Loss:  -4.5775 | Function Loss:  -2.8684\n",
      "Total loss:  -2.7913 | PDE Loss:  -4.5766 | Function Loss:  -2.8691\n",
      "Total loss:  -2.7918 | PDE Loss:  -4.5769 | Function Loss:  -2.8697\n",
      "Total loss:  -2.7923 | PDE Loss:  -4.5764 | Function Loss:  -2.8703\n",
      "Total loss:  -2.7929 | PDE Loss:  -4.5771 | Function Loss:  -2.8708\n",
      "Total loss:  -2.7935 | PDE Loss:  -4.5783 | Function Loss:  -2.8714\n",
      "Total loss:  -2.7941 | PDE Loss:  -4.5791 | Function Loss:  -2.8719\n",
      "Total loss:  -2.7947 | PDE Loss:  -4.5816 | Function Loss:  -2.8721\n",
      "Total loss:  -2.7953 | PDE Loss:  -4.5826 | Function Loss:  -2.8727\n",
      "Total loss:  -2.796 | PDE Loss:  -4.5852 | Function Loss:  -2.873\n",
      "Total loss:  -2.7967 | PDE Loss:  -4.5863 | Function Loss:  -2.8736\n",
      "Total loss:  -2.7974 | PDE Loss:  -4.5879 | Function Loss:  -2.8741\n",
      "Total loss:  -2.7979 | PDE Loss:  -4.5883 | Function Loss:  -2.8747\n",
      "Total loss:  -2.7985 | PDE Loss:  -4.5879 | Function Loss:  -2.8755\n",
      "Total loss:  -2.7991 | PDE Loss:  -4.5874 | Function Loss:  -2.8763\n",
      "Total loss:  -2.7997 | PDE Loss:  -4.5855 | Function Loss:  -2.8774\n",
      "Total loss:  -2.8004 | PDE Loss:  -4.5847 | Function Loss:  -2.8783\n",
      "Total loss:  -2.8011 | PDE Loss:  -4.5832 | Function Loss:  -2.8795\n",
      "Total loss:  -2.8018 | PDE Loss:  -4.5824 | Function Loss:  -2.8804\n",
      "Total loss:  -2.8024 | PDE Loss:  -4.5823 | Function Loss:  -2.8813\n",
      "Total loss:  -2.8031 | PDE Loss:  -4.5822 | Function Loss:  -2.882\n",
      "Total loss:  -2.8037 | PDE Loss:  -4.583 | Function Loss:  -2.8826\n",
      "Total loss:  -2.8043 | PDE Loss:  -4.5834 | Function Loss:  -2.8833\n",
      "Total loss:  -2.805 | PDE Loss:  -4.5838 | Function Loss:  -2.8841\n",
      "Total loss:  -2.8059 | PDE Loss:  -4.5832 | Function Loss:  -2.8852\n",
      "Total loss:  -2.8068 | PDE Loss:  -4.5817 | Function Loss:  -2.8867\n",
      "Total loss:  -2.8078 | PDE Loss:  -4.579 | Function Loss:  -2.8884\n",
      "Total loss:  -2.8089 | PDE Loss:  -4.5746 | Function Loss:  -2.8906\n",
      "Total loss:  -2.81 | PDE Loss:  -4.5696 | Function Loss:  -2.893\n",
      "Total loss:  -2.811 | PDE Loss:  -4.5641 | Function Loss:  -2.8953\n",
      "Total loss:  -2.8118 | PDE Loss:  -4.5586 | Function Loss:  -2.8976\n",
      "Total loss:  -2.8126 | PDE Loss:  -4.5549 | Function Loss:  -2.8994\n",
      "Total loss:  -2.8132 | PDE Loss:  -4.5513 | Function Loss:  -2.9008\n",
      "Total loss:  -2.8137 | PDE Loss:  -4.5506 | Function Loss:  -2.9016\n",
      "Total loss:  -2.8141 | PDE Loss:  -4.5494 | Function Loss:  -2.9024\n",
      "Total loss:  -2.8145 | PDE Loss:  -4.5501 | Function Loss:  -2.9028\n",
      "Total loss:  -2.815 | PDE Loss:  -4.5508 | Function Loss:  -2.9031\n",
      "Total loss:  -2.8155 | PDE Loss:  -4.5522 | Function Loss:  -2.9035\n",
      "Total loss:  -2.8161 | PDE Loss:  -4.5547 | Function Loss:  -2.9037\n",
      "Total loss:  -2.8162 | PDE Loss:  -4.5482 | Function Loss:  -2.9052\n",
      "Total loss:  -2.8165 | PDE Loss:  -4.5521 | Function Loss:  -2.9047\n",
      "Total loss:  -2.8171 | PDE Loss:  -4.5558 | Function Loss:  -2.9046\n",
      "Total loss:  -2.8177 | PDE Loss:  -4.5583 | Function Loss:  -2.9048\n",
      "Total loss:  -2.8184 | PDE Loss:  -4.5609 | Function Loss:  -2.9051\n",
      "Total loss:  -2.8191 | PDE Loss:  -4.5619 | Function Loss:  -2.9057\n",
      "Total loss:  -2.8197 | PDE Loss:  -4.5625 | Function Loss:  -2.9063\n",
      "Total loss:  -2.8201 | PDE Loss:  -4.562 | Function Loss:  -2.9069\n",
      "Total loss:  -2.8205 | PDE Loss:  -4.5607 | Function Loss:  -2.9077\n",
      "Total loss:  -2.8209 | PDE Loss:  -4.56 | Function Loss:  -2.9083\n",
      "Total loss:  -2.8214 | PDE Loss:  -4.5587 | Function Loss:  -2.9092\n",
      "Total loss:  -2.822 | PDE Loss:  -4.5587 | Function Loss:  -2.9099\n",
      "Total loss:  -2.8226 | PDE Loss:  -4.5585 | Function Loss:  -2.9107\n",
      "Total loss:  -2.8232 | PDE Loss:  -4.5601 | Function Loss:  -2.9112\n",
      "Total loss:  -2.8239 | PDE Loss:  -4.5615 | Function Loss:  -2.9116\n",
      "Total loss:  -2.8245 | PDE Loss:  -4.5644 | Function Loss:  -2.9117\n",
      "Total loss:  -2.8248 | PDE Loss:  -4.567 | Function Loss:  -2.9116\n",
      "Total loss:  -2.8255 | PDE Loss:  -4.5696 | Function Loss:  -2.9118\n",
      "Total loss:  -2.8257 | PDE Loss:  -4.5711 | Function Loss:  -2.9118\n",
      "Total loss:  -2.8262 | PDE Loss:  -4.5728 | Function Loss:  -2.912\n",
      "Total loss:  -2.8266 | PDE Loss:  -4.5746 | Function Loss:  -2.9121\n",
      "Total loss:  -2.827 | PDE Loss:  -4.5749 | Function Loss:  -2.9125\n",
      "Total loss:  -2.8275 | PDE Loss:  -4.5754 | Function Loss:  -2.913\n",
      "Total loss:  -2.8279 | PDE Loss:  -4.5757 | Function Loss:  -2.9134\n",
      "Total loss:  -2.8282 | PDE Loss:  -4.574 | Function Loss:  -2.9142\n",
      "Total loss:  -2.8285 | PDE Loss:  -4.5737 | Function Loss:  -2.9146\n",
      "Total loss:  -2.8289 | PDE Loss:  -4.5733 | Function Loss:  -2.9152\n",
      "Total loss:  -2.8294 | PDE Loss:  -4.5735 | Function Loss:  -2.9157\n",
      "Total loss:  -2.8296 | PDE Loss:  -4.5737 | Function Loss:  -2.916\n",
      "Total loss:  -2.8299 | PDE Loss:  -4.5744 | Function Loss:  -2.9162\n",
      "Total loss:  -2.8303 | PDE Loss:  -4.5756 | Function Loss:  -2.9163\n",
      "Total loss:  -2.8306 | PDE Loss:  -4.5771 | Function Loss:  -2.9164\n",
      "Total loss:  -2.831 | PDE Loss:  -4.5784 | Function Loss:  -2.9166\n",
      "Total loss:  -2.8312 | PDE Loss:  -4.5796 | Function Loss:  -2.9165\n",
      "Total loss:  -2.8315 | PDE Loss:  -4.5805 | Function Loss:  -2.9167\n",
      "Total loss:  -2.8317 | PDE Loss:  -4.5809 | Function Loss:  -2.9168\n",
      "Total loss:  -2.8318 | PDE Loss:  -4.5812 | Function Loss:  -2.917\n",
      "Total loss:  -2.832 | PDE Loss:  -4.5811 | Function Loss:  -2.9172\n",
      "Total loss:  -2.8322 | PDE Loss:  -4.5813 | Function Loss:  -2.9174\n",
      "Total loss:  -2.8323 | PDE Loss:  -4.5813 | Function Loss:  -2.9176\n",
      "Total loss:  -2.8325 | PDE Loss:  -4.5823 | Function Loss:  -2.9176\n",
      "Total loss:  -2.8328 | PDE Loss:  -4.5832 | Function Loss:  -2.9177\n",
      "Total loss:  -2.8331 | PDE Loss:  -4.585 | Function Loss:  -2.9177\n",
      "Total loss:  -2.8335 | PDE Loss:  -4.5883 | Function Loss:  -2.9175\n",
      "Total loss:  -2.8338 | PDE Loss:  -4.5904 | Function Loss:  -2.9174\n",
      "Total loss:  -2.8342 | PDE Loss:  -4.5929 | Function Loss:  -2.9174\n",
      "Total loss:  -2.8346 | PDE Loss:  -4.5949 | Function Loss:  -2.9174\n",
      "Total loss:  -2.8349 | PDE Loss:  -4.5959 | Function Loss:  -2.9176\n",
      "Total loss:  -2.8352 | PDE Loss:  -4.5971 | Function Loss:  -2.9177\n",
      "Total loss:  -2.8356 | PDE Loss:  -4.5977 | Function Loss:  -2.9181\n",
      "Total loss:  -2.8361 | PDE Loss:  -4.599 | Function Loss:  -2.9185\n",
      "Total loss:  -2.8368 | PDE Loss:  -4.5998 | Function Loss:  -2.9191\n",
      "Total loss:  -2.8374 | PDE Loss:  -4.6013 | Function Loss:  -2.9195\n",
      "Total loss:  -2.838 | PDE Loss:  -4.602 | Function Loss:  -2.9201\n",
      "Total loss:  -2.8387 | PDE Loss:  -4.6029 | Function Loss:  -2.9208\n",
      "Total loss:  -2.8394 | PDE Loss:  -4.6029 | Function Loss:  -2.9216\n",
      "Total loss:  -2.84 | PDE Loss:  -4.6038 | Function Loss:  -2.9222\n",
      "Total loss:  -2.8405 | PDE Loss:  -4.6037 | Function Loss:  -2.9227\n",
      "Total loss:  -2.8409 | PDE Loss:  -4.6049 | Function Loss:  -2.923\n",
      "Total loss:  -2.8414 | PDE Loss:  -4.6064 | Function Loss:  -2.9232\n",
      "Total loss:  -2.8419 | PDE Loss:  -4.6088 | Function Loss:  -2.9234\n",
      "Total loss:  -2.8425 | PDE Loss:  -4.611 | Function Loss:  -2.9237\n",
      "Total loss:  -2.843 | PDE Loss:  -4.6129 | Function Loss:  -2.9239\n",
      "Total loss:  -2.8434 | PDE Loss:  -4.6144 | Function Loss:  -2.9241\n",
      "Total loss:  -2.8438 | PDE Loss:  -4.6136 | Function Loss:  -2.9246\n",
      "Total loss:  -2.844 | PDE Loss:  -4.6146 | Function Loss:  -2.9247\n",
      "Total loss:  -2.8442 | PDE Loss:  -4.6145 | Function Loss:  -2.925\n",
      "Total loss:  -2.8445 | PDE Loss:  -4.6136 | Function Loss:  -2.9255\n",
      "Total loss:  -2.8447 | PDE Loss:  -4.6129 | Function Loss:  -2.9259\n",
      "Total loss:  -2.8449 | PDE Loss:  -4.6119 | Function Loss:  -2.9264\n",
      "Total loss:  -2.8452 | PDE Loss:  -4.611 | Function Loss:  -2.9269\n",
      "Total loss:  -2.8451 | PDE Loss:  -4.6094 | Function Loss:  -2.9271\n",
      "Total loss:  -2.8454 | PDE Loss:  -4.6111 | Function Loss:  -2.9271\n",
      "Total loss:  -2.8458 | PDE Loss:  -4.6103 | Function Loss:  -2.9278\n",
      "Total loss:  -2.8462 | PDE Loss:  -4.611 | Function Loss:  -2.9281\n",
      "Total loss:  -2.8466 | PDE Loss:  -4.6109 | Function Loss:  -2.9286\n",
      "Total loss:  -2.8469 | PDE Loss:  -4.6127 | Function Loss:  -2.9287\n",
      "Total loss:  -2.8472 | PDE Loss:  -4.6136 | Function Loss:  -2.9288\n",
      "Total loss:  -2.8475 | PDE Loss:  -4.6158 | Function Loss:  -2.9287\n",
      "Total loss:  -2.8478 | PDE Loss:  -4.6172 | Function Loss:  -2.9288\n",
      "Total loss:  -2.8481 | PDE Loss:  -4.6183 | Function Loss:  -2.9289\n",
      "Total loss:  -2.8484 | PDE Loss:  -4.6191 | Function Loss:  -2.9291\n",
      "Total loss:  -2.8487 | PDE Loss:  -4.6194 | Function Loss:  -2.9294\n",
      "Total loss:  -2.8491 | PDE Loss:  -4.6196 | Function Loss:  -2.9299\n",
      "Total loss:  -2.8489 | PDE Loss:  -4.6126 | Function Loss:  -2.931\n",
      "Total loss:  -2.8494 | PDE Loss:  -4.6179 | Function Loss:  -2.9305\n",
      "Total loss:  -2.8499 | PDE Loss:  -4.6188 | Function Loss:  -2.9309\n",
      "Total loss:  -2.8505 | PDE Loss:  -4.6192 | Function Loss:  -2.9316\n",
      "Total loss:  -2.851 | PDE Loss:  -4.6194 | Function Loss:  -2.9322\n",
      "Total loss:  -2.8514 | PDE Loss:  -4.6194 | Function Loss:  -2.9327\n",
      "Total loss:  -2.8517 | PDE Loss:  -4.619 | Function Loss:  -2.9331\n",
      "Total loss:  -2.852 | PDE Loss:  -4.6181 | Function Loss:  -2.9336\n",
      "Total loss:  -2.8522 | PDE Loss:  -4.6167 | Function Loss:  -2.9342\n",
      "Total loss:  -2.8526 | PDE Loss:  -4.6141 | Function Loss:  -2.9352\n",
      "Total loss:  -2.8531 | PDE Loss:  -4.6073 | Function Loss:  -2.9373\n",
      "Total loss:  -2.8536 | PDE Loss:  -4.6041 | Function Loss:  -2.9386\n",
      "Total loss:  -2.854 | PDE Loss:  -4.6009 | Function Loss:  -2.9397\n",
      "Total loss:  -2.8547 | PDE Loss:  -4.5977 | Function Loss:  -2.9412\n",
      "Total loss:  -2.8552 | PDE Loss:  -4.5956 | Function Loss:  -2.9423\n",
      "Total loss:  -2.8558 | PDE Loss:  -4.594 | Function Loss:  -2.9435\n",
      "Total loss:  -2.8565 | PDE Loss:  -4.5929 | Function Loss:  -2.9446\n",
      "Total loss:  -2.8572 | PDE Loss:  -4.5946 | Function Loss:  -2.9451\n",
      "Total loss:  -2.8578 | PDE Loss:  -4.5951 | Function Loss:  -2.9456\n",
      "Total loss:  -2.8585 | PDE Loss:  -4.5982 | Function Loss:  -2.9458\n",
      "Total loss:  -2.8592 | PDE Loss:  -4.6007 | Function Loss:  -2.9461\n",
      "Total loss:  -2.8597 | PDE Loss:  -4.6042 | Function Loss:  -2.946\n",
      "Total loss:  -2.8601 | PDE Loss:  -4.6071 | Function Loss:  -2.9458\n",
      "Total loss:  -2.8603 | PDE Loss:  -4.6077 | Function Loss:  -2.9459\n",
      "Total loss:  -2.8609 | PDE Loss:  -4.6114 | Function Loss:  -2.9458\n",
      "Total loss:  -2.8616 | PDE Loss:  -4.6124 | Function Loss:  -2.9465\n",
      "Total loss:  -2.8622 | PDE Loss:  -4.6116 | Function Loss:  -2.9474\n",
      "Total loss:  -2.8628 | PDE Loss:  -4.61 | Function Loss:  -2.9485\n",
      "Total loss:  -2.8634 | PDE Loss:  -4.6073 | Function Loss:  -2.9498\n",
      "Total loss:  -2.8639 | PDE Loss:  -4.6046 | Function Loss:  -2.951\n",
      "Total loss:  -2.8645 | PDE Loss:  -4.6027 | Function Loss:  -2.9522\n",
      "Total loss:  -2.8653 | PDE Loss:  -4.6014 | Function Loss:  -2.9534\n",
      "Total loss:  -2.866 | PDE Loss:  -4.6012 | Function Loss:  -2.9543\n",
      "Total loss:  -2.8666 | PDE Loss:  -4.6022 | Function Loss:  -2.9548\n",
      "Total loss:  -2.8671 | PDE Loss:  -4.6047 | Function Loss:  -2.9549\n",
      "Total loss:  -2.8676 | PDE Loss:  -4.607 | Function Loss:  -2.9549\n",
      "Total loss:  -2.8679 | PDE Loss:  -4.609 | Function Loss:  -2.9549\n",
      "Total loss:  -2.8685 | PDE Loss:  -4.6111 | Function Loss:  -2.9551\n",
      "Total loss:  -2.869 | PDE Loss:  -4.6108 | Function Loss:  -2.9559\n",
      "Total loss:  -2.8696 | PDE Loss:  -4.6105 | Function Loss:  -2.9566\n",
      "Total loss:  -2.8701 | PDE Loss:  -4.609 | Function Loss:  -2.9576\n",
      "Total loss:  -2.8706 | PDE Loss:  -4.6067 | Function Loss:  -2.9587\n",
      "Total loss:  -2.8709 | PDE Loss:  -4.605 | Function Loss:  -2.9595\n",
      "Total loss:  -2.8712 | PDE Loss:  -4.6033 | Function Loss:  -2.9602\n",
      "Total loss:  -2.8714 | PDE Loss:  -4.6024 | Function Loss:  -2.9607\n",
      "Total loss:  -2.8717 | PDE Loss:  -4.6018 | Function Loss:  -2.9611\n",
      "Total loss:  -2.872 | PDE Loss:  -4.6019 | Function Loss:  -2.9615\n",
      "Total loss:  -2.8723 | PDE Loss:  -4.6031 | Function Loss:  -2.9616\n",
      "Total loss:  -2.8726 | PDE Loss:  -4.6039 | Function Loss:  -2.9618\n",
      "Total loss:  -2.8729 | PDE Loss:  -4.606 | Function Loss:  -2.9617\n",
      "Total loss:  -2.8732 | PDE Loss:  -4.6081 | Function Loss:  -2.9616\n",
      "Total loss:  -2.8736 | PDE Loss:  -4.6101 | Function Loss:  -2.9616\n",
      "Total loss:  -2.8738 | PDE Loss:  -4.6123 | Function Loss:  -2.9614\n",
      "Total loss:  -2.8741 | PDE Loss:  -4.6137 | Function Loss:  -2.9614\n",
      "Total loss:  -2.8744 | PDE Loss:  -4.615 | Function Loss:  -2.9615\n",
      "Total loss:  -2.8746 | PDE Loss:  -4.6168 | Function Loss:  -2.9614\n",
      "Total loss:  -2.8749 | PDE Loss:  -4.6174 | Function Loss:  -2.9616\n",
      "Total loss:  -2.8752 | PDE Loss:  -4.6185 | Function Loss:  -2.9617\n",
      "Total loss:  -2.8756 | PDE Loss:  -4.6195 | Function Loss:  -2.962\n",
      "Total loss:  -2.876 | PDE Loss:  -4.6213 | Function Loss:  -2.9621\n",
      "Total loss:  -2.8764 | PDE Loss:  -4.622 | Function Loss:  -2.9623\n",
      "Total loss:  -2.8766 | PDE Loss:  -4.6238 | Function Loss:  -2.9623\n",
      "Total loss:  -2.8769 | PDE Loss:  -4.6248 | Function Loss:  -2.9624\n",
      "Total loss:  -2.8774 | PDE Loss:  -4.627 | Function Loss:  -2.9625\n",
      "Total loss:  -2.878 | PDE Loss:  -4.6275 | Function Loss:  -2.9631\n",
      "Total loss:  -2.8785 | PDE Loss:  -4.6294 | Function Loss:  -2.9634\n",
      "Total loss:  -2.8791 | PDE Loss:  -4.6297 | Function Loss:  -2.964\n",
      "Total loss:  -2.8797 | PDE Loss:  -4.6308 | Function Loss:  -2.9645\n",
      "Total loss:  -2.8802 | PDE Loss:  -4.6299 | Function Loss:  -2.9653\n",
      "Total loss:  -2.8807 | PDE Loss:  -4.6316 | Function Loss:  -2.9655\n",
      "Total loss:  -2.881 | PDE Loss:  -4.6321 | Function Loss:  -2.9658\n",
      "Total loss:  -2.8814 | PDE Loss:  -4.6323 | Function Loss:  -2.9662\n",
      "Total loss:  -2.8817 | PDE Loss:  -4.6344 | Function Loss:  -2.9661\n",
      "Total loss:  -2.8819 | PDE Loss:  -4.6356 | Function Loss:  -2.9662\n",
      "Total loss:  -2.8823 | PDE Loss:  -4.6389 | Function Loss:  -2.9659\n",
      "Total loss:  -2.8826 | PDE Loss:  -4.6419 | Function Loss:  -2.9657\n",
      "Total loss:  -2.883 | PDE Loss:  -4.6458 | Function Loss:  -2.9653\n",
      "Total loss:  -2.8834 | PDE Loss:  -4.6506 | Function Loss:  -2.9648\n",
      "Total loss:  -2.8838 | PDE Loss:  -4.6544 | Function Loss:  -2.9645\n",
      "Total loss:  -2.8841 | PDE Loss:  -4.6568 | Function Loss:  -2.9644\n",
      "Total loss:  -2.8843 | PDE Loss:  -4.6584 | Function Loss:  -2.9643\n",
      "Total loss:  -2.8845 | PDE Loss:  -4.6591 | Function Loss:  -2.9644\n",
      "Total loss:  -2.8848 | PDE Loss:  -4.6595 | Function Loss:  -2.9646\n",
      "Total loss:  -2.8851 | PDE Loss:  -4.6602 | Function Loss:  -2.9649\n",
      "Total loss:  -2.8856 | PDE Loss:  -4.6607 | Function Loss:  -2.9654\n",
      "Total loss:  -2.8859 | PDE Loss:  -4.6614 | Function Loss:  -2.9656\n",
      "Total loss:  -2.8861 | PDE Loss:  -4.6614 | Function Loss:  -2.9658\n",
      "Total loss:  -2.8863 | PDE Loss:  -4.6614 | Function Loss:  -2.966\n",
      "Total loss:  -2.8864 | PDE Loss:  -4.6612 | Function Loss:  -2.9663\n",
      "Total loss:  -2.8866 | PDE Loss:  -4.6612 | Function Loss:  -2.9665\n",
      "Total loss:  -2.8868 | PDE Loss:  -4.6607 | Function Loss:  -2.9669\n",
      "Total loss:  -2.887 | PDE Loss:  -4.6603 | Function Loss:  -2.9672\n",
      "Total loss:  -2.8873 | PDE Loss:  -4.6597 | Function Loss:  -2.9676\n",
      "Total loss:  -2.8876 | PDE Loss:  -4.6587 | Function Loss:  -2.9682\n",
      "Total loss:  -2.8879 | PDE Loss:  -4.6576 | Function Loss:  -2.9688\n",
      "Total loss:  -2.8883 | PDE Loss:  -4.6563 | Function Loss:  -2.9695\n",
      "Total loss:  -2.8886 | PDE Loss:  -4.6549 | Function Loss:  -2.9702\n",
      "Total loss:  -2.889 | PDE Loss:  -4.6527 | Function Loss:  -2.9711\n",
      "Total loss:  -2.8893 | PDE Loss:  -4.6507 | Function Loss:  -2.9719\n",
      "Total loss:  -2.8896 | PDE Loss:  -4.6486 | Function Loss:  -2.9727\n",
      "Total loss:  -2.89 | PDE Loss:  -4.6464 | Function Loss:  -2.9737\n",
      "Total loss:  -2.8905 | PDE Loss:  -4.6447 | Function Loss:  -2.9746\n",
      "Total loss:  -2.8908 | PDE Loss:  -4.6434 | Function Loss:  -2.9752\n",
      "Total loss:  -2.891 | PDE Loss:  -4.643 | Function Loss:  -2.9757\n",
      "Total loss:  -2.8913 | PDE Loss:  -4.6429 | Function Loss:  -2.976\n",
      "Total loss:  -2.8915 | PDE Loss:  -4.6428 | Function Loss:  -2.9763\n",
      "Total loss:  -2.8918 | PDE Loss:  -4.643 | Function Loss:  -2.9765\n",
      "Total loss:  -2.892 | PDE Loss:  -4.6432 | Function Loss:  -2.9768\n",
      "Total loss:  -2.8924 | PDE Loss:  -4.6431 | Function Loss:  -2.9772\n",
      "Total loss:  -2.8927 | PDE Loss:  -4.6449 | Function Loss:  -2.9773\n",
      "Total loss:  -2.893 | PDE Loss:  -4.6444 | Function Loss:  -2.9778\n",
      "Total loss:  -2.8933 | PDE Loss:  -4.6441 | Function Loss:  -2.9782\n",
      "Total loss:  -2.8937 | PDE Loss:  -4.6436 | Function Loss:  -2.9788\n",
      "Total loss:  -2.8942 | PDE Loss:  -4.6431 | Function Loss:  -2.9794\n",
      "Total loss:  -2.8947 | PDE Loss:  -4.643 | Function Loss:  -2.9801\n",
      "Total loss:  -2.8953 | PDE Loss:  -4.6426 | Function Loss:  -2.9809\n",
      "Total loss:  -2.8958 | PDE Loss:  -4.6428 | Function Loss:  -2.9815\n",
      "Total loss:  -2.8963 | PDE Loss:  -4.6425 | Function Loss:  -2.9821\n",
      "Total loss:  -2.8967 | PDE Loss:  -4.6436 | Function Loss:  -2.9824\n",
      "Total loss:  -2.897 | PDE Loss:  -4.6434 | Function Loss:  -2.9829\n",
      "Total loss:  -2.8974 | PDE Loss:  -4.6434 | Function Loss:  -2.9833\n",
      "Total loss:  -2.8977 | PDE Loss:  -4.6419 | Function Loss:  -2.984\n",
      "Total loss:  -2.8981 | PDE Loss:  -4.644 | Function Loss:  -2.984\n",
      "Total loss:  -2.8983 | PDE Loss:  -4.6436 | Function Loss:  -2.9844\n",
      "Total loss:  -2.8988 | PDE Loss:  -4.645 | Function Loss:  -2.9847\n",
      "Total loss:  -2.8995 | PDE Loss:  -4.6441 | Function Loss:  -2.9857\n",
      "Total loss:  -2.8999 | PDE Loss:  -4.6447 | Function Loss:  -2.9861\n",
      "Total loss:  -2.9005 | PDE Loss:  -4.6445 | Function Loss:  -2.9869\n",
      "Total loss:  -2.9011 | PDE Loss:  -4.6447 | Function Loss:  -2.9875\n",
      "Total loss:  -2.9017 | PDE Loss:  -4.644 | Function Loss:  -2.9885\n",
      "Total loss:  -2.9023 | PDE Loss:  -4.6444 | Function Loss:  -2.9891\n",
      "Total loss:  -2.9029 | PDE Loss:  -4.6439 | Function Loss:  -2.99\n",
      "Total loss:  -2.9035 | PDE Loss:  -4.6457 | Function Loss:  -2.9903\n",
      "Total loss:  -2.9041 | PDE Loss:  -4.6468 | Function Loss:  -2.9908\n",
      "Total loss:  -2.9046 | PDE Loss:  -4.6498 | Function Loss:  -2.9907\n",
      "Total loss:  -2.9051 | PDE Loss:  -4.6526 | Function Loss:  -2.9907\n",
      "Total loss:  -2.9056 | PDE Loss:  -4.6567 | Function Loss:  -2.9904\n",
      "Total loss:  -2.906 | PDE Loss:  -4.6597 | Function Loss:  -2.9902\n",
      "Total loss:  -2.9064 | PDE Loss:  -4.6632 | Function Loss:  -2.99\n",
      "Total loss:  -2.9069 | PDE Loss:  -4.6667 | Function Loss:  -2.9898\n",
      "Total loss:  -2.9074 | PDE Loss:  -4.6699 | Function Loss:  -2.9898\n",
      "Total loss:  -2.9079 | PDE Loss:  -4.6728 | Function Loss:  -2.9897\n",
      "Total loss:  -2.9083 | PDE Loss:  -4.6745 | Function Loss:  -2.99\n",
      "Total loss:  -2.9088 | PDE Loss:  -4.6765 | Function Loss:  -2.9902\n",
      "Total loss:  -2.9095 | PDE Loss:  -4.679 | Function Loss:  -2.9905\n",
      "Total loss:  -2.9102 | PDE Loss:  -4.68 | Function Loss:  -2.9911\n",
      "Total loss:  -2.9109 | PDE Loss:  -4.6811 | Function Loss:  -2.9917\n",
      "Total loss:  -2.9115 | PDE Loss:  -4.6778 | Function Loss:  -2.9931\n",
      "Total loss:  -2.9121 | PDE Loss:  -4.6806 | Function Loss:  -2.9933\n",
      "Total loss:  -2.9127 | PDE Loss:  -4.6834 | Function Loss:  -2.9934\n",
      "Total loss:  -2.9133 | PDE Loss:  -4.685 | Function Loss:  -2.9938\n",
      "Total loss:  -2.9138 | PDE Loss:  -4.6874 | Function Loss:  -2.9939\n",
      "Total loss:  -2.9143 | PDE Loss:  -4.6892 | Function Loss:  -2.9942\n",
      "Total loss:  -2.9149 | PDE Loss:  -4.6922 | Function Loss:  -2.9942\n",
      "Total loss:  -2.9155 | PDE Loss:  -4.6936 | Function Loss:  -2.9947\n",
      "Total loss:  -2.9162 | PDE Loss:  -4.6982 | Function Loss:  -2.9946\n",
      "Total loss:  -2.9166 | PDE Loss:  -4.6987 | Function Loss:  -2.995\n",
      "Total loss:  -2.9171 | PDE Loss:  -4.6987 | Function Loss:  -2.9956\n",
      "Total loss:  -2.9177 | PDE Loss:  -4.6994 | Function Loss:  -2.9962\n",
      "Total loss:  -2.9184 | PDE Loss:  -4.697 | Function Loss:  -2.9975\n",
      "Total loss:  -2.919 | PDE Loss:  -4.6968 | Function Loss:  -2.9983\n",
      "Total loss:  -2.9195 | PDE Loss:  -4.6942 | Function Loss:  -2.9994\n",
      "Total loss:  -2.92 | PDE Loss:  -4.6935 | Function Loss:  -3.0001\n",
      "Total loss:  -2.9206 | PDE Loss:  -4.6913 | Function Loss:  -3.0012\n",
      "Total loss:  -2.9212 | PDE Loss:  -4.6902 | Function Loss:  -3.0022\n",
      "Total loss:  -2.9217 | PDE Loss:  -4.6888 | Function Loss:  -3.0032\n",
      "Total loss:  -2.9223 | PDE Loss:  -4.6888 | Function Loss:  -3.0038\n",
      "Total loss:  -2.9228 | PDE Loss:  -4.6887 | Function Loss:  -3.0044\n",
      "Total loss:  -2.9231 | PDE Loss:  -4.6903 | Function Loss:  -3.0045\n",
      "Total loss:  -2.9233 | PDE Loss:  -4.6912 | Function Loss:  -3.0046\n",
      "Total loss:  -2.9237 | PDE Loss:  -4.6914 | Function Loss:  -3.005\n",
      "Total loss:  -2.924 | PDE Loss:  -4.692 | Function Loss:  -3.0053\n",
      "Total loss:  -2.9243 | PDE Loss:  -4.6942 | Function Loss:  -3.0051\n",
      "Total loss:  -2.9246 | PDE Loss:  -4.6951 | Function Loss:  -3.0053\n",
      "Total loss:  -2.925 | PDE Loss:  -4.6973 | Function Loss:  -3.0053\n",
      "Total loss:  -2.9253 | PDE Loss:  -4.6998 | Function Loss:  -3.0052\n",
      "Total loss:  -2.9259 | PDE Loss:  -4.7027 | Function Loss:  -3.0054\n",
      "Total loss:  -2.9265 | PDE Loss:  -4.7061 | Function Loss:  -3.0054\n",
      "Total loss:  -2.927 | PDE Loss:  -4.708 | Function Loss:  -3.0056\n",
      "Total loss:  -2.9275 | PDE Loss:  -4.71 | Function Loss:  -3.0058\n",
      "Total loss:  -2.928 | PDE Loss:  -4.7111 | Function Loss:  -3.0062\n",
      "Total loss:  -2.9284 | PDE Loss:  -4.7125 | Function Loss:  -3.0064\n",
      "Total loss:  -2.9289 | PDE Loss:  -4.7136 | Function Loss:  -3.0068\n",
      "Total loss:  -2.9294 | PDE Loss:  -4.7149 | Function Loss:  -3.0071\n",
      "Total loss:  -2.9299 | PDE Loss:  -4.7156 | Function Loss:  -3.0075\n",
      "Total loss:  -2.9303 | PDE Loss:  -4.7167 | Function Loss:  -3.0079\n",
      "Total loss:  -2.9307 | PDE Loss:  -4.7174 | Function Loss:  -3.0081\n",
      "Total loss:  -2.9311 | PDE Loss:  -4.7186 | Function Loss:  -3.0084\n",
      "Total loss:  -2.9314 | PDE Loss:  -4.7204 | Function Loss:  -3.0085\n",
      "Total loss:  -2.9317 | PDE Loss:  -4.7216 | Function Loss:  -3.0086\n",
      "Total loss:  -2.932 | PDE Loss:  -4.7233 | Function Loss:  -3.0086\n",
      "Total loss:  -2.9323 | PDE Loss:  -4.7241 | Function Loss:  -3.0088\n",
      "Total loss:  -2.9328 | PDE Loss:  -4.7299 | Function Loss:  -3.0083\n",
      "Total loss:  -2.9333 | PDE Loss:  -4.73 | Function Loss:  -3.0089\n",
      "Total loss:  -2.9338 | PDE Loss:  -4.7314 | Function Loss:  -3.0092\n",
      "Total loss:  -2.9344 | PDE Loss:  -4.7322 | Function Loss:  -3.0097\n",
      "Total loss:  -2.935 | PDE Loss:  -4.7328 | Function Loss:  -3.0103\n",
      "Total loss:  -2.9354 | PDE Loss:  -4.7333 | Function Loss:  -3.0107\n",
      "Total loss:  -2.9357 | PDE Loss:  -4.7327 | Function Loss:  -3.0112\n",
      "Total loss:  -2.936 | PDE Loss:  -4.733 | Function Loss:  -3.0115\n",
      "Total loss:  -2.9363 | PDE Loss:  -4.7328 | Function Loss:  -3.0119\n",
      "Total loss:  -2.9367 | PDE Loss:  -4.7337 | Function Loss:  -3.0122\n",
      "Total loss:  -2.9371 | PDE Loss:  -4.7345 | Function Loss:  -3.0126\n",
      "Total loss:  -2.9375 | PDE Loss:  -4.7364 | Function Loss:  -3.0126\n",
      "Total loss:  -2.9378 | PDE Loss:  -4.7376 | Function Loss:  -3.0127\n",
      "Total loss:  -2.938 | PDE Loss:  -4.7392 | Function Loss:  -3.0127\n",
      "Total loss:  -2.9382 | PDE Loss:  -4.7413 | Function Loss:  -3.0125\n",
      "Total loss:  -2.9384 | PDE Loss:  -4.7426 | Function Loss:  -3.0125\n",
      "Total loss:  -2.9385 | PDE Loss:  -4.7435 | Function Loss:  -3.0125\n",
      "Total loss:  -2.9386 | PDE Loss:  -4.7439 | Function Loss:  -3.0126\n",
      "Total loss:  -2.9389 | PDE Loss:  -4.7443 | Function Loss:  -3.0128\n",
      "Total loss:  -2.9392 | PDE Loss:  -4.7448 | Function Loss:  -3.013\n",
      "Total loss:  -2.9394 | PDE Loss:  -4.7443 | Function Loss:  -3.0135\n",
      "Total loss:  -2.9397 | PDE Loss:  -4.7449 | Function Loss:  -3.0137\n",
      "Total loss:  -2.94 | PDE Loss:  -4.7442 | Function Loss:  -3.0142\n",
      "Total loss:  -2.9403 | PDE Loss:  -4.7441 | Function Loss:  -3.0145\n",
      "Total loss:  -2.9406 | PDE Loss:  -4.7448 | Function Loss:  -3.0148\n",
      "Total loss:  -2.941 | PDE Loss:  -4.7452 | Function Loss:  -3.0152\n",
      "Total loss:  -2.9414 | PDE Loss:  -4.7466 | Function Loss:  -3.0153\n",
      "Total loss:  -2.9417 | PDE Loss:  -4.7481 | Function Loss:  -3.0155\n",
      "Total loss:  -2.9421 | PDE Loss:  -4.7507 | Function Loss:  -3.0154\n",
      "Total loss:  -2.9426 | PDE Loss:  -4.7529 | Function Loss:  -3.0156\n",
      "Total loss:  -2.9432 | PDE Loss:  -4.7588 | Function Loss:  -3.0153\n",
      "Total loss:  -2.9438 | PDE Loss:  -4.7607 | Function Loss:  -3.0157\n",
      "Total loss:  -2.9449 | PDE Loss:  -4.7649 | Function Loss:  -3.0162\n",
      "Total loss:  -2.946 | PDE Loss:  -4.7677 | Function Loss:  -3.017\n",
      "Total loss:  -2.9469 | PDE Loss:  -4.77 | Function Loss:  -3.0177\n",
      "Total loss:  -2.9476 | PDE Loss:  -4.7702 | Function Loss:  -3.0184\n",
      "Total loss:  -2.9481 | PDE Loss:  -4.77 | Function Loss:  -3.019\n",
      "Total loss:  -2.9486 | PDE Loss:  -4.7698 | Function Loss:  -3.0197\n",
      "Total loss:  -2.9491 | PDE Loss:  -4.7686 | Function Loss:  -3.0205\n",
      "Total loss:  -2.9496 | PDE Loss:  -4.769 | Function Loss:  -3.021\n",
      "Total loss:  -2.9501 | PDE Loss:  -4.769 | Function Loss:  -3.0216\n",
      "Total loss:  -2.9506 | PDE Loss:  -4.7708 | Function Loss:  -3.0219\n",
      "Total loss:  -2.9512 | PDE Loss:  -4.774 | Function Loss:  -3.022\n",
      "Total loss:  -2.9518 | PDE Loss:  -4.7777 | Function Loss:  -3.022\n",
      "Total loss:  -2.9522 | PDE Loss:  -4.7807 | Function Loss:  -3.022\n",
      "Total loss:  -2.9527 | PDE Loss:  -4.7847 | Function Loss:  -3.0218\n",
      "Total loss:  -2.9533 | PDE Loss:  -4.7876 | Function Loss:  -3.0221\n",
      "Total loss:  -2.9539 | PDE Loss:  -4.79 | Function Loss:  -3.0224\n",
      "Total loss:  -2.9544 | PDE Loss:  -4.7933 | Function Loss:  -3.0224\n",
      "Total loss:  -2.9549 | PDE Loss:  -4.7921 | Function Loss:  -3.0232\n",
      "Total loss:  -2.9554 | PDE Loss:  -4.7913 | Function Loss:  -3.0238\n",
      "Total loss:  -2.9559 | PDE Loss:  -4.7896 | Function Loss:  -3.0248\n",
      "Total loss:  -2.9565 | PDE Loss:  -4.7886 | Function Loss:  -3.0256\n",
      "Total loss:  -2.9571 | PDE Loss:  -4.7888 | Function Loss:  -3.0263\n",
      "Total loss:  -2.9576 | PDE Loss:  -4.7879 | Function Loss:  -3.0271\n",
      "Total loss:  -2.958 | PDE Loss:  -4.7897 | Function Loss:  -3.0273\n",
      "Total loss:  -2.9584 | PDE Loss:  -4.7916 | Function Loss:  -3.0274\n",
      "Total loss:  -2.9587 | PDE Loss:  -4.7941 | Function Loss:  -3.0273\n",
      "Total loss:  -2.9589 | PDE Loss:  -4.7964 | Function Loss:  -3.0272\n",
      "Total loss:  -2.9591 | PDE Loss:  -4.7981 | Function Loss:  -3.0271\n",
      "Total loss:  -2.9592 | PDE Loss:  -4.7999 | Function Loss:  -3.0269\n",
      "Total loss:  -2.9596 | PDE Loss:  -4.7997 | Function Loss:  -3.0273\n",
      "Total loss:  -2.9598 | PDE Loss:  -4.801 | Function Loss:  -3.0274\n",
      "Total loss:  -2.9602 | PDE Loss:  -4.8024 | Function Loss:  -3.0276\n",
      "Total loss:  -2.9605 | PDE Loss:  -4.803 | Function Loss:  -3.0279\n",
      "Total loss:  -2.9608 | PDE Loss:  -4.8034 | Function Loss:  -3.0281\n",
      "Total loss:  -2.9609 | PDE Loss:  -4.8029 | Function Loss:  -3.0284\n",
      "Total loss:  -2.9612 | PDE Loss:  -4.8032 | Function Loss:  -3.0286\n",
      "Total loss:  -2.9614 | PDE Loss:  -4.803 | Function Loss:  -3.0289\n",
      "Total loss:  -2.9616 | PDE Loss:  -4.8036 | Function Loss:  -3.0291\n",
      "Total loss:  -2.9619 | PDE Loss:  -4.8045 | Function Loss:  -3.0293\n",
      "Total loss:  -2.9621 | PDE Loss:  -4.8055 | Function Loss:  -3.0293\n",
      "Total loss:  -2.9623 | PDE Loss:  -4.8071 | Function Loss:  -3.0293\n",
      "Total loss:  -2.9626 | PDE Loss:  -4.809 | Function Loss:  -3.0293\n",
      "Total loss:  -2.9628 | PDE Loss:  -4.8119 | Function Loss:  -3.0291\n",
      "Total loss:  -2.9631 | PDE Loss:  -4.8142 | Function Loss:  -3.0291\n",
      "Total loss:  -2.9634 | PDE Loss:  -4.8158 | Function Loss:  -3.0292\n",
      "Total loss:  -2.9638 | PDE Loss:  -4.818 | Function Loss:  -3.0293\n",
      "Total loss:  -2.9642 | PDE Loss:  -4.8192 | Function Loss:  -3.0296\n",
      "Total loss:  -2.9646 | PDE Loss:  -4.8203 | Function Loss:  -3.0298\n",
      "Total loss:  -2.965 | PDE Loss:  -4.8208 | Function Loss:  -3.0302\n",
      "Total loss:  -2.9654 | PDE Loss:  -4.8212 | Function Loss:  -3.0306\n",
      "Total loss:  -2.9658 | PDE Loss:  -4.8214 | Function Loss:  -3.0311\n",
      "Total loss:  -2.9663 | PDE Loss:  -4.8221 | Function Loss:  -3.0315\n",
      "Total loss:  -2.9668 | PDE Loss:  -4.823 | Function Loss:  -3.032\n",
      "Total loss:  -2.9674 | PDE Loss:  -4.8245 | Function Loss:  -3.0324\n",
      "Total loss:  -2.9678 | PDE Loss:  -4.8265 | Function Loss:  -3.0326\n",
      "Total loss:  -2.9683 | PDE Loss:  -4.8284 | Function Loss:  -3.0328\n",
      "Total loss:  -2.9688 | PDE Loss:  -4.8308 | Function Loss:  -3.0329\n",
      "Total loss:  -2.9693 | PDE Loss:  -4.8332 | Function Loss:  -3.0332\n",
      "Total loss:  -2.9699 | PDE Loss:  -4.8362 | Function Loss:  -3.0335\n",
      "Total loss:  -2.9706 | PDE Loss:  -4.838 | Function Loss:  -3.034\n",
      "Total loss:  -2.9712 | PDE Loss:  -4.841 | Function Loss:  -3.0342\n",
      "Total loss:  -2.9718 | PDE Loss:  -4.8427 | Function Loss:  -3.0346\n",
      "Total loss:  -2.9723 | PDE Loss:  -4.8448 | Function Loss:  -3.0349\n",
      "Total loss:  -2.9729 | PDE Loss:  -4.8472 | Function Loss:  -3.0351\n",
      "Total loss:  -2.9734 | PDE Loss:  -4.8489 | Function Loss:  -3.0355\n",
      "Total loss:  -2.9739 | PDE Loss:  -4.8515 | Function Loss:  -3.0357\n",
      "Total loss:  -2.9744 | PDE Loss:  -4.8529 | Function Loss:  -3.036\n",
      "Total loss:  -2.9749 | PDE Loss:  -4.8561 | Function Loss:  -3.0361\n",
      "Total loss:  -2.9754 | PDE Loss:  -4.859 | Function Loss:  -3.0362\n",
      "Total loss:  -2.9759 | PDE Loss:  -4.8619 | Function Loss:  -3.0364\n",
      "Total loss:  -2.9764 | PDE Loss:  -4.8651 | Function Loss:  -3.0365\n",
      "Total loss:  -2.9768 | PDE Loss:  -4.868 | Function Loss:  -3.0366\n",
      "Total loss:  -2.9772 | PDE Loss:  -4.8711 | Function Loss:  -3.0365\n",
      "Total loss:  -2.9775 | PDE Loss:  -4.874 | Function Loss:  -3.0365\n",
      "Total loss:  -2.9779 | PDE Loss:  -4.8778 | Function Loss:  -3.0364\n",
      "Total loss:  -2.9783 | PDE Loss:  -4.8809 | Function Loss:  -3.0364\n",
      "Total loss:  -2.9786 | PDE Loss:  -4.887 | Function Loss:  -3.0358\n",
      "Total loss:  -2.9789 | PDE Loss:  -4.8878 | Function Loss:  -3.036\n",
      "Total loss:  -2.9792 | PDE Loss:  -4.8892 | Function Loss:  -3.0362\n",
      "Total loss:  -2.9796 | PDE Loss:  -4.8902 | Function Loss:  -3.0365\n",
      "Total loss:  -2.9801 | PDE Loss:  -4.8925 | Function Loss:  -3.0367\n",
      "Total loss:  -2.9806 | PDE Loss:  -4.8929 | Function Loss:  -3.0373\n",
      "Total loss:  -2.9813 | PDE Loss:  -4.8931 | Function Loss:  -3.038\n",
      "Total loss:  -2.9819 | PDE Loss:  -4.8977 | Function Loss:  -3.0381\n",
      "Total loss:  -2.9825 | PDE Loss:  -4.8973 | Function Loss:  -3.0389\n",
      "Total loss:  -2.9833 | PDE Loss:  -4.8981 | Function Loss:  -3.0396\n",
      "Total loss:  -2.984 | PDE Loss:  -4.8954 | Function Loss:  -3.0408\n",
      "Total loss:  -2.9845 | PDE Loss:  -4.8941 | Function Loss:  -3.0415\n",
      "Total loss:  -2.9849 | PDE Loss:  -4.8925 | Function Loss:  -3.0423\n",
      "Total loss:  -2.9854 | PDE Loss:  -4.8898 | Function Loss:  -3.0432\n",
      "Total loss:  -2.9859 | PDE Loss:  -4.8887 | Function Loss:  -3.0439\n",
      "Total loss:  -2.9864 | PDE Loss:  -4.8864 | Function Loss:  -3.0448\n",
      "Total loss:  -2.9869 | PDE Loss:  -4.8855 | Function Loss:  -3.0455\n",
      "Total loss:  -2.9874 | PDE Loss:  -4.8848 | Function Loss:  -3.0462\n",
      "Total loss:  -2.988 | PDE Loss:  -4.8848 | Function Loss:  -3.0469\n",
      "Total loss:  -2.9885 | PDE Loss:  -4.8847 | Function Loss:  -3.0475\n",
      "Total loss:  -2.9888 | PDE Loss:  -4.8865 | Function Loss:  -3.0476\n",
      "Total loss:  -2.9891 | PDE Loss:  -4.8868 | Function Loss:  -3.0479\n",
      "Total loss:  -2.9896 | PDE Loss:  -4.8889 | Function Loss:  -3.0482\n",
      "Total loss:  -2.9902 | PDE Loss:  -4.8894 | Function Loss:  -3.0488\n",
      "Total loss:  -2.9908 | PDE Loss:  -4.8899 | Function Loss:  -3.0493\n",
      "Total loss:  -2.9912 | PDE Loss:  -4.8888 | Function Loss:  -3.05\n",
      "Total loss:  -2.9916 | PDE Loss:  -4.887 | Function Loss:  -3.0507\n",
      "Total loss:  -2.9921 | PDE Loss:  -4.8817 | Function Loss:  -3.0521\n",
      "Total loss:  -2.9926 | PDE Loss:  -4.879 | Function Loss:  -3.053\n",
      "Total loss:  -2.9932 | PDE Loss:  -4.8771 | Function Loss:  -3.054\n",
      "Total loss:  -2.9938 | PDE Loss:  -4.8745 | Function Loss:  -3.0551\n",
      "Total loss:  -2.9942 | PDE Loss:  -4.8736 | Function Loss:  -3.0557\n",
      "Total loss:  -2.9946 | PDE Loss:  -4.8717 | Function Loss:  -3.0564\n",
      "Total loss:  -2.9949 | PDE Loss:  -4.8715 | Function Loss:  -3.0568\n",
      "Total loss:  -2.9953 | PDE Loss:  -4.8696 | Function Loss:  -3.0575\n",
      "Total loss:  -2.9955 | PDE Loss:  -4.8689 | Function Loss:  -3.0579\n",
      "Total loss:  -2.9958 | PDE Loss:  -4.8691 | Function Loss:  -3.0582\n",
      "Total loss:  -2.996 | PDE Loss:  -4.8688 | Function Loss:  -3.0584\n",
      "Total loss:  -2.9962 | PDE Loss:  -4.8693 | Function Loss:  -3.0587\n",
      "Total loss:  -2.9965 | PDE Loss:  -4.869 | Function Loss:  -3.0591\n",
      "Total loss:  -2.9968 | PDE Loss:  -4.8684 | Function Loss:  -3.0595\n",
      "Total loss:  -2.9972 | PDE Loss:  -4.8686 | Function Loss:  -3.0599\n",
      "Total loss:  -2.9976 | PDE Loss:  -4.8652 | Function Loss:  -3.0609\n",
      "Total loss:  -2.9979 | PDE Loss:  -4.8635 | Function Loss:  -3.0615\n",
      "Total loss:  -2.9982 | PDE Loss:  -4.8638 | Function Loss:  -3.0618\n",
      "Total loss:  -2.9984 | PDE Loss:  -4.8631 | Function Loss:  -3.0622\n",
      "Total loss:  -2.9987 | PDE Loss:  -4.8626 | Function Loss:  -3.0625\n",
      "Total loss:  -2.9988 | PDE Loss:  -4.8622 | Function Loss:  -3.0628\n",
      "Total loss:  -2.999 | PDE Loss:  -4.8621 | Function Loss:  -3.0631\n",
      "Total loss:  -2.9994 | PDE Loss:  -4.8622 | Function Loss:  -3.0634\n",
      "Total loss:  -2.9998 | PDE Loss:  -4.8628 | Function Loss:  -3.0638\n",
      "Total loss:  -3.0002 | PDE Loss:  -4.8633 | Function Loss:  -3.0642\n",
      "Total loss:  -3.0006 | PDE Loss:  -4.8662 | Function Loss:  -3.0643\n",
      "Total loss:  -3.001 | PDE Loss:  -4.8676 | Function Loss:  -3.0645\n",
      "Total loss:  -3.0014 | PDE Loss:  -4.8697 | Function Loss:  -3.0646\n",
      "Total loss:  -3.0018 | PDE Loss:  -4.8723 | Function Loss:  -3.0646\n",
      "Total loss:  -3.0022 | PDE Loss:  -4.874 | Function Loss:  -3.0649\n",
      "Total loss:  -3.0026 | PDE Loss:  -4.8774 | Function Loss:  -3.0648\n",
      "Total loss:  -3.0029 | PDE Loss:  -4.8789 | Function Loss:  -3.0649\n",
      "Total loss:  -3.0032 | PDE Loss:  -4.8803 | Function Loss:  -3.065\n",
      "Total loss:  -3.0035 | PDE Loss:  -4.8826 | Function Loss:  -3.0651\n",
      "Total loss:  -3.0039 | PDE Loss:  -4.8851 | Function Loss:  -3.0651\n",
      "Total loss:  -3.0042 | PDE Loss:  -4.8855 | Function Loss:  -3.0654\n",
      "Total loss:  -3.0045 | PDE Loss:  -4.8884 | Function Loss:  -3.0653\n",
      "Total loss:  -3.0048 | PDE Loss:  -4.8907 | Function Loss:  -3.0653\n",
      "Total loss:  -3.0052 | PDE Loss:  -4.8936 | Function Loss:  -3.0654\n",
      "Total loss:  -3.0056 | PDE Loss:  -4.8965 | Function Loss:  -3.0654\n",
      "Total loss:  -3.006 | PDE Loss:  -4.8989 | Function Loss:  -3.0655\n",
      "Total loss:  -3.0064 | PDE Loss:  -4.9007 | Function Loss:  -3.0657\n",
      "Total loss:  -3.0068 | PDE Loss:  -4.9017 | Function Loss:  -3.066\n",
      "Total loss:  -3.0071 | PDE Loss:  -4.9016 | Function Loss:  -3.0664\n",
      "Total loss:  -3.0074 | PDE Loss:  -4.9011 | Function Loss:  -3.0668\n",
      "Total loss:  -3.0077 | PDE Loss:  -4.8993 | Function Loss:  -3.0673\n",
      "Total loss:  -3.008 | PDE Loss:  -4.8974 | Function Loss:  -3.0679\n",
      "Total loss:  -3.0082 | PDE Loss:  -4.8946 | Function Loss:  -3.0687\n",
      "Total loss:  -3.0085 | PDE Loss:  -4.8921 | Function Loss:  -3.0694\n",
      "Total loss:  -3.0087 | PDE Loss:  -4.8894 | Function Loss:  -3.07\n",
      "Total loss:  -3.0089 | PDE Loss:  -4.887 | Function Loss:  -3.0706\n",
      "Total loss:  -3.0092 | PDE Loss:  -4.8846 | Function Loss:  -3.0713\n",
      "Total loss:  -3.0094 | PDE Loss:  -4.8819 | Function Loss:  -3.072\n",
      "Total loss:  -3.0097 | PDE Loss:  -4.8806 | Function Loss:  -3.0724\n",
      "Total loss:  -3.0099 | PDE Loss:  -4.8772 | Function Loss:  -3.0732\n",
      "Total loss:  -3.01 | PDE Loss:  -4.8758 | Function Loss:  -3.0736\n",
      "Total loss:  -3.0102 | PDE Loss:  -4.874 | Function Loss:  -3.0741\n",
      "Total loss:  -3.0104 | PDE Loss:  -4.8729 | Function Loss:  -3.0745\n",
      "Total loss:  -3.0106 | PDE Loss:  -4.872 | Function Loss:  -3.0748\n",
      "Total loss:  -3.0108 | PDE Loss:  -4.8692 | Function Loss:  -3.0756\n",
      "Total loss:  -3.0109 | PDE Loss:  -4.8704 | Function Loss:  -3.0755\n",
      "Total loss:  -3.0112 | PDE Loss:  -4.8718 | Function Loss:  -3.0756\n",
      "Total loss:  -3.0116 | PDE Loss:  -4.8737 | Function Loss:  -3.0757\n",
      "Total loss:  -3.0119 | PDE Loss:  -4.8755 | Function Loss:  -3.0759\n",
      "Total loss:  -3.0123 | PDE Loss:  -4.8767 | Function Loss:  -3.0761\n",
      "Total loss:  -3.0127 | PDE Loss:  -4.8794 | Function Loss:  -3.0762\n",
      "Total loss:  -3.0132 | PDE Loss:  -4.8807 | Function Loss:  -3.0765\n",
      "Total loss:  -3.0136 | PDE Loss:  -4.8821 | Function Loss:  -3.0767\n",
      "Total loss:  -3.0142 | PDE Loss:  -4.8842 | Function Loss:  -3.0771\n",
      "Total loss:  -3.0147 | PDE Loss:  -4.8861 | Function Loss:  -3.0774\n",
      "Total loss:  -3.0151 | PDE Loss:  -4.8881 | Function Loss:  -3.0776\n",
      "Total loss:  -3.0155 | PDE Loss:  -4.8903 | Function Loss:  -3.0777\n",
      "Total loss:  -3.0159 | PDE Loss:  -4.8936 | Function Loss:  -3.0776\n",
      "Total loss:  -3.0163 | PDE Loss:  -4.8996 | Function Loss:  -3.0772\n",
      "Total loss:  -3.0167 | PDE Loss:  -4.9038 | Function Loss:  -3.0771\n",
      "Total loss:  -3.017 | PDE Loss:  -4.9086 | Function Loss:  -3.0766\n",
      "Total loss:  -3.0173 | PDE Loss:  -4.9112 | Function Loss:  -3.0766\n",
      "Total loss:  -3.0175 | PDE Loss:  -4.9152 | Function Loss:  -3.0763\n",
      "Total loss:  -3.0177 | PDE Loss:  -4.9189 | Function Loss:  -3.076\n",
      "Total loss:  -3.018 | PDE Loss:  -4.9227 | Function Loss:  -3.0757\n",
      "Total loss:  -3.0182 | PDE Loss:  -4.9258 | Function Loss:  -3.0756\n",
      "Total loss:  -3.0185 | PDE Loss:  -4.9286 | Function Loss:  -3.0755\n",
      "Total loss:  -3.0187 | PDE Loss:  -4.9299 | Function Loss:  -3.0755\n",
      "Total loss:  -3.0189 | PDE Loss:  -4.9312 | Function Loss:  -3.0756\n",
      "Total loss:  -3.0192 | PDE Loss:  -4.9313 | Function Loss:  -3.0759\n",
      "Total loss:  -3.0195 | PDE Loss:  -4.9327 | Function Loss:  -3.076\n",
      "Total loss:  -3.0197 | PDE Loss:  -4.933 | Function Loss:  -3.0763\n",
      "Total loss:  -3.0201 | PDE Loss:  -4.9346 | Function Loss:  -3.0765\n",
      "Total loss:  -3.0204 | PDE Loss:  -4.9361 | Function Loss:  -3.0766\n",
      "Total loss:  -3.0208 | PDE Loss:  -4.9401 | Function Loss:  -3.0765\n",
      "Total loss:  -3.0211 | PDE Loss:  -4.9432 | Function Loss:  -3.0764\n",
      "Total loss:  -3.0215 | PDE Loss:  -4.9476 | Function Loss:  -3.0763\n",
      "Total loss:  -3.0219 | PDE Loss:  -4.9537 | Function Loss:  -3.076\n",
      "Total loss:  -3.0224 | PDE Loss:  -4.9598 | Function Loss:  -3.0758\n",
      "Total loss:  -3.0229 | PDE Loss:  -4.9654 | Function Loss:  -3.0755\n",
      "Total loss:  -3.0232 | PDE Loss:  -4.9709 | Function Loss:  -3.0752\n",
      "Total loss:  -3.0235 | PDE Loss:  -4.974 | Function Loss:  -3.0752\n",
      "Total loss:  -3.0237 | PDE Loss:  -4.9756 | Function Loss:  -3.0752\n",
      "Total loss:  -3.0239 | PDE Loss:  -4.9766 | Function Loss:  -3.0753\n",
      "Total loss:  -3.0242 | PDE Loss:  -4.9782 | Function Loss:  -3.0754\n",
      "Total loss:  -3.0247 | PDE Loss:  -4.9799 | Function Loss:  -3.0757\n",
      "Total loss:  -3.0251 | PDE Loss:  -4.9812 | Function Loss:  -3.0761\n",
      "Total loss:  -3.0256 | PDE Loss:  -4.9836 | Function Loss:  -3.0763\n",
      "Total loss:  -3.0261 | PDE Loss:  -4.9851 | Function Loss:  -3.0767\n",
      "Total loss:  -3.0267 | PDE Loss:  -4.9899 | Function Loss:  -3.0768\n",
      "Total loss:  -3.0274 | PDE Loss:  -4.9925 | Function Loss:  -3.0772\n",
      "Total loss:  -3.0281 | PDE Loss:  -4.9993 | Function Loss:  -3.0772\n",
      "Total loss:  -3.0288 | PDE Loss:  -5.0039 | Function Loss:  -3.0775\n",
      "Total loss:  -3.0295 | PDE Loss:  -5.0103 | Function Loss:  -3.0775\n",
      "Total loss:  -3.03 | PDE Loss:  -5.0164 | Function Loss:  -3.0773\n",
      "Total loss:  -3.0304 | PDE Loss:  -5.0181 | Function Loss:  -3.0776\n",
      "Total loss:  -3.0307 | PDE Loss:  -5.0208 | Function Loss:  -3.0776\n",
      "Total loss:  -3.0311 | PDE Loss:  -5.0216 | Function Loss:  -3.078\n",
      "Total loss:  -3.0317 | PDE Loss:  -5.0209 | Function Loss:  -3.0787\n",
      "Total loss:  -3.0324 | PDE Loss:  -5.018 | Function Loss:  -3.0798\n",
      "Total loss:  -3.0331 | PDE Loss:  -5.0152 | Function Loss:  -3.0809\n",
      "Total loss:  -3.0338 | PDE Loss:  -5.0114 | Function Loss:  -3.0821\n",
      "Total loss:  -3.0346 | PDE Loss:  -5.0093 | Function Loss:  -3.0832\n",
      "Total loss:  -3.0352 | PDE Loss:  -5.0007 | Function Loss:  -3.085\n",
      "Total loss:  -3.0356 | PDE Loss:  -4.9999 | Function Loss:  -3.0855\n",
      "Total loss:  -3.036 | PDE Loss:  -4.9996 | Function Loss:  -3.086\n",
      "Total loss:  -3.0364 | PDE Loss:  -4.9983 | Function Loss:  -3.0866\n",
      "Total loss:  -3.0368 | PDE Loss:  -4.9998 | Function Loss:  -3.0869\n",
      "Total loss:  -3.0373 | PDE Loss:  -5.001 | Function Loss:  -3.0873\n",
      "Total loss:  -3.0377 | PDE Loss:  -5.0023 | Function Loss:  -3.0876\n",
      "Total loss:  -3.038 | PDE Loss:  -5.0036 | Function Loss:  -3.0877\n",
      "Total loss:  -3.0382 | PDE Loss:  -5.004 | Function Loss:  -3.088\n",
      "Total loss:  -3.0385 | PDE Loss:  -5.0045 | Function Loss:  -3.0882\n",
      "Total loss:  -3.0389 | PDE Loss:  -5.005 | Function Loss:  -3.0886\n",
      "Total loss:  -3.0391 | PDE Loss:  -5.0046 | Function Loss:  -3.0888\n",
      "Total loss:  -3.0394 | PDE Loss:  -5.0058 | Function Loss:  -3.089\n",
      "Total loss:  -3.0395 | PDE Loss:  -5.0071 | Function Loss:  -3.0891\n",
      "Total loss:  -3.0397 | PDE Loss:  -5.008 | Function Loss:  -3.0892\n",
      "Total loss:  -3.0399 | PDE Loss:  -5.0088 | Function Loss:  -3.0893\n",
      "Total loss:  -3.0402 | PDE Loss:  -5.0087 | Function Loss:  -3.0896\n",
      "Total loss:  -3.0403 | PDE Loss:  -5.0079 | Function Loss:  -3.0899\n",
      "Total loss:  -3.0405 | PDE Loss:  -5.0078 | Function Loss:  -3.0901\n",
      "Total loss:  -3.0407 | PDE Loss:  -5.0068 | Function Loss:  -3.0904\n",
      "Total loss:  -3.0409 | PDE Loss:  -5.0057 | Function Loss:  -3.0907\n",
      "Total loss:  -3.0411 | PDE Loss:  -5.0056 | Function Loss:  -3.091\n",
      "Total loss:  -3.0414 | PDE Loss:  -5.0046 | Function Loss:  -3.0914\n",
      "Total loss:  -3.0416 | PDE Loss:  -5.0052 | Function Loss:  -3.0916\n",
      "Total loss:  -3.0421 | PDE Loss:  -5.0074 | Function Loss:  -3.0919\n",
      "Total loss:  -3.0425 | PDE Loss:  -5.0102 | Function Loss:  -3.092\n",
      "Total loss:  -3.0429 | PDE Loss:  -5.014 | Function Loss:  -3.092\n",
      "Total loss:  -3.0432 | PDE Loss:  -5.0149 | Function Loss:  -3.0922\n",
      "Total loss:  -3.0434 | PDE Loss:  -5.0162 | Function Loss:  -3.0923\n",
      "Total loss:  -3.0437 | PDE Loss:  -5.0181 | Function Loss:  -3.0924\n",
      "Total loss:  -3.044 | PDE Loss:  -5.0195 | Function Loss:  -3.0925\n",
      "Total loss:  -3.0442 | PDE Loss:  -5.0203 | Function Loss:  -3.0927\n",
      "Total loss:  -3.0446 | PDE Loss:  -5.0209 | Function Loss:  -3.093\n",
      "Total loss:  -3.0449 | PDE Loss:  -5.0205 | Function Loss:  -3.0935\n",
      "Total loss:  -3.0454 | PDE Loss:  -5.0197 | Function Loss:  -3.0941\n",
      "Total loss:  -3.0459 | PDE Loss:  -5.0167 | Function Loss:  -3.095\n",
      "Total loss:  -3.0463 | PDE Loss:  -5.0161 | Function Loss:  -3.0955\n",
      "Total loss:  -3.0467 | PDE Loss:  -5.0153 | Function Loss:  -3.0961\n",
      "Total loss:  -3.0471 | PDE Loss:  -5.0142 | Function Loss:  -3.0967\n",
      "Total loss:  -3.0474 | PDE Loss:  -5.0143 | Function Loss:  -3.097\n",
      "Total loss:  -3.0477 | PDE Loss:  -5.0143 | Function Loss:  -3.0973\n",
      "Total loss:  -3.0479 | PDE Loss:  -5.0141 | Function Loss:  -3.0976\n",
      "Total loss:  -3.0481 | PDE Loss:  -5.0139 | Function Loss:  -3.0978\n",
      "Total loss:  -3.0483 | PDE Loss:  -5.0144 | Function Loss:  -3.098\n",
      "Total loss:  -3.0484 | PDE Loss:  -5.0125 | Function Loss:  -3.0984\n",
      "Total loss:  -3.0486 | PDE Loss:  -5.0136 | Function Loss:  -3.0984\n",
      "Total loss:  -3.0487 | PDE Loss:  -5.0147 | Function Loss:  -3.0984\n",
      "Total loss:  -3.0489 | PDE Loss:  -5.0157 | Function Loss:  -3.0985\n",
      "Total loss:  -3.049 | PDE Loss:  -5.0161 | Function Loss:  -3.0986\n",
      "Total loss:  -3.0492 | PDE Loss:  -5.0164 | Function Loss:  -3.0987\n",
      "Total loss:  -3.0493 | PDE Loss:  -5.016 | Function Loss:  -3.099\n",
      "Total loss:  -3.0495 | PDE Loss:  -5.0154 | Function Loss:  -3.0993\n",
      "Total loss:  -3.0498 | PDE Loss:  -5.0138 | Function Loss:  -3.0997\n",
      "Total loss:  -3.05 | PDE Loss:  -5.0129 | Function Loss:  -3.1001\n",
      "Total loss:  -3.0502 | PDE Loss:  -5.0119 | Function Loss:  -3.1004\n",
      "Total loss:  -3.0505 | PDE Loss:  -5.0111 | Function Loss:  -3.1009\n",
      "Total loss:  -3.0508 | PDE Loss:  -5.0102 | Function Loss:  -3.1013\n",
      "Total loss:  -3.0511 | PDE Loss:  -5.0099 | Function Loss:  -3.1017\n",
      "Total loss:  -3.0514 | PDE Loss:  -5.0091 | Function Loss:  -3.1021\n",
      "Total loss:  -3.0517 | PDE Loss:  -5.0092 | Function Loss:  -3.1024\n",
      "Total loss:  -3.052 | PDE Loss:  -5.0087 | Function Loss:  -3.1028\n",
      "Total loss:  -3.0523 | PDE Loss:  -5.01 | Function Loss:  -3.103\n",
      "Total loss:  -3.0526 | PDE Loss:  -5.0113 | Function Loss:  -3.1032\n",
      "Total loss:  -3.0528 | PDE Loss:  -5.0122 | Function Loss:  -3.1033\n",
      "Total loss:  -3.0529 | PDE Loss:  -5.0134 | Function Loss:  -3.1033\n",
      "Total loss:  -3.0531 | PDE Loss:  -5.0145 | Function Loss:  -3.1033\n",
      "Total loss:  -3.0533 | PDE Loss:  -5.0161 | Function Loss:  -3.1034\n",
      "Total loss:  -3.0537 | PDE Loss:  -5.0144 | Function Loss:  -3.104\n",
      "Total loss:  -3.054 | PDE Loss:  -5.0191 | Function Loss:  -3.1039\n",
      "Total loss:  -3.0543 | PDE Loss:  -5.0186 | Function Loss:  -3.1042\n",
      "Total loss:  -3.0547 | PDE Loss:  -5.0182 | Function Loss:  -3.1047\n",
      "Total loss:  -3.0552 | PDE Loss:  -5.0173 | Function Loss:  -3.1053\n",
      "Total loss:  -3.0555 | PDE Loss:  -5.0173 | Function Loss:  -3.1058\n",
      "Total loss:  -3.0559 | PDE Loss:  -5.0161 | Function Loss:  -3.1063\n",
      "Total loss:  -3.0562 | PDE Loss:  -5.0165 | Function Loss:  -3.1066\n",
      "Total loss:  -3.0565 | PDE Loss:  -5.0162 | Function Loss:  -3.107\n",
      "Total loss:  -3.0568 | PDE Loss:  -5.0154 | Function Loss:  -3.1074\n",
      "Total loss:  -3.0569 | PDE Loss:  -5.016 | Function Loss:  -3.1075\n",
      "Total loss:  -3.0571 | PDE Loss:  -5.0169 | Function Loss:  -3.1076\n",
      "Total loss:  -3.0573 | PDE Loss:  -5.0181 | Function Loss:  -3.1077\n",
      "Total loss:  -3.0576 | PDE Loss:  -5.0202 | Function Loss:  -3.1077\n",
      "Total loss:  -3.0578 | PDE Loss:  -5.0222 | Function Loss:  -3.1077\n",
      "Total loss:  -3.0581 | PDE Loss:  -5.0258 | Function Loss:  -3.1076\n",
      "Total loss:  -3.0584 | PDE Loss:  -5.0283 | Function Loss:  -3.1077\n",
      "Total loss:  -3.0588 | PDE Loss:  -5.0325 | Function Loss:  -3.1076\n",
      "Total loss:  -3.0591 | PDE Loss:  -5.0356 | Function Loss:  -3.1075\n",
      "Total loss:  -3.0595 | PDE Loss:  -5.0379 | Function Loss:  -3.1077\n",
      "Total loss:  -3.06 | PDE Loss:  -5.0401 | Function Loss:  -3.108\n",
      "Total loss:  -3.0606 | PDE Loss:  -5.0405 | Function Loss:  -3.1087\n",
      "Total loss:  -3.0613 | PDE Loss:  -5.0405 | Function Loss:  -3.1094\n",
      "Total loss:  -3.0619 | PDE Loss:  -5.0383 | Function Loss:  -3.1104\n",
      "Total loss:  -3.0625 | PDE Loss:  -5.0337 | Function Loss:  -3.1116\n",
      "Total loss:  -3.0629 | PDE Loss:  -5.0323 | Function Loss:  -3.1122\n",
      "Total loss:  -3.0633 | PDE Loss:  -5.0307 | Function Loss:  -3.1128\n",
      "Total loss:  -3.0636 | PDE Loss:  -5.0297 | Function Loss:  -3.1133\n",
      "Total loss:  -3.0639 | PDE Loss:  -5.0291 | Function Loss:  -3.1137\n",
      "Total loss:  -3.0641 | PDE Loss:  -5.0289 | Function Loss:  -3.1139\n",
      "Total loss:  -3.0642 | PDE Loss:  -5.0292 | Function Loss:  -3.1141\n",
      "Total loss:  -3.0645 | PDE Loss:  -5.0293 | Function Loss:  -3.1144\n",
      "Total loss:  -3.0648 | PDE Loss:  -5.0291 | Function Loss:  -3.1148\n",
      "Total loss:  -3.0651 | PDE Loss:  -5.0278 | Function Loss:  -3.1152\n",
      "Total loss:  -3.0653 | PDE Loss:  -5.0291 | Function Loss:  -3.1153\n",
      "Total loss:  -3.0655 | PDE Loss:  -5.0281 | Function Loss:  -3.1156\n",
      "Total loss:  -3.0656 | PDE Loss:  -5.0274 | Function Loss:  -3.1159\n",
      "Total loss:  -3.0658 | PDE Loss:  -5.0268 | Function Loss:  -3.1161\n",
      "Total loss:  -3.0659 | PDE Loss:  -5.0257 | Function Loss:  -3.1164\n",
      "Total loss:  -3.0661 | PDE Loss:  -5.0255 | Function Loss:  -3.1166\n",
      "Total loss:  -3.0663 | PDE Loss:  -5.0242 | Function Loss:  -3.117\n",
      "Total loss:  -3.0666 | PDE Loss:  -5.0253 | Function Loss:  -3.1172\n",
      "Total loss:  -3.0668 | PDE Loss:  -5.0255 | Function Loss:  -3.1174\n",
      "Total loss:  -3.0671 | PDE Loss:  -5.0269 | Function Loss:  -3.1176\n",
      "Total loss:  -3.0673 | PDE Loss:  -5.029 | Function Loss:  -3.1175\n",
      "Total loss:  -3.0675 | PDE Loss:  -5.0311 | Function Loss:  -3.1175\n",
      "Total loss:  -3.0678 | PDE Loss:  -5.0337 | Function Loss:  -3.1175\n",
      "Total loss:  -3.068 | PDE Loss:  -5.0358 | Function Loss:  -3.1175\n",
      "Total loss:  -3.0684 | PDE Loss:  -5.0386 | Function Loss:  -3.1176\n",
      "Total loss:  -3.0688 | PDE Loss:  -5.0407 | Function Loss:  -3.1178\n",
      "Total loss:  -3.0691 | PDE Loss:  -5.0419 | Function Loss:  -3.118\n",
      "Total loss:  -3.0696 | PDE Loss:  -5.044 | Function Loss:  -3.1183\n",
      "Total loss:  -3.0701 | PDE Loss:  -5.0444 | Function Loss:  -3.1188\n",
      "Total loss:  -3.0706 | PDE Loss:  -5.0451 | Function Loss:  -3.1192\n",
      "Total loss:  -3.071 | PDE Loss:  -5.044 | Function Loss:  -3.1198\n",
      "Total loss:  -3.0714 | PDE Loss:  -5.0454 | Function Loss:  -3.1201\n",
      "Total loss:  -3.0718 | PDE Loss:  -5.0444 | Function Loss:  -3.1207\n",
      "Total loss:  -3.0723 | PDE Loss:  -5.0476 | Function Loss:  -3.1208\n",
      "Total loss:  -3.0727 | PDE Loss:  -5.0483 | Function Loss:  -3.1212\n",
      "Total loss:  -3.073 | PDE Loss:  -5.05 | Function Loss:  -3.1214\n",
      "Total loss:  -3.0732 | PDE Loss:  -5.0517 | Function Loss:  -3.1214\n",
      "Total loss:  -3.0734 | PDE Loss:  -5.0531 | Function Loss:  -3.1215\n",
      "Total loss:  -3.0735 | PDE Loss:  -5.0547 | Function Loss:  -3.1214\n",
      "Total loss:  -3.0738 | PDE Loss:  -5.0562 | Function Loss:  -3.1216\n",
      "Total loss:  -3.0742 | PDE Loss:  -5.0577 | Function Loss:  -3.1218\n",
      "Total loss:  -3.0744 | PDE Loss:  -5.0579 | Function Loss:  -3.1221\n",
      "Total loss:  -3.0746 | PDE Loss:  -5.058 | Function Loss:  -3.1223\n",
      "Total loss:  -3.0749 | PDE Loss:  -5.0586 | Function Loss:  -3.1225\n",
      "Total loss:  -3.0752 | PDE Loss:  -5.0593 | Function Loss:  -3.1228\n",
      "Total loss:  -3.0756 | PDE Loss:  -5.058 | Function Loss:  -3.1234\n",
      "Total loss:  -3.076 | PDE Loss:  -5.0607 | Function Loss:  -3.1234\n",
      "Total loss:  -3.0763 | PDE Loss:  -5.0575 | Function Loss:  -3.1242\n",
      "Total loss:  -3.0767 | PDE Loss:  -5.0594 | Function Loss:  -3.1244\n",
      "Total loss:  -3.0771 | PDE Loss:  -5.0617 | Function Loss:  -3.1246\n",
      "Total loss:  -3.0775 | PDE Loss:  -5.0613 | Function Loss:  -3.1251\n",
      "Total loss:  -3.0778 | PDE Loss:  -5.0648 | Function Loss:  -3.1251\n",
      "Total loss:  -3.078 | PDE Loss:  -5.0644 | Function Loss:  -3.1253\n",
      "Total loss:  -3.0783 | PDE Loss:  -5.0647 | Function Loss:  -3.1256\n",
      "Total loss:  -3.0785 | PDE Loss:  -5.0643 | Function Loss:  -3.1259\n",
      "Total loss:  -3.0789 | PDE Loss:  -5.0629 | Function Loss:  -3.1264\n",
      "Total loss:  -3.0792 | PDE Loss:  -5.0611 | Function Loss:  -3.127\n",
      "Total loss:  -3.0796 | PDE Loss:  -5.0597 | Function Loss:  -3.1276\n",
      "Total loss:  -3.0799 | PDE Loss:  -5.0567 | Function Loss:  -3.1283\n",
      "Total loss:  -3.0803 | PDE Loss:  -5.054 | Function Loss:  -3.129\n",
      "Total loss:  -3.0805 | PDE Loss:  -5.0538 | Function Loss:  -3.1293\n",
      "Total loss:  -3.0807 | PDE Loss:  -5.052 | Function Loss:  -3.1297\n",
      "Total loss:  -3.0809 | PDE Loss:  -5.052 | Function Loss:  -3.13\n",
      "Total loss:  -3.0811 | PDE Loss:  -5.0519 | Function Loss:  -3.1302\n",
      "Total loss:  -3.0814 | PDE Loss:  -5.0518 | Function Loss:  -3.1306\n",
      "Total loss:  -3.0818 | PDE Loss:  -5.0519 | Function Loss:  -3.131\n",
      "Total loss:  -3.0822 | PDE Loss:  -5.051 | Function Loss:  -3.1316\n",
      "Total loss:  -3.0827 | PDE Loss:  -5.0519 | Function Loss:  -3.132\n",
      "Total loss:  -3.0831 | PDE Loss:  -5.0514 | Function Loss:  -3.1326\n",
      "Total loss:  -3.0835 | PDE Loss:  -5.053 | Function Loss:  -3.1328\n",
      "Total loss:  -3.0838 | PDE Loss:  -5.0552 | Function Loss:  -3.1329\n",
      "Total loss:  -3.0841 | PDE Loss:  -5.0573 | Function Loss:  -3.133\n",
      "Total loss:  -3.0844 | PDE Loss:  -5.06 | Function Loss:  -3.1329\n",
      "Total loss:  -3.0846 | PDE Loss:  -5.0627 | Function Loss:  -3.1329\n",
      "Total loss:  -3.0849 | PDE Loss:  -5.0655 | Function Loss:  -3.1328\n",
      "Total loss:  -3.0852 | PDE Loss:  -5.0691 | Function Loss:  -3.1328\n",
      "Total loss:  -3.0855 | PDE Loss:  -5.0728 | Function Loss:  -3.1327\n",
      "Total loss:  -3.0859 | PDE Loss:  -5.0766 | Function Loss:  -3.1327\n",
      "Total loss:  -3.0863 | PDE Loss:  -5.0804 | Function Loss:  -3.1327\n",
      "Total loss:  -3.0867 | PDE Loss:  -5.0821 | Function Loss:  -3.133\n",
      "Total loss:  -3.087 | PDE Loss:  -5.0835 | Function Loss:  -3.1332\n",
      "Total loss:  -3.0873 | PDE Loss:  -5.0829 | Function Loss:  -3.1335\n",
      "Total loss:  -3.0876 | PDE Loss:  -5.0815 | Function Loss:  -3.134\n",
      "Total loss:  -3.0878 | PDE Loss:  -5.0798 | Function Loss:  -3.1345\n",
      "Total loss:  -3.0881 | PDE Loss:  -5.0767 | Function Loss:  -3.1352\n",
      "Total loss:  -3.0887 | PDE Loss:  -5.0733 | Function Loss:  -3.1362\n",
      "Total loss:  -3.0894 | PDE Loss:  -5.0645 | Function Loss:  -3.138\n",
      "Total loss:  -3.0899 | PDE Loss:  -5.0598 | Function Loss:  -3.1392\n",
      "Total loss:  -3.0903 | PDE Loss:  -5.0576 | Function Loss:  -3.1398\n",
      "Total loss:  -3.0907 | PDE Loss:  -5.0543 | Function Loss:  -3.1407\n",
      "Total loss:  -3.0911 | PDE Loss:  -5.0532 | Function Loss:  -3.1413\n",
      "Total loss:  -3.0914 | PDE Loss:  -5.0521 | Function Loss:  -3.1417\n",
      "Total loss:  -3.0916 | PDE Loss:  -5.0528 | Function Loss:  -3.1419\n",
      "Total loss:  -3.092 | PDE Loss:  -5.0528 | Function Loss:  -3.1423\n",
      "Total loss:  -3.0924 | PDE Loss:  -5.0529 | Function Loss:  -3.1427\n",
      "Total loss:  -3.0927 | PDE Loss:  -5.0539 | Function Loss:  -3.143\n",
      "Total loss:  -3.093 | PDE Loss:  -5.0543 | Function Loss:  -3.1433\n",
      "Total loss:  -3.0934 | PDE Loss:  -5.0549 | Function Loss:  -3.1436\n",
      "Total loss:  -3.0939 | PDE Loss:  -5.056 | Function Loss:  -3.144\n",
      "Total loss:  -3.0943 | PDE Loss:  -5.0576 | Function Loss:  -3.1444\n",
      "Total loss:  -3.0948 | PDE Loss:  -5.059 | Function Loss:  -3.1447\n",
      "Total loss:  -3.0952 | PDE Loss:  -5.0619 | Function Loss:  -3.1448\n",
      "Total loss:  -3.0955 | PDE Loss:  -5.0629 | Function Loss:  -3.1451\n",
      "Total loss:  -3.0958 | PDE Loss:  -5.0644 | Function Loss:  -3.1452\n",
      "Total loss:  -3.096 | PDE Loss:  -5.0645 | Function Loss:  -3.1455\n",
      "Total loss:  -3.0963 | PDE Loss:  -5.0648 | Function Loss:  -3.1457\n",
      "Total loss:  -3.0965 | PDE Loss:  -5.0653 | Function Loss:  -3.1459\n",
      "Total loss:  -3.0968 | PDE Loss:  -5.0652 | Function Loss:  -3.1462\n",
      "Total loss:  -3.0971 | PDE Loss:  -5.0658 | Function Loss:  -3.1465\n",
      "Total loss:  -3.0975 | PDE Loss:  -5.0657 | Function Loss:  -3.147\n",
      "Total loss:  -3.0979 | PDE Loss:  -5.0659 | Function Loss:  -3.1473\n",
      "Total loss:  -3.0981 | PDE Loss:  -5.0658 | Function Loss:  -3.1476\n",
      "Total loss:  -3.0983 | PDE Loss:  -5.0648 | Function Loss:  -3.148\n",
      "Total loss:  -3.0986 | PDE Loss:  -5.0659 | Function Loss:  -3.1481\n",
      "Total loss:  -3.0987 | PDE Loss:  -5.0658 | Function Loss:  -3.1483\n",
      "Total loss:  -3.099 | PDE Loss:  -5.0671 | Function Loss:  -3.1484\n",
      "Total loss:  -3.0992 | PDE Loss:  -5.0689 | Function Loss:  -3.1485\n",
      "Total loss:  -3.0997 | PDE Loss:  -5.0709 | Function Loss:  -3.1487\n",
      "Total loss:  -3.1001 | PDE Loss:  -5.0751 | Function Loss:  -3.1488\n",
      "Total loss:  -3.1005 | PDE Loss:  -5.0764 | Function Loss:  -3.1491\n",
      "Total loss:  -3.1008 | PDE Loss:  -5.0785 | Function Loss:  -3.1491\n",
      "Total loss:  -3.1011 | PDE Loss:  -5.0813 | Function Loss:  -3.1491\n",
      "Total loss:  -3.1014 | PDE Loss:  -5.0837 | Function Loss:  -3.1492\n",
      "Total loss:  -3.1017 | PDE Loss:  -5.0854 | Function Loss:  -3.1493\n",
      "Total loss:  -3.1019 | PDE Loss:  -5.0855 | Function Loss:  -3.1495\n",
      "Total loss:  -3.1021 | PDE Loss:  -5.0853 | Function Loss:  -3.1498\n",
      "Total loss:  -3.1024 | PDE Loss:  -5.0839 | Function Loss:  -3.1503\n",
      "Total loss:  -3.1027 | PDE Loss:  -5.0819 | Function Loss:  -3.1508\n",
      "Total loss:  -3.103 | PDE Loss:  -5.0794 | Function Loss:  -3.1514\n",
      "Total loss:  -3.1032 | PDE Loss:  -5.0768 | Function Loss:  -3.152\n",
      "Total loss:  -3.1034 | PDE Loss:  -5.0746 | Function Loss:  -3.1525\n",
      "Total loss:  -3.1037 | PDE Loss:  -5.0732 | Function Loss:  -3.153\n",
      "Total loss:  -3.104 | PDE Loss:  -5.0678 | Function Loss:  -3.154\n",
      "Total loss:  -3.1042 | PDE Loss:  -5.0683 | Function Loss:  -3.1541\n",
      "Total loss:  -3.1044 | PDE Loss:  -5.0687 | Function Loss:  -3.1543\n",
      "Total loss:  -3.1047 | PDE Loss:  -5.0701 | Function Loss:  -3.1544\n",
      "Total loss:  -3.1049 | PDE Loss:  -5.0711 | Function Loss:  -3.1546\n",
      "Total loss:  -3.1016 | PDE Loss:  -5.0537 | Function Loss:  -3.153\n",
      "Total loss:  -3.105 | PDE Loss:  -5.0712 | Function Loss:  -3.1547\n",
      "Total loss:  -3.1053 | PDE Loss:  -5.0723 | Function Loss:  -3.1548\n",
      "Total loss:  -3.1055 | PDE Loss:  -5.0725 | Function Loss:  -3.1551\n",
      "Total loss:  -3.1059 | PDE Loss:  -5.0722 | Function Loss:  -3.1555\n",
      "Total loss:  -3.1061 | PDE Loss:  -5.0721 | Function Loss:  -3.1558\n",
      "Total loss:  -3.1064 | PDE Loss:  -5.0705 | Function Loss:  -3.1564\n",
      "Total loss:  -3.1068 | PDE Loss:  -5.0699 | Function Loss:  -3.1569\n",
      "Total loss:  -3.1072 | PDE Loss:  -5.0676 | Function Loss:  -3.1576\n",
      "Total loss:  -3.1075 | PDE Loss:  -5.0681 | Function Loss:  -3.1579\n",
      "Total loss:  -3.108 | PDE Loss:  -5.0695 | Function Loss:  -3.1582\n",
      "Total loss:  -3.1086 | PDE Loss:  -5.0695 | Function Loss:  -3.1589\n",
      "Total loss:  -3.1093 | PDE Loss:  -5.0737 | Function Loss:  -3.1592\n",
      "Total loss:  -3.1099 | PDE Loss:  -5.0744 | Function Loss:  -3.1598\n",
      "Total loss:  -3.1106 | PDE Loss:  -5.0743 | Function Loss:  -3.1606\n",
      "Total loss:  -3.1112 | PDE Loss:  -5.0756 | Function Loss:  -3.1611\n",
      "Total loss:  -3.1116 | PDE Loss:  -5.0753 | Function Loss:  -3.1616\n",
      "Total loss:  -3.1118 | PDE Loss:  -5.0758 | Function Loss:  -3.1617\n",
      "Total loss:  -3.1119 | PDE Loss:  -5.0759 | Function Loss:  -3.1619\n",
      "Total loss:  -3.1121 | PDE Loss:  -5.0752 | Function Loss:  -3.1622\n",
      "Total loss:  -3.1124 | PDE Loss:  -5.0764 | Function Loss:  -3.1623\n",
      "Total loss:  -3.1126 | PDE Loss:  -5.0759 | Function Loss:  -3.1626\n",
      "Total loss:  -3.1128 | PDE Loss:  -5.0766 | Function Loss:  -3.1627\n",
      "Total loss:  -3.113 | PDE Loss:  -5.0775 | Function Loss:  -3.1629\n",
      "Total loss:  -3.1132 | PDE Loss:  -5.0791 | Function Loss:  -3.163\n",
      "Total loss:  -3.1135 | PDE Loss:  -5.0807 | Function Loss:  -3.163\n",
      "Total loss:  -3.1137 | PDE Loss:  -5.0819 | Function Loss:  -3.1631\n",
      "Total loss:  -3.1139 | PDE Loss:  -5.0827 | Function Loss:  -3.1633\n",
      "Total loss:  -3.1141 | PDE Loss:  -5.0838 | Function Loss:  -3.1634\n",
      "Total loss:  -3.1143 | PDE Loss:  -5.0835 | Function Loss:  -3.1637\n",
      "Total loss:  -3.1145 | PDE Loss:  -5.0841 | Function Loss:  -3.1638\n",
      "Total loss:  -3.1147 | PDE Loss:  -5.0839 | Function Loss:  -3.164\n",
      "Total loss:  -3.1148 | PDE Loss:  -5.0833 | Function Loss:  -3.1642\n",
      "Total loss:  -3.115 | PDE Loss:  -5.083 | Function Loss:  -3.1645\n",
      "Total loss:  -3.1152 | PDE Loss:  -5.0828 | Function Loss:  -3.1648\n",
      "Total loss:  -3.1155 | PDE Loss:  -5.0832 | Function Loss:  -3.165\n",
      "Total loss:  -3.1157 | PDE Loss:  -5.0836 | Function Loss:  -3.1652\n",
      "Total loss:  -3.1159 | PDE Loss:  -5.0848 | Function Loss:  -3.1652\n",
      "Total loss:  -3.116 | PDE Loss:  -5.086 | Function Loss:  -3.1652\n",
      "Total loss:  -3.1161 | PDE Loss:  -5.0875 | Function Loss:  -3.1652\n",
      "Total loss:  -3.1162 | PDE Loss:  -5.0894 | Function Loss:  -3.1651\n",
      "Total loss:  -3.1164 | PDE Loss:  -5.0921 | Function Loss:  -3.1649\n",
      "Total loss:  -3.1166 | PDE Loss:  -5.0942 | Function Loss:  -3.1649\n",
      "Total loss:  -3.1168 | PDE Loss:  -5.0964 | Function Loss:  -3.1648\n",
      "Total loss:  -3.117 | PDE Loss:  -5.0981 | Function Loss:  -3.1649\n",
      "Total loss:  -3.1172 | PDE Loss:  -5.0995 | Function Loss:  -3.165\n",
      "Total loss:  -3.1175 | PDE Loss:  -5.0997 | Function Loss:  -3.1653\n",
      "Total loss:  -3.1177 | PDE Loss:  -5.0991 | Function Loss:  -3.1656\n",
      "Total loss:  -3.118 | PDE Loss:  -5.0984 | Function Loss:  -3.166\n",
      "Total loss:  -3.1184 | PDE Loss:  -5.0953 | Function Loss:  -3.1668\n",
      "Total loss:  -3.1187 | PDE Loss:  -5.0954 | Function Loss:  -3.1671\n",
      "Total loss:  -3.1189 | PDE Loss:  -5.096 | Function Loss:  -3.1673\n",
      "Total loss:  -3.1192 | PDE Loss:  -5.0962 | Function Loss:  -3.1676\n",
      "Total loss:  -3.1195 | PDE Loss:  -5.0986 | Function Loss:  -3.1677\n",
      "Total loss:  -3.1198 | PDE Loss:  -5.1 | Function Loss:  -3.1678\n",
      "Total loss:  -3.12 | PDE Loss:  -5.1027 | Function Loss:  -3.1677\n",
      "Total loss:  -3.1202 | PDE Loss:  -5.1054 | Function Loss:  -3.1676\n",
      "Total loss:  -3.1206 | PDE Loss:  -5.1075 | Function Loss:  -3.1678\n",
      "Total loss:  -3.121 | PDE Loss:  -5.1113 | Function Loss:  -3.1679\n",
      "Total loss:  -3.1216 | PDE Loss:  -5.1134 | Function Loss:  -3.1682\n",
      "Total loss:  -3.1221 | PDE Loss:  -5.1159 | Function Loss:  -3.1686\n",
      "Total loss:  -3.1227 | PDE Loss:  -5.1165 | Function Loss:  -3.1692\n",
      "Total loss:  -3.1233 | PDE Loss:  -5.119 | Function Loss:  -3.1696\n",
      "Total loss:  -3.1238 | PDE Loss:  -5.1188 | Function Loss:  -3.1701\n",
      "Total loss:  -3.1243 | PDE Loss:  -5.1202 | Function Loss:  -3.1705\n",
      "Total loss:  -3.1248 | PDE Loss:  -5.1175 | Function Loss:  -3.1713\n",
      "Total loss:  -3.1251 | PDE Loss:  -5.1175 | Function Loss:  -3.1717\n",
      "Total loss:  -3.1253 | PDE Loss:  -5.1172 | Function Loss:  -3.172\n",
      "Total loss:  -3.1256 | PDE Loss:  -5.1169 | Function Loss:  -3.1724\n",
      "Total loss:  -3.126 | PDE Loss:  -5.1166 | Function Loss:  -3.1728\n",
      "Total loss:  -3.1262 | PDE Loss:  -5.1173 | Function Loss:  -3.173\n",
      "Total loss:  -3.1264 | PDE Loss:  -5.1178 | Function Loss:  -3.1732\n",
      "Total loss:  -3.1269 | PDE Loss:  -5.118 | Function Loss:  -3.1736\n",
      "Total loss:  -3.1275 | PDE Loss:  -5.1174 | Function Loss:  -3.1744\n",
      "Total loss:  -3.1281 | PDE Loss:  -5.1172 | Function Loss:  -3.1751\n",
      "Total loss:  -3.1286 | PDE Loss:  -5.114 | Function Loss:  -3.1761\n",
      "Total loss:  -3.129 | PDE Loss:  -5.1136 | Function Loss:  -3.1765\n",
      "Total loss:  -3.1294 | PDE Loss:  -5.1119 | Function Loss:  -3.1772\n",
      "Total loss:  -3.1299 | PDE Loss:  -5.1106 | Function Loss:  -3.1778\n",
      "Total loss:  -3.1302 | PDE Loss:  -5.1078 | Function Loss:  -3.1785\n",
      "Total loss:  -3.1305 | PDE Loss:  -5.106 | Function Loss:  -3.1791\n",
      "Total loss:  -3.1307 | PDE Loss:  -5.1034 | Function Loss:  -3.1797\n",
      "Total loss:  -3.131 | PDE Loss:  -5.1019 | Function Loss:  -3.1801\n",
      "Total loss:  -3.1314 | PDE Loss:  -5.0999 | Function Loss:  -3.1808\n",
      "Total loss:  -3.132 | PDE Loss:  -5.0989 | Function Loss:  -3.1816\n",
      "Total loss:  -3.1329 | PDE Loss:  -5.0956 | Function Loss:  -3.183\n",
      "Total loss:  -3.1336 | PDE Loss:  -5.0954 | Function Loss:  -3.1838\n",
      "Total loss:  -3.1341 | PDE Loss:  -5.0917 | Function Loss:  -3.1849\n",
      "Total loss:  -3.1345 | PDE Loss:  -5.0906 | Function Loss:  -3.1854\n",
      "Total loss:  -3.1348 | PDE Loss:  -5.0882 | Function Loss:  -3.186\n",
      "Total loss:  -3.135 | PDE Loss:  -5.0877 | Function Loss:  -3.1863\n",
      "Total loss:  -3.1352 | PDE Loss:  -5.0861 | Function Loss:  -3.1868\n",
      "Total loss:  -3.1355 | PDE Loss:  -5.0861 | Function Loss:  -3.1871\n",
      "Total loss:  -3.1358 | PDE Loss:  -5.0857 | Function Loss:  -3.1875\n",
      "Total loss:  -3.1361 | PDE Loss:  -5.086 | Function Loss:  -3.1877\n",
      "Total loss:  -3.1363 | PDE Loss:  -5.0858 | Function Loss:  -3.188\n",
      "Total loss:  -3.1365 | PDE Loss:  -5.087 | Function Loss:  -3.1881\n",
      "Total loss:  -3.1367 | PDE Loss:  -5.0868 | Function Loss:  -3.1883\n",
      "Total loss:  -3.1368 | PDE Loss:  -5.0881 | Function Loss:  -3.1884\n",
      "Total loss:  -3.137 | PDE Loss:  -5.0869 | Function Loss:  -3.1887\n",
      "Total loss:  -3.1372 | PDE Loss:  -5.0873 | Function Loss:  -3.1889\n",
      "Total loss:  -3.1373 | PDE Loss:  -5.0861 | Function Loss:  -3.1892\n",
      "Total loss:  -3.1375 | PDE Loss:  -5.0858 | Function Loss:  -3.1895\n",
      "Total loss:  -3.1377 | PDE Loss:  -5.0842 | Function Loss:  -3.1899\n",
      "Total loss:  -3.1379 | PDE Loss:  -5.0827 | Function Loss:  -3.1902\n",
      "Total loss:  -3.1381 | PDE Loss:  -5.0829 | Function Loss:  -3.1904\n",
      "Total loss:  -3.1383 | PDE Loss:  -5.0808 | Function Loss:  -3.1909\n",
      "Total loss:  -3.1385 | PDE Loss:  -5.0788 | Function Loss:  -3.1914\n",
      "Total loss:  -3.1388 | PDE Loss:  -5.0763 | Function Loss:  -3.1921\n",
      "Total loss:  -3.1391 | PDE Loss:  -5.0728 | Function Loss:  -3.1929\n",
      "Total loss:  -3.1394 | PDE Loss:  -5.0705 | Function Loss:  -3.1936\n",
      "Total loss:  -3.1398 | PDE Loss:  -5.0675 | Function Loss:  -3.1944\n",
      "Total loss:  -3.1402 | PDE Loss:  -5.0657 | Function Loss:  -3.195\n",
      "Total loss:  -3.1406 | PDE Loss:  -5.0645 | Function Loss:  -3.1957\n",
      "Total loss:  -3.1409 | PDE Loss:  -5.0644 | Function Loss:  -3.1961\n",
      "Total loss:  -3.1412 | PDE Loss:  -5.0644 | Function Loss:  -3.1964\n",
      "Total loss:  -3.1414 | PDE Loss:  -5.065 | Function Loss:  -3.1966\n",
      "Total loss:  -3.1417 | PDE Loss:  -5.0656 | Function Loss:  -3.1968\n",
      "Total loss:  -3.142 | PDE Loss:  -5.0673 | Function Loss:  -3.1969\n",
      "Total loss:  -3.1423 | PDE Loss:  -5.0668 | Function Loss:  -3.1973\n",
      "Total loss:  -3.1426 | PDE Loss:  -5.0697 | Function Loss:  -3.1973\n",
      "Total loss:  -3.1429 | PDE Loss:  -5.072 | Function Loss:  -3.1973\n",
      "Total loss:  -3.1432 | PDE Loss:  -5.0743 | Function Loss:  -3.1974\n",
      "Total loss:  -3.1435 | PDE Loss:  -5.0761 | Function Loss:  -3.1974\n",
      "Total loss:  -3.1437 | PDE Loss:  -5.0768 | Function Loss:  -3.1975\n",
      "Total loss:  -3.1438 | PDE Loss:  -5.0772 | Function Loss:  -3.1977\n",
      "Total loss:  -3.144 | PDE Loss:  -5.0779 | Function Loss:  -3.1978\n",
      "Total loss:  -3.1442 | PDE Loss:  -5.0778 | Function Loss:  -3.198\n",
      "Total loss:  -3.1444 | PDE Loss:  -5.0798 | Function Loss:  -3.198\n",
      "Total loss:  -3.1445 | PDE Loss:  -5.0798 | Function Loss:  -3.1981\n",
      "Total loss:  -3.1448 | PDE Loss:  -5.0801 | Function Loss:  -3.1984\n",
      "Total loss:  -3.145 | PDE Loss:  -5.0811 | Function Loss:  -3.1985\n",
      "Total loss:  -3.1452 | PDE Loss:  -5.0818 | Function Loss:  -3.1986\n",
      "Total loss:  -3.1454 | PDE Loss:  -5.0823 | Function Loss:  -3.1987\n",
      "Total loss:  -3.1455 | PDE Loss:  -5.0828 | Function Loss:  -3.1988\n",
      "Total loss:  -3.1457 | PDE Loss:  -5.0829 | Function Loss:  -3.199\n",
      "Total loss:  -3.1458 | PDE Loss:  -5.083 | Function Loss:  -3.1992\n",
      "Total loss:  -3.146 | PDE Loss:  -5.0824 | Function Loss:  -3.1994\n",
      "Total loss:  -3.1462 | PDE Loss:  -5.0818 | Function Loss:  -3.1997\n",
      "Total loss:  -3.1463 | PDE Loss:  -5.0808 | Function Loss:  -3.2\n",
      "Total loss:  -3.1465 | PDE Loss:  -5.0801 | Function Loss:  -3.2003\n",
      "Total loss:  -3.1467 | PDE Loss:  -5.0793 | Function Loss:  -3.2007\n",
      "Total loss:  -3.147 | PDE Loss:  -5.0778 | Function Loss:  -3.2012\n",
      "Total loss:  -3.1473 | PDE Loss:  -5.0774 | Function Loss:  -3.2016\n",
      "Total loss:  -3.1476 | PDE Loss:  -5.0765 | Function Loss:  -3.202\n",
      "Total loss:  -3.148 | PDE Loss:  -5.0766 | Function Loss:  -3.2024\n",
      "Total loss:  -3.1484 | PDE Loss:  -5.0765 | Function Loss:  -3.2029\n",
      "Total loss:  -3.1487 | PDE Loss:  -5.0765 | Function Loss:  -3.2033\n",
      "Total loss:  -3.1491 | PDE Loss:  -5.0767 | Function Loss:  -3.2037\n",
      "Total loss:  -3.1496 | PDE Loss:  -5.0759 | Function Loss:  -3.2043\n",
      "Total loss:  -3.1501 | PDE Loss:  -5.0758 | Function Loss:  -3.2049\n",
      "Total loss:  -3.1506 | PDE Loss:  -5.0743 | Function Loss:  -3.2057\n",
      "Total loss:  -3.151 | PDE Loss:  -5.0734 | Function Loss:  -3.2063\n",
      "Total loss:  -3.1514 | PDE Loss:  -5.0724 | Function Loss:  -3.2069\n",
      "Total loss:  -3.1516 | PDE Loss:  -5.0711 | Function Loss:  -3.2073\n",
      "Total loss:  -3.1518 | PDE Loss:  -5.0705 | Function Loss:  -3.2076\n",
      "Total loss:  -3.152 | PDE Loss:  -5.0693 | Function Loss:  -3.208\n",
      "Total loss:  -3.1522 | PDE Loss:  -5.0688 | Function Loss:  -3.2084\n",
      "Total loss:  -3.1525 | PDE Loss:  -5.0676 | Function Loss:  -3.2089\n",
      "Total loss:  -3.1528 | PDE Loss:  -5.0673 | Function Loss:  -3.2092\n",
      "Total loss:  -3.1532 | PDE Loss:  -5.0669 | Function Loss:  -3.2097\n",
      "Total loss:  -3.1534 | PDE Loss:  -5.0674 | Function Loss:  -3.2099\n",
      "Total loss:  -3.1536 | PDE Loss:  -5.0666 | Function Loss:  -3.2102\n",
      "Total loss:  -3.1538 | PDE Loss:  -5.0667 | Function Loss:  -3.2104\n",
      "Total loss:  -3.1539 | PDE Loss:  -5.0652 | Function Loss:  -3.2108\n",
      "Total loss:  -3.1541 | PDE Loss:  -5.0642 | Function Loss:  -3.211\n",
      "Total loss:  -3.1541 | PDE Loss:  -5.0613 | Function Loss:  -3.2115\n",
      "Total loss:  -3.1542 | PDE Loss:  -5.0611 | Function Loss:  -3.2117\n",
      "Total loss:  -3.1543 | PDE Loss:  -5.0608 | Function Loss:  -3.2118\n",
      "Total loss:  -3.1545 | PDE Loss:  -5.0605 | Function Loss:  -3.212\n",
      "Total loss:  -3.1546 | PDE Loss:  -5.0601 | Function Loss:  -3.2122\n",
      "Total loss:  -3.1547 | PDE Loss:  -5.0599 | Function Loss:  -3.2123\n",
      "Total loss:  -3.1547 | PDE Loss:  -5.0592 | Function Loss:  -3.2125\n",
      "Total loss:  -3.1549 | PDE Loss:  -5.0593 | Function Loss:  -3.2127\n",
      "Total loss:  -3.155 | PDE Loss:  -5.0595 | Function Loss:  -3.2128\n",
      "Total loss:  -3.1551 | PDE Loss:  -5.0599 | Function Loss:  -3.2129\n",
      "Total loss:  -3.1553 | PDE Loss:  -5.0602 | Function Loss:  -3.213\n",
      "Total loss:  -3.1554 | PDE Loss:  -5.0603 | Function Loss:  -3.2131\n",
      "Total loss:  -3.1556 | PDE Loss:  -5.0602 | Function Loss:  -3.2134\n",
      "Total loss:  -3.1558 | PDE Loss:  -5.0601 | Function Loss:  -3.2136\n",
      "Total loss:  -3.1559 | PDE Loss:  -5.0599 | Function Loss:  -3.2138\n",
      "Total loss:  -3.1561 | PDE Loss:  -5.0599 | Function Loss:  -3.214\n",
      "Total loss:  -3.1562 | PDE Loss:  -5.0601 | Function Loss:  -3.2141\n",
      "Total loss:  -3.1564 | PDE Loss:  -5.0603 | Function Loss:  -3.2142\n",
      "Total loss:  -3.1565 | PDE Loss:  -5.0604 | Function Loss:  -3.2143\n",
      "Total loss:  -3.1566 | PDE Loss:  -5.0606 | Function Loss:  -3.2144\n",
      "Total loss:  -3.1567 | PDE Loss:  -5.061 | Function Loss:  -3.2145\n",
      "Total loss:  -3.1568 | PDE Loss:  -5.0617 | Function Loss:  -3.2146\n",
      "Total loss:  -3.157 | PDE Loss:  -5.0627 | Function Loss:  -3.2146\n",
      "Total loss:  -3.1572 | PDE Loss:  -5.0644 | Function Loss:  -3.2147\n",
      "Total loss:  -3.1575 | PDE Loss:  -5.0655 | Function Loss:  -3.2148\n",
      "Total loss:  -3.1576 | PDE Loss:  -5.0669 | Function Loss:  -3.2147\n",
      "Total loss:  -3.1578 | PDE Loss:  -5.0677 | Function Loss:  -3.2148\n",
      "Total loss:  -3.1579 | PDE Loss:  -5.0683 | Function Loss:  -3.2149\n",
      "Total loss:  -3.158 | PDE Loss:  -5.0678 | Function Loss:  -3.2151\n",
      "Total loss:  -3.1581 | PDE Loss:  -5.0673 | Function Loss:  -3.2153\n",
      "Total loss:  -3.1583 | PDE Loss:  -5.0658 | Function Loss:  -3.2157\n",
      "Total loss:  -3.1585 | PDE Loss:  -5.0645 | Function Loss:  -3.2161\n",
      "Total loss:  -3.1588 | PDE Loss:  -5.0627 | Function Loss:  -3.2167\n",
      "Total loss:  -3.159 | PDE Loss:  -5.0604 | Function Loss:  -3.2173\n",
      "Total loss:  -3.1592 | PDE Loss:  -5.0598 | Function Loss:  -3.2175\n",
      "Total loss:  -3.1593 | PDE Loss:  -5.0595 | Function Loss:  -3.2177\n",
      "Total loss:  -3.1595 | PDE Loss:  -5.0596 | Function Loss:  -3.2179\n",
      "Total loss:  -3.1596 | PDE Loss:  -5.0602 | Function Loss:  -3.218\n",
      "Total loss:  -3.1597 | PDE Loss:  -5.0607 | Function Loss:  -3.218\n",
      "Total loss:  -3.1599 | PDE Loss:  -5.0618 | Function Loss:  -3.218\n",
      "Total loss:  -3.16 | PDE Loss:  -5.0622 | Function Loss:  -3.2181\n",
      "Total loss:  -3.1602 | PDE Loss:  -5.0624 | Function Loss:  -3.2183\n",
      "Total loss:  -3.1603 | PDE Loss:  -5.0626 | Function Loss:  -3.2184\n",
      "Total loss:  -3.1604 | PDE Loss:  -5.062 | Function Loss:  -3.2186\n",
      "Total loss:  -3.1605 | PDE Loss:  -5.0613 | Function Loss:  -3.2189\n",
      "Total loss:  -3.1607 | PDE Loss:  -5.0598 | Function Loss:  -3.2193\n",
      "Total loss:  -3.1609 | PDE Loss:  -5.0575 | Function Loss:  -3.2198\n",
      "Total loss:  -3.1612 | PDE Loss:  -5.0547 | Function Loss:  -3.2205\n",
      "Total loss:  -3.1614 | PDE Loss:  -5.0512 | Function Loss:  -3.2213\n",
      "Total loss:  -3.1616 | PDE Loss:  -5.0483 | Function Loss:  -3.222\n",
      "Total loss:  -3.1618 | PDE Loss:  -5.0457 | Function Loss:  -3.2226\n",
      "Total loss:  -3.162 | PDE Loss:  -5.0443 | Function Loss:  -3.2231\n",
      "Total loss:  -3.1622 | PDE Loss:  -5.0423 | Function Loss:  -3.2236\n",
      "Total loss:  -3.1624 | PDE Loss:  -5.0427 | Function Loss:  -3.2237\n",
      "Total loss:  -3.1625 | PDE Loss:  -5.0431 | Function Loss:  -3.2238\n",
      "Total loss:  -3.1626 | PDE Loss:  -5.0449 | Function Loss:  -3.2237\n",
      "Total loss:  -3.1627 | PDE Loss:  -5.0454 | Function Loss:  -3.2237\n",
      "Total loss:  -3.1628 | PDE Loss:  -5.0471 | Function Loss:  -3.2235\n",
      "Total loss:  -3.1629 | PDE Loss:  -5.0492 | Function Loss:  -3.2233\n",
      "Total loss:  -3.163 | PDE Loss:  -5.0508 | Function Loss:  -3.2232\n",
      "Total loss:  -3.163 | PDE Loss:  -5.0523 | Function Loss:  -3.2231\n",
      "Total loss:  -3.1631 | PDE Loss:  -5.0533 | Function Loss:  -3.223\n",
      "Total loss:  -3.1632 | PDE Loss:  -5.0544 | Function Loss:  -3.223\n",
      "Total loss:  -3.1633 | PDE Loss:  -5.0548 | Function Loss:  -3.223\n",
      "Total loss:  -3.1634 | PDE Loss:  -5.0557 | Function Loss:  -3.223\n",
      "Total loss:  -3.1636 | PDE Loss:  -5.0553 | Function Loss:  -3.2232\n",
      "Total loss:  -3.1637 | PDE Loss:  -5.055 | Function Loss:  -3.2234\n",
      "Total loss:  -3.1638 | PDE Loss:  -5.0542 | Function Loss:  -3.2236\n",
      "Total loss:  -3.164 | PDE Loss:  -5.0536 | Function Loss:  -3.2239\n",
      "Total loss:  -3.1641 | PDE Loss:  -5.0524 | Function Loss:  -3.2243\n",
      "Total loss:  -3.1643 | PDE Loss:  -5.0517 | Function Loss:  -3.2246\n",
      "Total loss:  -3.1645 | PDE Loss:  -5.0507 | Function Loss:  -3.2249\n",
      "Total loss:  -3.1646 | PDE Loss:  -5.0503 | Function Loss:  -3.2251\n",
      "Total loss:  -3.1648 | PDE Loss:  -5.0506 | Function Loss:  -3.2253\n",
      "Total loss:  -3.165 | PDE Loss:  -5.0518 | Function Loss:  -3.2254\n",
      "Total loss:  -3.1652 | PDE Loss:  -5.0535 | Function Loss:  -3.2254\n",
      "Total loss:  -3.1654 | PDE Loss:  -5.0548 | Function Loss:  -3.2254\n",
      "Total loss:  -3.1656 | PDE Loss:  -5.0564 | Function Loss:  -3.2254\n",
      "Total loss:  -3.1657 | PDE Loss:  -5.0571 | Function Loss:  -3.2254\n",
      "Total loss:  -3.1659 | PDE Loss:  -5.058 | Function Loss:  -3.2255\n",
      "Total loss:  -3.166 | PDE Loss:  -5.0568 | Function Loss:  -3.2258\n",
      "Total loss:  -3.1661 | PDE Loss:  -5.0571 | Function Loss:  -3.2259\n",
      "Total loss:  -3.1663 | PDE Loss:  -5.0573 | Function Loss:  -3.226\n",
      "Total loss:  -3.1666 | PDE Loss:  -5.0565 | Function Loss:  -3.2265\n",
      "Total loss:  -3.1669 | PDE Loss:  -5.0557 | Function Loss:  -3.227\n",
      "Total loss:  -3.1673 | PDE Loss:  -5.0533 | Function Loss:  -3.2278\n",
      "Total loss:  -3.1677 | PDE Loss:  -5.0511 | Function Loss:  -3.2286\n",
      "Total loss:  -3.1681 | PDE Loss:  -5.0479 | Function Loss:  -3.2295\n",
      "Total loss:  -3.1684 | PDE Loss:  -5.0457 | Function Loss:  -3.2302\n",
      "Total loss:  -3.1686 | PDE Loss:  -5.0439 | Function Loss:  -3.2307\n",
      "Total loss:  -3.1688 | PDE Loss:  -5.0427 | Function Loss:  -3.2312\n",
      "Total loss:  -3.1691 | PDE Loss:  -5.0423 | Function Loss:  -3.2315\n",
      "Total loss:  -3.1694 | PDE Loss:  -5.0401 | Function Loss:  -3.2322\n",
      "Total loss:  -3.1697 | PDE Loss:  -5.0397 | Function Loss:  -3.2326\n",
      "Total loss:  -3.1701 | PDE Loss:  -5.0394 | Function Loss:  -3.2331\n",
      "Total loss:  -3.1706 | PDE Loss:  -5.0368 | Function Loss:  -3.2342\n",
      "Total loss:  -3.1711 | PDE Loss:  -5.0374 | Function Loss:  -3.2346\n",
      "Total loss:  -3.1716 | PDE Loss:  -5.0374 | Function Loss:  -3.2352\n",
      "Total loss:  -3.1721 | PDE Loss:  -5.0375 | Function Loss:  -3.2357\n",
      "Total loss:  -3.1725 | PDE Loss:  -5.037 | Function Loss:  -3.2363\n",
      "Total loss:  -3.1729 | PDE Loss:  -5.0372 | Function Loss:  -3.2367\n",
      "Total loss:  -3.1732 | PDE Loss:  -5.0373 | Function Loss:  -3.2371\n",
      "Total loss:  -3.1735 | PDE Loss:  -5.0384 | Function Loss:  -3.2373\n",
      "Total loss:  -3.1738 | PDE Loss:  -5.0401 | Function Loss:  -3.2373\n",
      "Total loss:  -3.174 | PDE Loss:  -5.0421 | Function Loss:  -3.2372\n",
      "Total loss:  -3.1742 | PDE Loss:  -5.0441 | Function Loss:  -3.2371\n",
      "Total loss:  -3.1743 | PDE Loss:  -5.0473 | Function Loss:  -3.2368\n",
      "Total loss:  -3.1744 | PDE Loss:  -5.0488 | Function Loss:  -3.2367\n",
      "Total loss:  -3.1745 | PDE Loss:  -5.0512 | Function Loss:  -3.2364\n",
      "Total loss:  -3.1746 | PDE Loss:  -5.0539 | Function Loss:  -3.2361\n",
      "Total loss:  -3.1748 | PDE Loss:  -5.0582 | Function Loss:  -3.2357\n",
      "Total loss:  -3.175 | PDE Loss:  -5.0604 | Function Loss:  -3.2356\n",
      "Total loss:  -3.1751 | PDE Loss:  -5.0634 | Function Loss:  -3.2353\n",
      "Total loss:  -3.1753 | PDE Loss:  -5.0658 | Function Loss:  -3.2351\n",
      "Total loss:  -3.1755 | PDE Loss:  -5.0679 | Function Loss:  -3.235\n",
      "Total loss:  -3.1757 | PDE Loss:  -5.0691 | Function Loss:  -3.235\n",
      "Total loss:  -3.1759 | PDE Loss:  -5.0696 | Function Loss:  -3.2352\n",
      "Total loss:  -3.1761 | PDE Loss:  -5.0701 | Function Loss:  -3.2354\n",
      "Total loss:  -3.1764 | PDE Loss:  -5.0719 | Function Loss:  -3.2355\n",
      "Total loss:  -3.1766 | PDE Loss:  -5.0717 | Function Loss:  -3.2358\n",
      "Total loss:  -3.1769 | PDE Loss:  -5.0723 | Function Loss:  -3.236\n",
      "Total loss:  -3.1773 | PDE Loss:  -5.073 | Function Loss:  -3.2363\n",
      "Total loss:  -3.1777 | PDE Loss:  -5.0751 | Function Loss:  -3.2365\n",
      "Total loss:  -3.178 | PDE Loss:  -5.0769 | Function Loss:  -3.2366\n",
      "Total loss:  -3.1784 | PDE Loss:  -5.0798 | Function Loss:  -3.2366\n",
      "Total loss:  -3.1787 | PDE Loss:  -5.0824 | Function Loss:  -3.2366\n",
      "Total loss:  -3.179 | PDE Loss:  -5.0849 | Function Loss:  -3.2366\n",
      "Total loss:  -3.1794 | PDE Loss:  -5.0879 | Function Loss:  -3.2366\n",
      "Total loss:  -3.1797 | PDE Loss:  -5.0898 | Function Loss:  -3.2367\n",
      "Total loss:  -3.18 | PDE Loss:  -5.0918 | Function Loss:  -3.2368\n",
      "Total loss:  -3.1802 | PDE Loss:  -5.0928 | Function Loss:  -3.2369\n",
      "Total loss:  -3.1804 | PDE Loss:  -5.0936 | Function Loss:  -3.237\n",
      "Total loss:  -3.1807 | PDE Loss:  -5.0949 | Function Loss:  -3.2371\n",
      "Total loss:  -3.181 | PDE Loss:  -5.0955 | Function Loss:  -3.2374\n",
      "Total loss:  -3.1812 | PDE Loss:  -5.0968 | Function Loss:  -3.2375\n",
      "Total loss:  -3.1814 | PDE Loss:  -5.0973 | Function Loss:  -3.2376\n",
      "Total loss:  -3.1817 | PDE Loss:  -5.0974 | Function Loss:  -3.2379\n",
      "Total loss:  -3.1819 | PDE Loss:  -5.098 | Function Loss:  -3.238\n",
      "Total loss:  -3.1821 | PDE Loss:  -5.0952 | Function Loss:  -3.2387\n",
      "Total loss:  -3.1822 | PDE Loss:  -5.0953 | Function Loss:  -3.2388\n",
      "Total loss:  -3.1824 | PDE Loss:  -5.0948 | Function Loss:  -3.2391\n",
      "Total loss:  -3.1826 | PDE Loss:  -5.0943 | Function Loss:  -3.2393\n",
      "Total loss:  -3.1828 | PDE Loss:  -5.0932 | Function Loss:  -3.2397\n",
      "Total loss:  -3.183 | PDE Loss:  -5.0926 | Function Loss:  -3.24\n",
      "Total loss:  -3.1832 | PDE Loss:  -5.0908 | Function Loss:  -3.2406\n",
      "Total loss:  -3.1835 | PDE Loss:  -5.0907 | Function Loss:  -3.2409\n",
      "Total loss:  -3.1837 | PDE Loss:  -5.089 | Function Loss:  -3.2414\n",
      "Total loss:  -3.1839 | PDE Loss:  -5.0898 | Function Loss:  -3.2415\n",
      "Total loss:  -3.184 | PDE Loss:  -5.0896 | Function Loss:  -3.2417\n",
      "Total loss:  -3.1842 | PDE Loss:  -5.0902 | Function Loss:  -3.2418\n",
      "Total loss:  -3.1843 | PDE Loss:  -5.0914 | Function Loss:  -3.2417\n",
      "Total loss:  -3.1845 | PDE Loss:  -5.0919 | Function Loss:  -3.2418\n",
      "Total loss:  -3.1846 | PDE Loss:  -5.0925 | Function Loss:  -3.242\n",
      "Total loss:  -3.1848 | PDE Loss:  -5.0925 | Function Loss:  -3.2422\n",
      "Total loss:  -3.185 | PDE Loss:  -5.0893 | Function Loss:  -3.2428\n",
      "Total loss:  -3.1853 | PDE Loss:  -5.0905 | Function Loss:  -3.2429\n",
      "Total loss:  -3.1855 | PDE Loss:  -5.0908 | Function Loss:  -3.2431\n",
      "Total loss:  -3.1857 | PDE Loss:  -5.0908 | Function Loss:  -3.2434\n",
      "Total loss:  -3.1859 | PDE Loss:  -5.0906 | Function Loss:  -3.2437\n",
      "Total loss:  -3.1861 | PDE Loss:  -5.0903 | Function Loss:  -3.2439\n",
      "Total loss:  -3.1863 | PDE Loss:  -5.0907 | Function Loss:  -3.2441\n",
      "Total loss:  -3.1864 | PDE Loss:  -5.0912 | Function Loss:  -3.2442\n",
      "Total loss:  -3.1866 | PDE Loss:  -5.0925 | Function Loss:  -3.2442\n",
      "Total loss:  -3.1868 | PDE Loss:  -5.0942 | Function Loss:  -3.2441\n",
      "Total loss:  -3.1869 | PDE Loss:  -5.0961 | Function Loss:  -3.244\n",
      "Total loss:  -3.1871 | PDE Loss:  -5.0993 | Function Loss:  -3.2438\n",
      "Total loss:  -3.1872 | PDE Loss:  -5.1004 | Function Loss:  -3.2437\n",
      "Total loss:  -3.1873 | PDE Loss:  -5.1017 | Function Loss:  -3.2437\n",
      "Total loss:  -3.1874 | PDE Loss:  -5.1036 | Function Loss:  -3.2435\n",
      "Total loss:  -3.1876 | PDE Loss:  -5.1038 | Function Loss:  -3.2437\n",
      "Total loss:  -3.1877 | PDE Loss:  -5.1035 | Function Loss:  -3.2439\n",
      "Total loss:  -3.1878 | PDE Loss:  -5.103 | Function Loss:  -3.2441\n",
      "Total loss:  -3.188 | PDE Loss:  -5.1024 | Function Loss:  -3.2444\n",
      "Total loss:  -3.1882 | PDE Loss:  -5.1021 | Function Loss:  -3.2446\n",
      "Total loss:  -3.1884 | PDE Loss:  -5.102 | Function Loss:  -3.2449\n",
      "Total loss:  -3.1886 | PDE Loss:  -5.103 | Function Loss:  -3.245\n",
      "Total loss:  -3.1885 | PDE Loss:  -5.0979 | Function Loss:  -3.2456\n",
      "Total loss:  -3.1887 | PDE Loss:  -5.1015 | Function Loss:  -3.2453\n",
      "Total loss:  -3.1889 | PDE Loss:  -5.1035 | Function Loss:  -3.2453\n",
      "Total loss:  -3.1891 | PDE Loss:  -5.1056 | Function Loss:  -3.2452\n",
      "Total loss:  -3.1893 | PDE Loss:  -5.1073 | Function Loss:  -3.2452\n",
      "Total loss:  -3.1894 | PDE Loss:  -5.1086 | Function Loss:  -3.2451\n",
      "Total loss:  -3.1895 | PDE Loss:  -5.1095 | Function Loss:  -3.2451\n",
      "Total loss:  -3.1896 | PDE Loss:  -5.1097 | Function Loss:  -3.2452\n",
      "Total loss:  -3.1897 | PDE Loss:  -5.11 | Function Loss:  -3.2453\n",
      "Total loss:  -3.1899 | PDE Loss:  -5.1096 | Function Loss:  -3.2456\n",
      "Total loss:  -3.19 | PDE Loss:  -5.1086 | Function Loss:  -3.2459\n",
      "Total loss:  -3.1898 | PDE Loss:  -5.1045 | Function Loss:  -3.2462\n",
      "Total loss:  -3.1901 | PDE Loss:  -5.1076 | Function Loss:  -3.2461\n",
      "Total loss:  -3.1903 | PDE Loss:  -5.1064 | Function Loss:  -3.2464\n",
      "Total loss:  -3.1905 | PDE Loss:  -5.1048 | Function Loss:  -3.2469\n",
      "Total loss:  -3.1907 | PDE Loss:  -5.1032 | Function Loss:  -3.2474\n",
      "Total loss:  -3.191 | PDE Loss:  -5.1009 | Function Loss:  -3.248\n",
      "Total loss:  -3.1913 | PDE Loss:  -5.0983 | Function Loss:  -3.2487\n",
      "Total loss:  -3.1915 | PDE Loss:  -5.0965 | Function Loss:  -3.2492\n",
      "Total loss:  -3.1917 | PDE Loss:  -5.0949 | Function Loss:  -3.2497\n",
      "Total loss:  -3.1919 | PDE Loss:  -5.0951 | Function Loss:  -3.2499\n",
      "Total loss:  -3.192 | PDE Loss:  -5.0956 | Function Loss:  -3.2499\n",
      "Total loss:  -3.1922 | PDE Loss:  -5.0967 | Function Loss:  -3.2499\n",
      "Total loss:  -3.1924 | PDE Loss:  -5.098 | Function Loss:  -3.25\n",
      "Total loss:  -3.1927 | PDE Loss:  -5.0992 | Function Loss:  -3.2502\n",
      "Total loss:  -3.1931 | PDE Loss:  -5.1007 | Function Loss:  -3.2505\n",
      "Total loss:  -3.1936 | PDE Loss:  -5.1003 | Function Loss:  -3.2511\n",
      "Total loss:  -3.194 | PDE Loss:  -5.0995 | Function Loss:  -3.2517\n",
      "Total loss:  -3.1945 | PDE Loss:  -5.0978 | Function Loss:  -3.2524\n",
      "Total loss:  -3.195 | PDE Loss:  -5.0951 | Function Loss:  -3.2534\n",
      "Total loss:  -3.1954 | PDE Loss:  -5.0919 | Function Loss:  -3.2543\n",
      "Total loss:  -3.1957 | PDE Loss:  -5.0898 | Function Loss:  -3.255\n",
      "Total loss:  -3.196 | PDE Loss:  -5.0877 | Function Loss:  -3.2557\n",
      "Total loss:  -3.1965 | PDE Loss:  -5.0871 | Function Loss:  -3.2563\n",
      "Total loss:  -3.1972 | PDE Loss:  -5.0868 | Function Loss:  -3.2572\n",
      "Total loss:  -3.1979 | PDE Loss:  -5.0908 | Function Loss:  -3.2573\n",
      "Total loss:  -3.1983 | PDE Loss:  -5.0928 | Function Loss:  -3.2576\n",
      "Total loss:  -3.1987 | PDE Loss:  -5.0948 | Function Loss:  -3.2577\n",
      "Total loss:  -3.199 | PDE Loss:  -5.0967 | Function Loss:  -3.2578\n",
      "Total loss:  -3.1992 | PDE Loss:  -5.0978 | Function Loss:  -3.2578\n",
      "Total loss:  -3.1993 | PDE Loss:  -5.0981 | Function Loss:  -3.2579\n",
      "Total loss:  -3.1994 | PDE Loss:  -5.0979 | Function Loss:  -3.2581\n",
      "Total loss:  -3.1996 | PDE Loss:  -5.0972 | Function Loss:  -3.2584\n",
      "Total loss:  -3.1997 | PDE Loss:  -5.0958 | Function Loss:  -3.2587\n",
      "Total loss:  -3.1999 | PDE Loss:  -5.0936 | Function Loss:  -3.2593\n",
      "Total loss:  -3.2001 | PDE Loss:  -5.0911 | Function Loss:  -3.2599\n",
      "Total loss:  -3.2003 | PDE Loss:  -5.0894 | Function Loss:  -3.2604\n",
      "Total loss:  -3.2006 | PDE Loss:  -5.0873 | Function Loss:  -3.261\n",
      "Total loss:  -3.2008 | PDE Loss:  -5.086 | Function Loss:  -3.2615\n",
      "Total loss:  -3.2011 | PDE Loss:  -5.0844 | Function Loss:  -3.262\n",
      "Total loss:  -3.2013 | PDE Loss:  -5.0824 | Function Loss:  -3.2625\n",
      "Total loss:  -3.2014 | PDE Loss:  -5.0835 | Function Loss:  -3.2625\n",
      "Total loss:  -3.2016 | PDE Loss:  -5.0848 | Function Loss:  -3.2626\n",
      "Total loss:  -3.2018 | PDE Loss:  -5.0867 | Function Loss:  -3.2625\n",
      "Total loss:  -3.2019 | PDE Loss:  -5.0879 | Function Loss:  -3.2624\n",
      "Total loss:  -3.2021 | PDE Loss:  -5.0892 | Function Loss:  -3.2624\n",
      "Total loss:  -3.2022 | PDE Loss:  -5.0901 | Function Loss:  -3.2624\n",
      "Total loss:  -3.2024 | PDE Loss:  -5.0914 | Function Loss:  -3.2624\n",
      "Total loss:  -3.2025 | PDE Loss:  -5.0919 | Function Loss:  -3.2625\n",
      "Total loss:  -3.2027 | PDE Loss:  -5.0928 | Function Loss:  -3.2626\n",
      "Total loss:  -3.2029 | PDE Loss:  -5.0933 | Function Loss:  -3.2627\n",
      "Total loss:  -3.2031 | PDE Loss:  -5.094 | Function Loss:  -3.2629\n",
      "Total loss:  -3.2033 | PDE Loss:  -5.0943 | Function Loss:  -3.263\n",
      "Total loss:  -3.2035 | PDE Loss:  -5.0942 | Function Loss:  -3.2633\n",
      "Total loss:  -3.2037 | PDE Loss:  -5.0942 | Function Loss:  -3.2635\n",
      "Total loss:  -3.2038 | PDE Loss:  -5.0945 | Function Loss:  -3.2636\n",
      "Total loss:  -3.2039 | PDE Loss:  -5.0946 | Function Loss:  -3.2637\n",
      "Total loss:  -3.204 | PDE Loss:  -5.0951 | Function Loss:  -3.2637\n",
      "Total loss:  -3.2042 | PDE Loss:  -5.0952 | Function Loss:  -3.2639\n",
      "Total loss:  -3.2043 | PDE Loss:  -5.0957 | Function Loss:  -3.264\n",
      "Total loss:  -3.2045 | PDE Loss:  -5.0962 | Function Loss:  -3.2642\n",
      "Total loss:  -3.2047 | PDE Loss:  -5.0966 | Function Loss:  -3.2643\n",
      "Total loss:  -3.2049 | PDE Loss:  -5.0971 | Function Loss:  -3.2645\n",
      "Total loss:  -3.2051 | PDE Loss:  -5.0977 | Function Loss:  -3.2647\n",
      "Total loss:  -3.2054 | PDE Loss:  -5.0985 | Function Loss:  -3.2648\n",
      "Total loss:  -3.2055 | PDE Loss:  -5.0982 | Function Loss:  -3.265\n",
      "Total loss:  -3.2057 | PDE Loss:  -5.0989 | Function Loss:  -3.2651\n",
      "Total loss:  -3.2059 | PDE Loss:  -5.0984 | Function Loss:  -3.2654\n",
      "Total loss:  -3.2061 | PDE Loss:  -5.0986 | Function Loss:  -3.2656\n",
      "Total loss:  -3.2063 | PDE Loss:  -5.098 | Function Loss:  -3.2659\n",
      "Total loss:  -3.2064 | PDE Loss:  -5.0977 | Function Loss:  -3.2661\n",
      "Total loss:  -3.2066 | PDE Loss:  -5.0972 | Function Loss:  -3.2664\n",
      "Total loss:  -3.2068 | PDE Loss:  -5.0975 | Function Loss:  -3.2666\n",
      "Total loss:  -3.2071 | PDE Loss:  -5.0928 | Function Loss:  -3.2676\n",
      "Total loss:  -3.2072 | PDE Loss:  -5.098 | Function Loss:  -3.267\n",
      "Total loss:  -3.2074 | PDE Loss:  -5.098 | Function Loss:  -3.2672\n",
      "Total loss:  -3.2076 | PDE Loss:  -5.0979 | Function Loss:  -3.2674\n",
      "Total loss:  -3.2077 | PDE Loss:  -5.0979 | Function Loss:  -3.2676\n",
      "Total loss:  -3.2079 | PDE Loss:  -5.0979 | Function Loss:  -3.2678\n",
      "Total loss:  -3.2081 | PDE Loss:  -5.0982 | Function Loss:  -3.2679\n",
      "Total loss:  -3.2083 | PDE Loss:  -5.0982 | Function Loss:  -3.2682\n",
      "Total loss:  -3.2085 | PDE Loss:  -5.0979 | Function Loss:  -3.2685\n",
      "Total loss:  -3.2087 | PDE Loss:  -5.098 | Function Loss:  -3.2687\n",
      "Total loss:  -3.2089 | PDE Loss:  -5.0978 | Function Loss:  -3.2689\n",
      "Total loss:  -3.209 | PDE Loss:  -5.0975 | Function Loss:  -3.2691\n",
      "Total loss:  -3.2092 | PDE Loss:  -5.0967 | Function Loss:  -3.2695\n",
      "Total loss:  -3.2094 | PDE Loss:  -5.0953 | Function Loss:  -3.2699\n",
      "Total loss:  -3.2096 | PDE Loss:  -5.0931 | Function Loss:  -3.2705\n",
      "Total loss:  -3.2099 | PDE Loss:  -5.0882 | Function Loss:  -3.2716\n",
      "Total loss:  -3.2102 | PDE Loss:  -5.0867 | Function Loss:  -3.2721\n",
      "Total loss:  -3.2104 | PDE Loss:  -5.0852 | Function Loss:  -3.2725\n",
      "Total loss:  -3.2106 | PDE Loss:  -5.0843 | Function Loss:  -3.2729\n",
      "Total loss:  -3.2107 | PDE Loss:  -5.0836 | Function Loss:  -3.2732\n",
      "Total loss:  -3.2109 | PDE Loss:  -5.0836 | Function Loss:  -3.2734\n",
      "Total loss:  -3.211 | PDE Loss:  -5.0838 | Function Loss:  -3.2735\n",
      "Total loss:  -3.2111 | PDE Loss:  -5.0844 | Function Loss:  -3.2736\n",
      "Total loss:  -3.2114 | PDE Loss:  -5.0856 | Function Loss:  -3.2736\n",
      "Total loss:  -3.2116 | PDE Loss:  -5.0874 | Function Loss:  -3.2736\n",
      "Total loss:  -3.2118 | PDE Loss:  -5.0907 | Function Loss:  -3.2734\n",
      "Total loss:  -3.212 | PDE Loss:  -5.0922 | Function Loss:  -3.2734\n",
      "Total loss:  -3.2122 | PDE Loss:  -5.0935 | Function Loss:  -3.2734\n",
      "Total loss:  -3.2124 | PDE Loss:  -5.095 | Function Loss:  -3.2734\n",
      "Total loss:  -3.2126 | PDE Loss:  -5.096 | Function Loss:  -3.2735\n",
      "Total loss:  -3.2129 | PDE Loss:  -5.0974 | Function Loss:  -3.2736\n",
      "Total loss:  -3.2131 | PDE Loss:  -5.098 | Function Loss:  -3.2738\n",
      "Total loss:  -3.2134 | PDE Loss:  -5.0991 | Function Loss:  -3.274\n",
      "Total loss:  -3.2138 | PDE Loss:  -5.0992 | Function Loss:  -3.2744\n",
      "Total loss:  -3.2142 | PDE Loss:  -5.1007 | Function Loss:  -3.2746\n",
      "Total loss:  -3.2146 | PDE Loss:  -5.1017 | Function Loss:  -3.2749\n",
      "Total loss:  -3.215 | PDE Loss:  -5.1026 | Function Loss:  -3.2753\n",
      "Total loss:  -3.2153 | PDE Loss:  -5.1028 | Function Loss:  -3.2756\n",
      "Total loss:  -3.2156 | PDE Loss:  -5.1037 | Function Loss:  -3.2758\n",
      "Total loss:  -3.2161 | PDE Loss:  -5.1046 | Function Loss:  -3.2762\n",
      "Total loss:  -3.2166 | PDE Loss:  -5.108 | Function Loss:  -3.2763\n",
      "Total loss:  -3.2171 | PDE Loss:  -5.111 | Function Loss:  -3.2764\n",
      "Total loss:  -3.2177 | PDE Loss:  -5.1171 | Function Loss:  -3.2762\n",
      "Total loss:  -3.2183 | PDE Loss:  -5.1234 | Function Loss:  -3.276\n",
      "Total loss:  -3.2189 | PDE Loss:  -5.1313 | Function Loss:  -3.2756\n",
      "Total loss:  -3.2194 | PDE Loss:  -5.1383 | Function Loss:  -3.2752\n",
      "Total loss:  -3.2199 | PDE Loss:  -5.1434 | Function Loss:  -3.275\n",
      "Total loss:  -3.2205 | PDE Loss:  -5.1503 | Function Loss:  -3.2748\n",
      "Total loss:  -3.2214 | PDE Loss:  -5.1555 | Function Loss:  -3.2751\n",
      "Total loss:  -3.2212 | PDE Loss:  -5.1575 | Function Loss:  -3.2747\n",
      "Total loss:  -3.2217 | PDE Loss:  -5.1584 | Function Loss:  -3.2752\n",
      "Total loss:  -3.2224 | PDE Loss:  -5.1618 | Function Loss:  -3.2754\n",
      "Total loss:  -3.2229 | PDE Loss:  -5.1625 | Function Loss:  -3.2759\n",
      "Total loss:  -3.2234 | PDE Loss:  -5.163 | Function Loss:  -3.2764\n",
      "Total loss:  -3.2238 | PDE Loss:  -5.1625 | Function Loss:  -3.277\n",
      "Total loss:  -3.2242 | PDE Loss:  -5.1616 | Function Loss:  -3.2775\n",
      "Total loss:  -3.2245 | PDE Loss:  -5.1606 | Function Loss:  -3.278\n",
      "Total loss:  -3.2249 | PDE Loss:  -5.1589 | Function Loss:  -3.2786\n",
      "Total loss:  -3.2252 | PDE Loss:  -5.1577 | Function Loss:  -3.2791\n",
      "Total loss:  -3.2255 | PDE Loss:  -5.1562 | Function Loss:  -3.2796\n",
      "Total loss:  -3.2257 | PDE Loss:  -5.1552 | Function Loss:  -3.2801\n",
      "Total loss:  -3.2259 | PDE Loss:  -5.1549 | Function Loss:  -3.2804\n",
      "Total loss:  -3.2262 | PDE Loss:  -5.1534 | Function Loss:  -3.2809\n",
      "Total loss:  -3.2265 | PDE Loss:  -5.1546 | Function Loss:  -3.2811\n",
      "Total loss:  -3.2269 | PDE Loss:  -5.1547 | Function Loss:  -3.2814\n",
      "Total loss:  -3.2274 | PDE Loss:  -5.1574 | Function Loss:  -3.2817\n",
      "Total loss:  -3.2281 | PDE Loss:  -5.1599 | Function Loss:  -3.2822\n",
      "Total loss:  -3.2287 | PDE Loss:  -5.1622 | Function Loss:  -3.2826\n",
      "Total loss:  -3.2293 | PDE Loss:  -5.1662 | Function Loss:  -3.2827\n",
      "Total loss:  -3.2299 | PDE Loss:  -5.1708 | Function Loss:  -3.2827\n",
      "Total loss:  -3.2303 | PDE Loss:  -5.1742 | Function Loss:  -3.2828\n",
      "Total loss:  -3.2307 | PDE Loss:  -5.1787 | Function Loss:  -3.2827\n",
      "Total loss:  -3.2312 | PDE Loss:  -5.1819 | Function Loss:  -3.2828\n",
      "Total loss:  -3.2316 | PDE Loss:  -5.1855 | Function Loss:  -3.2828\n",
      "Total loss:  -3.232 | PDE Loss:  -5.1868 | Function Loss:  -3.283\n",
      "Total loss:  -3.2323 | PDE Loss:  -5.1887 | Function Loss:  -3.2831\n",
      "Total loss:  -3.2325 | PDE Loss:  -5.1886 | Function Loss:  -3.2834\n",
      "Total loss:  -3.2328 | PDE Loss:  -5.1887 | Function Loss:  -3.2838\n",
      "Total loss:  -3.2333 | PDE Loss:  -5.1866 | Function Loss:  -3.2846\n",
      "Total loss:  -3.2338 | PDE Loss:  -5.1863 | Function Loss:  -3.2852\n",
      "Total loss:  -3.2343 | PDE Loss:  -5.1829 | Function Loss:  -3.2862\n",
      "Total loss:  -3.2348 | PDE Loss:  -5.1817 | Function Loss:  -3.2869\n",
      "Total loss:  -3.2353 | PDE Loss:  -5.1808 | Function Loss:  -3.2875\n",
      "Total loss:  -3.2356 | PDE Loss:  -5.1814 | Function Loss:  -3.2878\n",
      "Total loss:  -3.2358 | PDE Loss:  -5.1824 | Function Loss:  -3.2879\n",
      "Total loss:  -3.2359 | PDE Loss:  -5.1836 | Function Loss:  -3.2879\n",
      "Total loss:  -3.236 | PDE Loss:  -5.1852 | Function Loss:  -3.2878\n",
      "Total loss:  -3.2362 | PDE Loss:  -5.1866 | Function Loss:  -3.2878\n",
      "Total loss:  -3.2364 | PDE Loss:  -5.1883 | Function Loss:  -3.2879\n",
      "Total loss:  -3.2367 | PDE Loss:  -5.19 | Function Loss:  -3.288\n",
      "Total loss:  -3.2371 | PDE Loss:  -5.1914 | Function Loss:  -3.2882\n",
      "Total loss:  -3.2374 | PDE Loss:  -5.1901 | Function Loss:  -3.2888\n",
      "Total loss:  -3.2377 | PDE Loss:  -5.1893 | Function Loss:  -3.2892\n",
      "Total loss:  -3.2382 | PDE Loss:  -5.187 | Function Loss:  -3.29\n",
      "Total loss:  -3.2388 | PDE Loss:  -5.1855 | Function Loss:  -3.2909\n",
      "Total loss:  -3.2394 | PDE Loss:  -5.1828 | Function Loss:  -3.292\n",
      "Total loss:  -3.24 | PDE Loss:  -5.1817 | Function Loss:  -3.2928\n",
      "Total loss:  -3.2407 | PDE Loss:  -5.1806 | Function Loss:  -3.2937\n",
      "Total loss:  -3.2409 | PDE Loss:  -5.1835 | Function Loss:  -3.2936\n",
      "Total loss:  -3.2414 | PDE Loss:  -5.1849 | Function Loss:  -3.294\n",
      "Total loss:  -3.2419 | PDE Loss:  -5.187 | Function Loss:  -3.2943\n",
      "Total loss:  -3.2424 | PDE Loss:  -5.1906 | Function Loss:  -3.2943\n",
      "Total loss:  -3.2427 | PDE Loss:  -5.1936 | Function Loss:  -3.2942\n",
      "Total loss:  -3.243 | PDE Loss:  -5.1971 | Function Loss:  -3.2941\n",
      "Total loss:  -3.2433 | PDE Loss:  -5.201 | Function Loss:  -3.2941\n",
      "Total loss:  -3.2437 | PDE Loss:  -5.2033 | Function Loss:  -3.2942\n",
      "Total loss:  -3.2438 | PDE Loss:  -5.2168 | Function Loss:  -3.2927\n",
      "Total loss:  -3.2442 | PDE Loss:  -5.2149 | Function Loss:  -3.2934\n",
      "Total loss:  -3.2445 | PDE Loss:  -5.213 | Function Loss:  -3.2939\n",
      "Total loss:  -3.2447 | PDE Loss:  -5.2097 | Function Loss:  -3.2946\n",
      "Total loss:  -3.2449 | PDE Loss:  -5.2079 | Function Loss:  -3.295\n",
      "Total loss:  -3.2451 | PDE Loss:  -5.2058 | Function Loss:  -3.2955\n",
      "Total loss:  -3.2454 | PDE Loss:  -5.2052 | Function Loss:  -3.2958\n",
      "Total loss:  -3.2456 | PDE Loss:  -5.2043 | Function Loss:  -3.2962\n",
      "Total loss:  -3.2459 | PDE Loss:  -5.205 | Function Loss:  -3.2964\n",
      "Total loss:  -3.2461 | PDE Loss:  -5.2058 | Function Loss:  -3.2966\n",
      "Total loss:  -3.2463 | PDE Loss:  -5.2073 | Function Loss:  -3.2967\n",
      "Total loss:  -3.2466 | PDE Loss:  -5.2088 | Function Loss:  -3.2967\n",
      "Total loss:  -3.2467 | PDE Loss:  -5.2121 | Function Loss:  -3.2965\n",
      "Total loss:  -3.2469 | PDE Loss:  -5.2127 | Function Loss:  -3.2967\n",
      "Total loss:  -3.2471 | PDE Loss:  -5.2134 | Function Loss:  -3.2968\n",
      "Total loss:  -3.2473 | PDE Loss:  -5.2139 | Function Loss:  -3.297\n",
      "Total loss:  -3.2475 | PDE Loss:  -5.2143 | Function Loss:  -3.2971\n",
      "Total loss:  -3.2477 | PDE Loss:  -5.2149 | Function Loss:  -3.2973\n",
      "Total loss:  -3.248 | PDE Loss:  -5.2156 | Function Loss:  -3.2975\n",
      "Total loss:  -3.2482 | PDE Loss:  -5.2158 | Function Loss:  -3.2978\n",
      "Total loss:  -3.2485 | PDE Loss:  -5.2167 | Function Loss:  -3.2979\n",
      "Total loss:  -3.2487 | PDE Loss:  -5.2174 | Function Loss:  -3.2981\n",
      "Total loss:  -3.249 | PDE Loss:  -5.2182 | Function Loss:  -3.2983\n",
      "Total loss:  -3.2492 | PDE Loss:  -5.2188 | Function Loss:  -3.2985\n",
      "Total loss:  -3.2494 | PDE Loss:  -5.2192 | Function Loss:  -3.2987\n",
      "Total loss:  -3.2496 | PDE Loss:  -5.2194 | Function Loss:  -3.2989\n",
      "Total loss:  -3.2499 | PDE Loss:  -5.2192 | Function Loss:  -3.2992\n",
      "Total loss:  -3.2501 | PDE Loss:  -5.2188 | Function Loss:  -3.2995\n",
      "Total loss:  -3.2503 | PDE Loss:  -5.2182 | Function Loss:  -3.2998\n",
      "Total loss:  -3.2506 | PDE Loss:  -5.2175 | Function Loss:  -3.3002\n",
      "Total loss:  -3.2508 | PDE Loss:  -5.2169 | Function Loss:  -3.3005\n",
      "Total loss:  -3.2512 | PDE Loss:  -5.2157 | Function Loss:  -3.3011\n",
      "Total loss:  -3.2515 | PDE Loss:  -5.2151 | Function Loss:  -3.3015\n",
      "Total loss:  -3.2518 | PDE Loss:  -5.2141 | Function Loss:  -3.3019\n",
      "Total loss:  -3.2521 | PDE Loss:  -5.2139 | Function Loss:  -3.3023\n",
      "Total loss:  -3.2526 | PDE Loss:  -5.2127 | Function Loss:  -3.3031\n",
      "Total loss:  -3.2533 | PDE Loss:  -5.2136 | Function Loss:  -3.3036\n",
      "Total loss:  -3.2538 | PDE Loss:  -5.2138 | Function Loss:  -3.3042\n",
      "Total loss:  -3.2543 | PDE Loss:  -5.2136 | Function Loss:  -3.3048\n",
      "Total loss:  -3.2548 | PDE Loss:  -5.2138 | Function Loss:  -3.3054\n",
      "Total loss:  -3.2551 | PDE Loss:  -5.2145 | Function Loss:  -3.3056\n",
      "Total loss:  -3.2554 | PDE Loss:  -5.2152 | Function Loss:  -3.3059\n",
      "Total loss:  -3.2556 | PDE Loss:  -5.2162 | Function Loss:  -3.306\n",
      "Total loss:  -3.2558 | PDE Loss:  -5.2174 | Function Loss:  -3.306\n",
      "Total loss:  -3.2559 | PDE Loss:  -5.219 | Function Loss:  -3.306\n",
      "Total loss:  -3.2561 | PDE Loss:  -5.2197 | Function Loss:  -3.3061\n",
      "Total loss:  -3.2563 | PDE Loss:  -5.2233 | Function Loss:  -3.3059\n",
      "Total loss:  -3.2565 | PDE Loss:  -5.2253 | Function Loss:  -3.3059\n",
      "Total loss:  -3.2568 | PDE Loss:  -5.2274 | Function Loss:  -3.3059\n",
      "Total loss:  -3.257 | PDE Loss:  -5.229 | Function Loss:  -3.306\n",
      "Total loss:  -3.2572 | PDE Loss:  -5.23 | Function Loss:  -3.3061\n",
      "Total loss:  -3.2573 | PDE Loss:  -5.2293 | Function Loss:  -3.3062\n",
      "Total loss:  -3.2575 | PDE Loss:  -5.2314 | Function Loss:  -3.3062\n",
      "Total loss:  -3.2576 | PDE Loss:  -5.2318 | Function Loss:  -3.3064\n",
      "Total loss:  -3.2578 | PDE Loss:  -5.2339 | Function Loss:  -3.3063\n",
      "Total loss:  -3.258 | PDE Loss:  -5.2344 | Function Loss:  -3.3064\n",
      "Total loss:  -3.2581 | PDE Loss:  -5.2356 | Function Loss:  -3.3065\n",
      "Total loss:  -3.2583 | PDE Loss:  -5.236 | Function Loss:  -3.3067\n",
      "Total loss:  -3.2586 | PDE Loss:  -5.2357 | Function Loss:  -3.307\n",
      "Total loss:  -3.2588 | PDE Loss:  -5.2358 | Function Loss:  -3.3072\n",
      "Total loss:  -3.2591 | PDE Loss:  -5.2352 | Function Loss:  -3.3075\n",
      "Total loss:  -3.2593 | PDE Loss:  -5.2344 | Function Loss:  -3.3079\n",
      "Total loss:  -3.2596 | PDE Loss:  -5.234 | Function Loss:  -3.3083\n",
      "Total loss:  -3.2598 | PDE Loss:  -5.2337 | Function Loss:  -3.3085\n",
      "Total loss:  -3.26 | PDE Loss:  -5.2335 | Function Loss:  -3.3088\n",
      "Total loss:  -3.2602 | PDE Loss:  -5.2335 | Function Loss:  -3.3091\n",
      "Total loss:  -3.2605 | PDE Loss:  -5.2328 | Function Loss:  -3.3095\n",
      "Total loss:  -3.2608 | PDE Loss:  -5.2328 | Function Loss:  -3.3098\n",
      "Total loss:  -3.2611 | PDE Loss:  -5.2325 | Function Loss:  -3.3101\n",
      "Total loss:  -3.2613 | PDE Loss:  -5.2313 | Function Loss:  -3.3105\n",
      "Total loss:  -3.2616 | PDE Loss:  -5.2308 | Function Loss:  -3.3109\n",
      "Total loss:  -3.2618 | PDE Loss:  -5.2286 | Function Loss:  -3.3115\n",
      "Total loss:  -3.2621 | PDE Loss:  -5.227 | Function Loss:  -3.3119\n",
      "Total loss:  -3.2625 | PDE Loss:  -5.2246 | Function Loss:  -3.3126\n",
      "Total loss:  -3.263 | PDE Loss:  -5.2214 | Function Loss:  -3.3136\n",
      "Total loss:  -3.2635 | PDE Loss:  -5.2197 | Function Loss:  -3.3144\n",
      "Total loss:  -3.2639 | PDE Loss:  -5.2179 | Function Loss:  -3.3151\n",
      "Total loss:  -3.2642 | PDE Loss:  -5.2182 | Function Loss:  -3.3154\n",
      "Total loss:  -3.2645 | PDE Loss:  -5.22 | Function Loss:  -3.3154\n",
      "Total loss:  -3.2647 | PDE Loss:  -5.223 | Function Loss:  -3.3154\n",
      "Total loss:  -3.2649 | PDE Loss:  -5.2264 | Function Loss:  -3.3152\n",
      "Total loss:  -3.2652 | PDE Loss:  -5.23 | Function Loss:  -3.315\n",
      "Total loss:  -3.2655 | PDE Loss:  -5.2345 | Function Loss:  -3.3148\n",
      "Total loss:  -3.2658 | PDE Loss:  -5.2363 | Function Loss:  -3.315\n",
      "Total loss:  -3.2661 | PDE Loss:  -5.2387 | Function Loss:  -3.315\n",
      "Total loss:  -3.2664 | PDE Loss:  -5.2393 | Function Loss:  -3.3153\n",
      "Total loss:  -3.2667 | PDE Loss:  -5.2367 | Function Loss:  -3.316\n",
      "Total loss:  -3.2671 | PDE Loss:  -5.2356 | Function Loss:  -3.3165\n",
      "Total loss:  -3.2673 | PDE Loss:  -5.2324 | Function Loss:  -3.3171\n",
      "Total loss:  -3.2675 | PDE Loss:  -5.2278 | Function Loss:  -3.3179\n",
      "Total loss:  -3.2678 | PDE Loss:  -5.2211 | Function Loss:  -3.3191\n",
      "Total loss:  -3.268 | PDE Loss:  -5.2234 | Function Loss:  -3.319\n",
      "Total loss:  -3.2682 | PDE Loss:  -5.2202 | Function Loss:  -3.3196\n",
      "Total loss:  -3.2685 | PDE Loss:  -5.2178 | Function Loss:  -3.3202\n",
      "Total loss:  -3.2687 | PDE Loss:  -5.2161 | Function Loss:  -3.3207\n",
      "Total loss:  -3.2688 | PDE Loss:  -5.2151 | Function Loss:  -3.321\n",
      "Total loss:  -3.269 | PDE Loss:  -5.2146 | Function Loss:  -3.3213\n",
      "Total loss:  -3.2692 | PDE Loss:  -5.2141 | Function Loss:  -3.3215\n",
      "Total loss:  -3.2695 | PDE Loss:  -5.2136 | Function Loss:  -3.3219\n",
      "Total loss:  -3.2698 | PDE Loss:  -5.2134 | Function Loss:  -3.3223\n",
      "Total loss:  -3.2701 | PDE Loss:  -5.2126 | Function Loss:  -3.3227\n",
      "Total loss:  -3.2703 | PDE Loss:  -5.2124 | Function Loss:  -3.323\n",
      "Total loss:  -3.2705 | PDE Loss:  -5.2124 | Function Loss:  -3.3232\n",
      "Total loss:  -3.2706 | PDE Loss:  -5.213 | Function Loss:  -3.3233\n",
      "Total loss:  -3.2708 | PDE Loss:  -5.2131 | Function Loss:  -3.3234\n",
      "Total loss:  -3.2709 | PDE Loss:  -5.2138 | Function Loss:  -3.3235\n",
      "Total loss:  -3.2711 | PDE Loss:  -5.2144 | Function Loss:  -3.3237\n",
      "Total loss:  -3.2714 | PDE Loss:  -5.2137 | Function Loss:  -3.324\n",
      "Total loss:  -3.2716 | PDE Loss:  -5.2142 | Function Loss:  -3.3242\n",
      "Total loss:  -3.272 | PDE Loss:  -5.2117 | Function Loss:  -3.325\n",
      "Total loss:  -3.2724 | PDE Loss:  -5.2121 | Function Loss:  -3.3254\n",
      "Total loss:  -3.2727 | PDE Loss:  -5.2107 | Function Loss:  -3.3259\n",
      "Total loss:  -3.273 | PDE Loss:  -5.2101 | Function Loss:  -3.3263\n",
      "Total loss:  -3.2732 | PDE Loss:  -5.2079 | Function Loss:  -3.3269\n",
      "Total loss:  -3.2734 | PDE Loss:  -5.2074 | Function Loss:  -3.3272\n",
      "Total loss:  -3.2736 | PDE Loss:  -5.2062 | Function Loss:  -3.3275\n",
      "Total loss:  -3.2737 | PDE Loss:  -5.2051 | Function Loss:  -3.3278\n",
      "Total loss:  -3.274 | PDE Loss:  -5.2042 | Function Loss:  -3.3282\n",
      "Total loss:  -3.2742 | PDE Loss:  -5.2036 | Function Loss:  -3.3285\n",
      "Total loss:  -3.2744 | PDE Loss:  -5.2034 | Function Loss:  -3.3288\n",
      "Total loss:  -3.2745 | PDE Loss:  -5.2037 | Function Loss:  -3.3289\n",
      "Total loss:  -3.2747 | PDE Loss:  -5.2047 | Function Loss:  -3.3289\n",
      "Total loss:  -3.2748 | PDE Loss:  -5.2062 | Function Loss:  -3.3289\n",
      "Total loss:  -3.2749 | PDE Loss:  -5.2082 | Function Loss:  -3.3288\n",
      "Total loss:  -3.2751 | PDE Loss:  -5.2107 | Function Loss:  -3.3287\n",
      "Total loss:  -3.2754 | PDE Loss:  -5.2138 | Function Loss:  -3.3285\n",
      "Total loss:  -3.2756 | PDE Loss:  -5.2167 | Function Loss:  -3.3284\n",
      "Total loss:  -3.2758 | PDE Loss:  -5.2186 | Function Loss:  -3.3284\n",
      "Total loss:  -3.276 | PDE Loss:  -5.2198 | Function Loss:  -3.3285\n",
      "Total loss:  -3.2762 | PDE Loss:  -5.2202 | Function Loss:  -3.3287\n",
      "Total loss:  -3.2764 | PDE Loss:  -5.2207 | Function Loss:  -3.3288\n",
      "Total loss:  -3.2765 | PDE Loss:  -5.2201 | Function Loss:  -3.329\n",
      "Total loss:  -3.2766 | PDE Loss:  -5.2195 | Function Loss:  -3.3292\n",
      "Total loss:  -3.2768 | PDE Loss:  -5.2187 | Function Loss:  -3.3295\n",
      "Total loss:  -3.2769 | PDE Loss:  -5.2187 | Function Loss:  -3.3297\n",
      "Total loss:  -3.2771 | PDE Loss:  -5.2189 | Function Loss:  -3.3298\n",
      "Total loss:  -3.2772 | PDE Loss:  -5.2202 | Function Loss:  -3.3298\n",
      "Total loss:  -3.2774 | PDE Loss:  -5.2215 | Function Loss:  -3.3298\n",
      "Total loss:  -3.2775 | PDE Loss:  -5.2234 | Function Loss:  -3.3297\n",
      "Total loss:  -3.2777 | PDE Loss:  -5.2258 | Function Loss:  -3.3296\n",
      "Total loss:  -3.278 | PDE Loss:  -5.2284 | Function Loss:  -3.3296\n",
      "Total loss:  -3.2782 | PDE Loss:  -5.2306 | Function Loss:  -3.3296\n",
      "Total loss:  -3.2785 | PDE Loss:  -5.2317 | Function Loss:  -3.3297\n",
      "Total loss:  -3.2787 | PDE Loss:  -5.2322 | Function Loss:  -3.3299\n",
      "Total loss:  -3.2789 | PDE Loss:  -5.2321 | Function Loss:  -3.3302\n",
      "Total loss:  -3.2792 | PDE Loss:  -5.2306 | Function Loss:  -3.3307\n",
      "Total loss:  -3.2793 | PDE Loss:  -5.2301 | Function Loss:  -3.3309\n",
      "Total loss:  -3.2795 | PDE Loss:  -5.2298 | Function Loss:  -3.3312\n",
      "Total loss:  -3.2798 | PDE Loss:  -5.2293 | Function Loss:  -3.3316\n",
      "Total loss:  -3.2801 | PDE Loss:  -5.2297 | Function Loss:  -3.3319\n",
      "Total loss:  -3.2804 | PDE Loss:  -5.2301 | Function Loss:  -3.3322\n",
      "Total loss:  -3.2808 | PDE Loss:  -5.2322 | Function Loss:  -3.3323\n",
      "Total loss:  -3.2811 | PDE Loss:  -5.2343 | Function Loss:  -3.3324\n",
      "Total loss:  -3.2814 | PDE Loss:  -5.2382 | Function Loss:  -3.3322\n",
      "Total loss:  -3.2817 | PDE Loss:  -5.2411 | Function Loss:  -3.3322\n",
      "Total loss:  -3.282 | PDE Loss:  -5.2465 | Function Loss:  -3.3319\n",
      "Total loss:  -3.2821 | PDE Loss:  -5.2486 | Function Loss:  -3.3318\n",
      "Total loss:  -3.2824 | PDE Loss:  -5.2505 | Function Loss:  -3.3319\n",
      "Total loss:  -3.2827 | PDE Loss:  -5.2506 | Function Loss:  -3.3322\n",
      "Total loss:  -3.283 | PDE Loss:  -5.2503 | Function Loss:  -3.3326\n",
      "Total loss:  -3.2832 | PDE Loss:  -5.2422 | Function Loss:  -3.3338\n",
      "Total loss:  -3.2835 | PDE Loss:  -5.2422 | Function Loss:  -3.3341\n",
      "Total loss:  -3.2838 | PDE Loss:  -5.2407 | Function Loss:  -3.3347\n",
      "Total loss:  -3.2842 | PDE Loss:  -5.2379 | Function Loss:  -3.3354\n",
      "Total loss:  -3.2846 | PDE Loss:  -5.2336 | Function Loss:  -3.3364\n",
      "Total loss:  -3.285 | PDE Loss:  -5.2297 | Function Loss:  -3.3373\n",
      "Total loss:  -3.2855 | PDE Loss:  -5.2217 | Function Loss:  -3.3389\n",
      "Total loss:  -3.2858 | PDE Loss:  -5.2185 | Function Loss:  -3.3397\n",
      "Total loss:  -3.2865 | PDE Loss:  -5.2155 | Function Loss:  -3.3409\n",
      "Total loss:  -3.2871 | PDE Loss:  -5.2124 | Function Loss:  -3.342\n",
      "Total loss:  -3.2877 | PDE Loss:  -5.2139 | Function Loss:  -3.3425\n",
      "Total loss:  -3.2881 | PDE Loss:  -5.2164 | Function Loss:  -3.3426\n",
      "Total loss:  -3.2887 | PDE Loss:  -5.2203 | Function Loss:  -3.3428\n",
      "Total loss:  -3.2894 | PDE Loss:  -5.2236 | Function Loss:  -3.3431\n",
      "Total loss:  -3.2899 | PDE Loss:  -5.2257 | Function Loss:  -3.3435\n",
      "Total loss:  -3.2905 | PDE Loss:  -5.2249 | Function Loss:  -3.3441\n",
      "Total loss:  -3.291 | PDE Loss:  -5.2219 | Function Loss:  -3.3451\n",
      "Total loss:  -3.2916 | PDE Loss:  -5.2159 | Function Loss:  -3.3467\n",
      "Total loss:  -3.2923 | PDE Loss:  -5.2061 | Function Loss:  -3.3488\n",
      "Total loss:  -3.2927 | PDE Loss:  -5.2041 | Function Loss:  -3.3495\n",
      "Total loss:  -3.2933 | PDE Loss:  -5.1987 | Function Loss:  -3.351\n",
      "Total loss:  -3.2937 | PDE Loss:  -5.1963 | Function Loss:  -3.3517\n",
      "Total loss:  -3.2939 | PDE Loss:  -5.1935 | Function Loss:  -3.3524\n",
      "Total loss:  -3.2942 | PDE Loss:  -5.1921 | Function Loss:  -3.3529\n",
      "Total loss:  -3.2945 | PDE Loss:  -5.1906 | Function Loss:  -3.3535\n",
      "Total loss:  -3.2949 | PDE Loss:  -5.1901 | Function Loss:  -3.354\n",
      "Total loss:  -3.2954 | PDE Loss:  -5.1899 | Function Loss:  -3.3546\n",
      "Total loss:  -3.2958 | PDE Loss:  -5.1903 | Function Loss:  -3.355\n",
      "Total loss:  -3.2962 | PDE Loss:  -5.1935 | Function Loss:  -3.3551\n",
      "Total loss:  -3.2966 | PDE Loss:  -5.1956 | Function Loss:  -3.3552\n",
      "Total loss:  -3.297 | PDE Loss:  -5.1995 | Function Loss:  -3.3551\n",
      "Total loss:  -3.2974 | PDE Loss:  -5.2021 | Function Loss:  -3.3552\n",
      "Total loss:  -3.2978 | PDE Loss:  -5.2046 | Function Loss:  -3.3553\n",
      "Total loss:  -3.2983 | PDE Loss:  -5.2053 | Function Loss:  -3.3558\n",
      "Total loss:  -3.2988 | PDE Loss:  -5.2071 | Function Loss:  -3.3561\n",
      "Total loss:  -3.299 | PDE Loss:  -5.2025 | Function Loss:  -3.3569\n",
      "Total loss:  -3.2995 | PDE Loss:  -5.2029 | Function Loss:  -3.3575\n",
      "Total loss:  -3.2999 | PDE Loss:  -5.202 | Function Loss:  -3.3581\n",
      "Total loss:  -3.3002 | PDE Loss:  -5.1998 | Function Loss:  -3.3587\n",
      "Total loss:  -3.3004 | PDE Loss:  -5.1972 | Function Loss:  -3.3593\n",
      "Total loss:  -3.3006 | PDE Loss:  -5.1955 | Function Loss:  -3.3598\n",
      "Total loss:  -3.3008 | PDE Loss:  -5.1926 | Function Loss:  -3.3604\n",
      "Total loss:  -3.301 | PDE Loss:  -5.1897 | Function Loss:  -3.3611\n",
      "Total loss:  -3.3014 | PDE Loss:  -5.1874 | Function Loss:  -3.3618\n",
      "Total loss:  -3.3018 | PDE Loss:  -5.1799 | Function Loss:  -3.3635\n",
      "Total loss:  -3.3021 | PDE Loss:  -5.1781 | Function Loss:  -3.3642\n",
      "Total loss:  -3.3026 | PDE Loss:  -5.1751 | Function Loss:  -3.3651\n",
      "Total loss:  -3.3031 | PDE Loss:  -5.1722 | Function Loss:  -3.3662\n",
      "Total loss:  -3.3037 | PDE Loss:  -5.1682 | Function Loss:  -3.3675\n",
      "Total loss:  -3.3043 | PDE Loss:  -5.1649 | Function Loss:  -3.3687\n",
      "Total loss:  -3.305 | PDE Loss:  -5.1591 | Function Loss:  -3.3704\n",
      "Total loss:  -3.3057 | PDE Loss:  -5.1539 | Function Loss:  -3.3722\n",
      "Total loss:  -3.3065 | PDE Loss:  -5.1487 | Function Loss:  -3.374\n",
      "Total loss:  -3.3071 | PDE Loss:  -5.1455 | Function Loss:  -3.3752\n",
      "Total loss:  -3.3075 | PDE Loss:  -5.1456 | Function Loss:  -3.3757\n",
      "Total loss:  -3.3079 | PDE Loss:  -5.1441 | Function Loss:  -3.3763\n",
      "Total loss:  -3.308 | PDE Loss:  -5.1463 | Function Loss:  -3.3761\n",
      "Total loss:  -3.3082 | PDE Loss:  -5.1472 | Function Loss:  -3.3762\n",
      "Total loss:  -3.3083 | PDE Loss:  -5.1491 | Function Loss:  -3.376\n",
      "Total loss:  -3.3085 | PDE Loss:  -5.1514 | Function Loss:  -3.3758\n",
      "Total loss:  -3.3087 | PDE Loss:  -5.1546 | Function Loss:  -3.3755\n",
      "Total loss:  -3.3088 | PDE Loss:  -5.1576 | Function Loss:  -3.3752\n",
      "Total loss:  -3.309 | PDE Loss:  -5.1602 | Function Loss:  -3.3749\n",
      "Total loss:  -3.3092 | PDE Loss:  -5.1624 | Function Loss:  -3.3748\n",
      "Total loss:  -3.3094 | PDE Loss:  -5.1643 | Function Loss:  -3.3747\n",
      "Total loss:  -3.3096 | PDE Loss:  -5.1658 | Function Loss:  -3.3747\n",
      "Total loss:  -3.3098 | PDE Loss:  -5.1668 | Function Loss:  -3.3747\n",
      "Total loss:  -3.31 | PDE Loss:  -5.1676 | Function Loss:  -3.3748\n",
      "Total loss:  -3.3102 | PDE Loss:  -5.1671 | Function Loss:  -3.3752\n",
      "Total loss:  -3.3104 | PDE Loss:  -5.1667 | Function Loss:  -3.3754\n",
      "Total loss:  -3.3106 | PDE Loss:  -5.1653 | Function Loss:  -3.376\n",
      "Total loss:  -3.3108 | PDE Loss:  -5.1649 | Function Loss:  -3.3762\n",
      "Total loss:  -3.311 | PDE Loss:  -5.1649 | Function Loss:  -3.3765\n",
      "Total loss:  -3.3111 | PDE Loss:  -5.1658 | Function Loss:  -3.3765\n",
      "Total loss:  -3.3113 | PDE Loss:  -5.1668 | Function Loss:  -3.3765\n",
      "Total loss:  -3.3115 | PDE Loss:  -5.1681 | Function Loss:  -3.3765\n",
      "Total loss:  -3.3116 | PDE Loss:  -5.1693 | Function Loss:  -3.3765\n",
      "Total loss:  -3.3118 | PDE Loss:  -5.17 | Function Loss:  -3.3766\n",
      "Total loss:  -3.312 | PDE Loss:  -5.1703 | Function Loss:  -3.3768\n",
      "Total loss:  -3.3121 | PDE Loss:  -5.1696 | Function Loss:  -3.377\n",
      "Total loss:  -3.3123 | PDE Loss:  -5.1686 | Function Loss:  -3.3774\n",
      "Total loss:  -3.3124 | PDE Loss:  -5.1676 | Function Loss:  -3.3777\n",
      "Total loss:  -3.3126 | PDE Loss:  -5.1666 | Function Loss:  -3.3781\n",
      "Total loss:  -3.3127 | PDE Loss:  -5.1651 | Function Loss:  -3.3785\n",
      "Total loss:  -3.3129 | PDE Loss:  -5.1639 | Function Loss:  -3.3789\n",
      "Total loss:  -3.3131 | PDE Loss:  -5.1636 | Function Loss:  -3.3791\n",
      "Total loss:  -3.3132 | PDE Loss:  -5.1627 | Function Loss:  -3.3795\n",
      "Total loss:  -3.3134 | PDE Loss:  -5.1631 | Function Loss:  -3.3796\n",
      "Total loss:  -3.3136 | PDE Loss:  -5.1629 | Function Loss:  -3.3798\n",
      "Total loss:  -3.3138 | PDE Loss:  -5.1636 | Function Loss:  -3.38\n",
      "Total loss:  -3.314 | PDE Loss:  -5.1629 | Function Loss:  -3.3804\n",
      "Total loss:  -3.3143 | PDE Loss:  -5.1627 | Function Loss:  -3.3807\n",
      "Total loss:  -3.3146 | PDE Loss:  -5.1602 | Function Loss:  -3.3814\n",
      "Total loss:  -3.3148 | PDE Loss:  -5.1584 | Function Loss:  -3.382\n",
      "Total loss:  -3.3151 | PDE Loss:  -5.1566 | Function Loss:  -3.3826\n",
      "Total loss:  -3.3154 | PDE Loss:  -5.1526 | Function Loss:  -3.3836\n",
      "Total loss:  -3.3156 | PDE Loss:  -5.1507 | Function Loss:  -3.3843\n",
      "Total loss:  -3.3159 | PDE Loss:  -5.1465 | Function Loss:  -3.3853\n",
      "Total loss:  -3.316 | PDE Loss:  -5.145 | Function Loss:  -3.3857\n",
      "Total loss:  -3.3162 | PDE Loss:  -5.1442 | Function Loss:  -3.386\n",
      "Total loss:  -3.3163 | PDE Loss:  -5.1434 | Function Loss:  -3.3864\n",
      "Total loss:  -3.3165 | PDE Loss:  -5.1444 | Function Loss:  -3.3863\n",
      "Total loss:  -3.3166 | PDE Loss:  -5.1452 | Function Loss:  -3.3863\n",
      "Total loss:  -3.3167 | PDE Loss:  -5.1466 | Function Loss:  -3.3862\n",
      "Total loss:  -3.3167 | PDE Loss:  -5.1479 | Function Loss:  -3.3861\n",
      "Total loss:  -3.3168 | PDE Loss:  -5.1491 | Function Loss:  -3.3859\n",
      "Total loss:  -3.3169 | PDE Loss:  -5.15 | Function Loss:  -3.3859\n",
      "Total loss:  -3.317 | PDE Loss:  -5.1502 | Function Loss:  -3.3859\n",
      "Total loss:  -3.3171 | PDE Loss:  -5.1498 | Function Loss:  -3.3862\n",
      "Total loss:  -3.3173 | PDE Loss:  -5.1488 | Function Loss:  -3.3865\n",
      "Total loss:  -3.3174 | PDE Loss:  -5.1449 | Function Loss:  -3.3873\n",
      "Total loss:  -3.3176 | PDE Loss:  -5.1445 | Function Loss:  -3.3876\n",
      "Total loss:  -3.3177 | PDE Loss:  -5.1437 | Function Loss:  -3.3879\n",
      "Total loss:  -3.3179 | PDE Loss:  -5.1431 | Function Loss:  -3.3883\n",
      "Total loss:  -3.3181 | PDE Loss:  -5.1426 | Function Loss:  -3.3886\n",
      "Total loss:  -3.3183 | PDE Loss:  -5.1426 | Function Loss:  -3.3888\n",
      "Total loss:  -3.3186 | PDE Loss:  -5.1427 | Function Loss:  -3.3891\n",
      "Total loss:  -3.3189 | PDE Loss:  -5.1439 | Function Loss:  -3.3893\n",
      "Total loss:  -3.3192 | PDE Loss:  -5.1445 | Function Loss:  -3.3895\n",
      "Total loss:  -3.3194 | PDE Loss:  -5.1453 | Function Loss:  -3.3897\n",
      "Total loss:  -3.3198 | PDE Loss:  -5.1468 | Function Loss:  -3.3898\n",
      "Total loss:  -3.32 | PDE Loss:  -5.1475 | Function Loss:  -3.39\n",
      "Total loss:  -3.3202 | PDE Loss:  -5.1482 | Function Loss:  -3.3901\n",
      "Total loss:  -3.3204 | PDE Loss:  -5.1477 | Function Loss:  -3.3904\n",
      "Total loss:  -3.3206 | PDE Loss:  -5.1471 | Function Loss:  -3.3907\n",
      "Total loss:  -3.3209 | PDE Loss:  -5.1453 | Function Loss:  -3.3914\n",
      "Total loss:  -3.3212 | PDE Loss:  -5.1439 | Function Loss:  -3.392\n",
      "Total loss:  -3.3216 | PDE Loss:  -5.1412 | Function Loss:  -3.3929\n",
      "Total loss:  -3.3219 | PDE Loss:  -5.1394 | Function Loss:  -3.3936\n",
      "Total loss:  -3.3223 | PDE Loss:  -5.1377 | Function Loss:  -3.3944\n",
      "Total loss:  -3.3226 | PDE Loss:  -5.1375 | Function Loss:  -3.3948\n",
      "Total loss:  -3.3229 | PDE Loss:  -5.138 | Function Loss:  -3.3951\n",
      "Total loss:  -3.3232 | PDE Loss:  -5.1393 | Function Loss:  -3.3952\n",
      "Total loss:  -3.3236 | PDE Loss:  -5.1409 | Function Loss:  -3.3954\n",
      "Total loss:  -3.3241 | PDE Loss:  -5.1435 | Function Loss:  -3.3955\n",
      "Total loss:  -3.3246 | PDE Loss:  -5.1469 | Function Loss:  -3.3955\n",
      "Total loss:  -3.3252 | PDE Loss:  -5.1504 | Function Loss:  -3.3956\n",
      "Total loss:  -3.3258 | PDE Loss:  -5.1533 | Function Loss:  -3.3957\n",
      "Total loss:  -3.3261 | PDE Loss:  -5.1538 | Function Loss:  -3.3961\n",
      "Total loss:  -3.3265 | PDE Loss:  -5.1549 | Function Loss:  -3.3963\n",
      "Total loss:  -3.3268 | PDE Loss:  -5.1519 | Function Loss:  -3.3972\n",
      "Total loss:  -3.327 | PDE Loss:  -5.1515 | Function Loss:  -3.3975\n",
      "Total loss:  -3.3272 | PDE Loss:  -5.1494 | Function Loss:  -3.398\n",
      "Total loss:  -3.3273 | PDE Loss:  -5.1479 | Function Loss:  -3.3985\n",
      "Total loss:  -3.3274 | PDE Loss:  -5.147 | Function Loss:  -3.3988\n",
      "Total loss:  -3.3276 | PDE Loss:  -5.146 | Function Loss:  -3.3991\n",
      "Total loss:  -3.3277 | PDE Loss:  -5.1456 | Function Loss:  -3.3994\n",
      "Total loss:  -3.3278 | PDE Loss:  -5.146 | Function Loss:  -3.3994\n",
      "Total loss:  -3.3279 | PDE Loss:  -5.1462 | Function Loss:  -3.3995\n",
      "Total loss:  -3.328 | PDE Loss:  -5.1472 | Function Loss:  -3.3994\n",
      "Total loss:  -3.3281 | PDE Loss:  -5.1482 | Function Loss:  -3.3993\n",
      "Total loss:  -3.3282 | PDE Loss:  -5.1513 | Function Loss:  -3.3989\n",
      "Total loss:  -3.3283 | PDE Loss:  -5.1526 | Function Loss:  -3.3988\n",
      "Total loss:  -3.3284 | PDE Loss:  -5.1536 | Function Loss:  -3.3988\n",
      "Total loss:  -3.3286 | PDE Loss:  -5.1548 | Function Loss:  -3.3987\n",
      "Total loss:  -3.3287 | PDE Loss:  -5.1551 | Function Loss:  -3.3988\n",
      "Total loss:  -3.3288 | PDE Loss:  -5.1561 | Function Loss:  -3.3988\n",
      "Total loss:  -3.329 | PDE Loss:  -5.156 | Function Loss:  -3.399\n",
      "Total loss:  -3.3291 | PDE Loss:  -5.1557 | Function Loss:  -3.3992\n",
      "Total loss:  -3.3294 | PDE Loss:  -5.156 | Function Loss:  -3.3995\n",
      "Total loss:  -3.3296 | PDE Loss:  -5.1554 | Function Loss:  -3.3998\n",
      "Total loss:  -3.3299 | PDE Loss:  -5.1542 | Function Loss:  -3.4004\n",
      "Total loss:  -3.3301 | PDE Loss:  -5.1538 | Function Loss:  -3.4008\n",
      "Total loss:  -3.3304 | PDE Loss:  -5.1536 | Function Loss:  -3.4012\n",
      "Total loss:  -3.3307 | PDE Loss:  -5.1539 | Function Loss:  -3.4014\n",
      "Total loss:  -3.3309 | PDE Loss:  -5.1546 | Function Loss:  -3.4015\n",
      "Total loss:  -3.3311 | PDE Loss:  -5.1557 | Function Loss:  -3.4016\n",
      "Total loss:  -3.3313 | PDE Loss:  -5.1578 | Function Loss:  -3.4014\n",
      "Total loss:  -3.3315 | PDE Loss:  -5.1597 | Function Loss:  -3.4013\n",
      "Total loss:  -3.3317 | PDE Loss:  -5.1621 | Function Loss:  -3.4011\n",
      "Total loss:  -3.3319 | PDE Loss:  -5.1636 | Function Loss:  -3.4012\n",
      "Total loss:  -3.3321 | PDE Loss:  -5.165 | Function Loss:  -3.4011\n",
      "Total loss:  -3.3323 | PDE Loss:  -5.1656 | Function Loss:  -3.4012\n",
      "Total loss:  -3.3325 | PDE Loss:  -5.1655 | Function Loss:  -3.4015\n",
      "Total loss:  -3.3328 | PDE Loss:  -5.165 | Function Loss:  -3.4019\n",
      "Total loss:  -3.3331 | PDE Loss:  -5.1649 | Function Loss:  -3.4023\n",
      "Total loss:  -3.3333 | PDE Loss:  -5.163 | Function Loss:  -3.4029\n",
      "Total loss:  -3.3336 | PDE Loss:  -5.1622 | Function Loss:  -3.4034\n",
      "Total loss:  -3.3339 | PDE Loss:  -5.1606 | Function Loss:  -3.404\n",
      "Total loss:  -3.3342 | PDE Loss:  -5.1592 | Function Loss:  -3.4045\n",
      "Total loss:  -3.3344 | PDE Loss:  -5.1587 | Function Loss:  -3.4049\n",
      "Total loss:  -3.3346 | PDE Loss:  -5.1579 | Function Loss:  -3.4053\n",
      "Total loss:  -3.3349 | PDE Loss:  -5.158 | Function Loss:  -3.4056\n",
      "Total loss:  -3.3351 | PDE Loss:  -5.158 | Function Loss:  -3.4059\n",
      "Total loss:  -3.3352 | PDE Loss:  -5.1586 | Function Loss:  -3.4059\n",
      "Total loss:  -3.3354 | PDE Loss:  -5.1594 | Function Loss:  -3.4059\n",
      "Total loss:  -3.3356 | PDE Loss:  -5.1604 | Function Loss:  -3.406\n",
      "Total loss:  -3.3358 | PDE Loss:  -5.1609 | Function Loss:  -3.4062\n",
      "Total loss:  -3.3361 | PDE Loss:  -5.1613 | Function Loss:  -3.4064\n",
      "Total loss:  -3.3363 | PDE Loss:  -5.1604 | Function Loss:  -3.4069\n",
      "Total loss:  -3.3365 | PDE Loss:  -5.1604 | Function Loss:  -3.4071\n",
      "Total loss:  -3.3366 | PDE Loss:  -5.1595 | Function Loss:  -3.4074\n",
      "Total loss:  -3.3368 | PDE Loss:  -5.1586 | Function Loss:  -3.4077\n",
      "Total loss:  -3.3369 | PDE Loss:  -5.1597 | Function Loss:  -3.4076\n",
      "Total loss:  -3.337 | PDE Loss:  -5.1579 | Function Loss:  -3.4081\n",
      "Total loss:  -3.3371 | PDE Loss:  -5.1578 | Function Loss:  -3.4082\n",
      "Total loss:  -3.3372 | PDE Loss:  -5.1588 | Function Loss:  -3.4082\n",
      "Total loss:  -3.3373 | PDE Loss:  -5.1598 | Function Loss:  -3.4081\n",
      "Total loss:  -3.3374 | PDE Loss:  -5.1612 | Function Loss:  -3.4081\n",
      "Total loss:  -3.3376 | PDE Loss:  -5.1622 | Function Loss:  -3.408\n",
      "Total loss:  -3.3378 | PDE Loss:  -5.1647 | Function Loss:  -3.4078\n",
      "Total loss:  -3.338 | PDE Loss:  -5.1659 | Function Loss:  -3.4078\n",
      "Total loss:  -3.3382 | PDE Loss:  -5.1678 | Function Loss:  -3.4077\n",
      "Total loss:  -3.3384 | PDE Loss:  -5.1689 | Function Loss:  -3.4078\n",
      "Total loss:  -3.3386 | PDE Loss:  -5.1696 | Function Loss:  -3.4079\n",
      "Total loss:  -3.3389 | PDE Loss:  -5.1698 | Function Loss:  -3.4082\n",
      "Total loss:  -3.3391 | PDE Loss:  -5.1713 | Function Loss:  -3.4083\n",
      "Total loss:  -3.3394 | PDE Loss:  -5.172 | Function Loss:  -3.4085\n",
      "Total loss:  -3.3398 | PDE Loss:  -5.1734 | Function Loss:  -3.4087\n",
      "Total loss:  -3.3401 | PDE Loss:  -5.1752 | Function Loss:  -3.4088\n",
      "Total loss:  -3.3404 | PDE Loss:  -5.1771 | Function Loss:  -3.4088\n",
      "Total loss:  -3.3407 | PDE Loss:  -5.1796 | Function Loss:  -3.4087\n",
      "Total loss:  -3.341 | PDE Loss:  -5.1819 | Function Loss:  -3.4086\n",
      "Total loss:  -3.3413 | PDE Loss:  -5.1846 | Function Loss:  -3.4086\n",
      "Total loss:  -3.3417 | PDE Loss:  -5.1872 | Function Loss:  -3.4086\n",
      "Total loss:  -3.3422 | PDE Loss:  -5.1897 | Function Loss:  -3.4088\n",
      "Total loss:  -3.3428 | PDE Loss:  -5.192 | Function Loss:  -3.4091\n",
      "Total loss:  -3.3434 | PDE Loss:  -5.1932 | Function Loss:  -3.4096\n",
      "Total loss:  -3.344 | PDE Loss:  -5.193 | Function Loss:  -3.4104\n",
      "Total loss:  -3.3447 | PDE Loss:  -5.1925 | Function Loss:  -3.4111\n",
      "Total loss:  -3.3452 | PDE Loss:  -5.1907 | Function Loss:  -3.4121\n",
      "Total loss:  -3.3457 | PDE Loss:  -5.1894 | Function Loss:  -3.4128\n",
      "Total loss:  -3.3462 | PDE Loss:  -5.1884 | Function Loss:  -3.4136\n",
      "Total loss:  -3.3467 | PDE Loss:  -5.1885 | Function Loss:  -3.4142\n",
      "Total loss:  -3.3471 | PDE Loss:  -5.1898 | Function Loss:  -3.4144\n",
      "Total loss:  -3.3474 | PDE Loss:  -5.1903 | Function Loss:  -3.4148\n",
      "Total loss:  -3.3477 | PDE Loss:  -5.1918 | Function Loss:  -3.4148\n",
      "Total loss:  -3.3479 | PDE Loss:  -5.1934 | Function Loss:  -3.4148\n",
      "Total loss:  -3.3481 | PDE Loss:  -5.1954 | Function Loss:  -3.4147\n",
      "Total loss:  -3.3483 | PDE Loss:  -5.1965 | Function Loss:  -3.4148\n",
      "Total loss:  -3.3485 | PDE Loss:  -5.198 | Function Loss:  -3.4148\n",
      "Total loss:  -3.3488 | PDE Loss:  -5.1987 | Function Loss:  -3.4149\n",
      "Total loss:  -3.3489 | PDE Loss:  -5.198 | Function Loss:  -3.4152\n",
      "Total loss:  -3.3491 | PDE Loss:  -5.1957 | Function Loss:  -3.4158\n",
      "Total loss:  -3.3493 | PDE Loss:  -5.1951 | Function Loss:  -3.4162\n",
      "Total loss:  -3.3495 | PDE Loss:  -5.1939 | Function Loss:  -3.4166\n",
      "Total loss:  -3.3497 | PDE Loss:  -5.1929 | Function Loss:  -3.417\n",
      "Total loss:  -3.35 | PDE Loss:  -5.1914 | Function Loss:  -3.4175\n",
      "Total loss:  -3.3502 | PDE Loss:  -5.1904 | Function Loss:  -3.418\n",
      "Total loss:  -3.3504 | PDE Loss:  -5.1898 | Function Loss:  -3.4183\n",
      "Total loss:  -3.3507 | PDE Loss:  -5.1895 | Function Loss:  -3.4187\n",
      "Total loss:  -3.351 | PDE Loss:  -5.1898 | Function Loss:  -3.419\n",
      "Total loss:  -3.3514 | PDE Loss:  -5.1902 | Function Loss:  -3.4193\n",
      "Total loss:  -3.3517 | PDE Loss:  -5.1908 | Function Loss:  -3.4196\n",
      "Total loss:  -3.352 | PDE Loss:  -5.191 | Function Loss:  -3.4199\n",
      "Total loss:  -3.3523 | PDE Loss:  -5.191 | Function Loss:  -3.4203\n",
      "Total loss:  -3.3526 | PDE Loss:  -5.1906 | Function Loss:  -3.4208\n",
      "Total loss:  -3.353 | PDE Loss:  -5.1899 | Function Loss:  -3.4213\n",
      "Total loss:  -3.3533 | PDE Loss:  -5.1885 | Function Loss:  -3.422\n",
      "Total loss:  -3.3537 | PDE Loss:  -5.1865 | Function Loss:  -3.4227\n",
      "Total loss:  -3.3541 | PDE Loss:  -5.1842 | Function Loss:  -3.4236\n",
      "Total loss:  -3.3545 | PDE Loss:  -5.1817 | Function Loss:  -3.4245\n",
      "Total loss:  -3.355 | PDE Loss:  -5.1786 | Function Loss:  -3.4256\n",
      "Total loss:  -3.3555 | PDE Loss:  -5.1773 | Function Loss:  -3.4264\n",
      "Total loss:  -3.3558 | PDE Loss:  -5.1733 | Function Loss:  -3.4276\n",
      "Total loss:  -3.3561 | PDE Loss:  -5.1732 | Function Loss:  -3.4279\n",
      "Total loss:  -3.3563 | PDE Loss:  -5.1729 | Function Loss:  -3.4282\n",
      "Total loss:  -3.3565 | PDE Loss:  -5.1732 | Function Loss:  -3.4284\n",
      "Total loss:  -3.3568 | PDE Loss:  -5.1716 | Function Loss:  -3.429\n",
      "Total loss:  -3.357 | PDE Loss:  -5.1714 | Function Loss:  -3.4293\n",
      "Total loss:  -3.3573 | PDE Loss:  -5.1686 | Function Loss:  -3.4302\n",
      "Total loss:  -3.3576 | PDE Loss:  -5.1654 | Function Loss:  -3.4311\n",
      "Total loss:  -3.3581 | PDE Loss:  -5.1618 | Function Loss:  -3.4323\n",
      "Total loss:  -3.3585 | PDE Loss:  -5.1575 | Function Loss:  -3.4337\n",
      "Total loss:  -3.359 | PDE Loss:  -5.1547 | Function Loss:  -3.4347\n",
      "Total loss:  -3.3595 | PDE Loss:  -5.153 | Function Loss:  -3.4356\n",
      "Total loss:  -3.36 | PDE Loss:  -5.1527 | Function Loss:  -3.4363\n",
      "Total loss:  -3.3607 | PDE Loss:  -5.1554 | Function Loss:  -3.4367\n",
      "Total loss:  -3.3612 | PDE Loss:  -5.1592 | Function Loss:  -3.4365\n",
      "Total loss:  -3.3616 | PDE Loss:  -5.1628 | Function Loss:  -3.4363\n",
      "Total loss:  -3.362 | PDE Loss:  -5.1685 | Function Loss:  -3.4358\n",
      "Total loss:  -3.3624 | PDE Loss:  -5.1746 | Function Loss:  -3.4351\n",
      "Total loss:  -3.3628 | PDE Loss:  -5.1794 | Function Loss:  -3.4346\n",
      "Total loss:  -3.3631 | PDE Loss:  -5.1831 | Function Loss:  -3.4343\n",
      "Total loss:  -3.3633 | PDE Loss:  -5.1854 | Function Loss:  -3.4342\n",
      "Total loss:  -3.3636 | PDE Loss:  -5.1866 | Function Loss:  -3.4343\n",
      "Total loss:  -3.3638 | PDE Loss:  -5.1859 | Function Loss:  -3.4347\n",
      "Total loss:  -3.364 | PDE Loss:  -5.1847 | Function Loss:  -3.4352\n",
      "Total loss:  -3.3642 | PDE Loss:  -5.1821 | Function Loss:  -3.4358\n",
      "Total loss:  -3.3644 | PDE Loss:  -5.1797 | Function Loss:  -3.4365\n",
      "Total loss:  -3.3645 | PDE Loss:  -5.1769 | Function Loss:  -3.4372\n",
      "Total loss:  -3.3647 | PDE Loss:  -5.1749 | Function Loss:  -3.4378\n",
      "Total loss:  -3.365 | PDE Loss:  -5.1732 | Function Loss:  -3.4384\n",
      "Total loss:  -3.3653 | PDE Loss:  -5.1706 | Function Loss:  -3.4393\n",
      "Total loss:  -3.3657 | PDE Loss:  -5.1719 | Function Loss:  -3.4395\n",
      "Total loss:  -3.3659 | PDE Loss:  -5.1715 | Function Loss:  -3.4398\n",
      "Total loss:  -3.3662 | PDE Loss:  -5.1742 | Function Loss:  -3.4397\n",
      "Total loss:  -3.3665 | PDE Loss:  -5.1766 | Function Loss:  -3.4396\n",
      "Total loss:  -3.3669 | PDE Loss:  -5.18 | Function Loss:  -3.4394\n",
      "Total loss:  -3.3671 | PDE Loss:  -5.1811 | Function Loss:  -3.4395\n",
      "Total loss:  -3.3674 | PDE Loss:  -5.1834 | Function Loss:  -3.4394\n",
      "Total loss:  -3.3678 | PDE Loss:  -5.1852 | Function Loss:  -3.4396\n",
      "Total loss:  -3.3682 | PDE Loss:  -5.1859 | Function Loss:  -3.44\n",
      "Total loss:  -3.3687 | PDE Loss:  -5.1851 | Function Loss:  -3.4406\n",
      "Total loss:  -3.369 | PDE Loss:  -5.1838 | Function Loss:  -3.4412\n",
      "Total loss:  -3.3693 | PDE Loss:  -5.182 | Function Loss:  -3.4418\n",
      "Total loss:  -3.3694 | PDE Loss:  -5.1805 | Function Loss:  -3.4423\n",
      "Total loss:  -3.3696 | PDE Loss:  -5.1796 | Function Loss:  -3.4427\n",
      "Total loss:  -3.3698 | PDE Loss:  -5.1789 | Function Loss:  -3.4431\n",
      "Total loss:  -3.3701 | PDE Loss:  -5.178 | Function Loss:  -3.4435\n",
      "Total loss:  -3.3704 | PDE Loss:  -5.1776 | Function Loss:  -3.444\n",
      "Total loss:  -3.3705 | PDE Loss:  -5.1779 | Function Loss:  -3.4441\n",
      "Total loss:  -3.371 | PDE Loss:  -5.1794 | Function Loss:  -3.4444\n",
      "Total loss:  -3.3714 | PDE Loss:  -5.181 | Function Loss:  -3.4445\n",
      "Total loss:  -3.3717 | PDE Loss:  -5.1811 | Function Loss:  -3.4449\n",
      "Total loss:  -3.3721 | PDE Loss:  -5.1823 | Function Loss:  -3.4451\n",
      "Total loss:  -3.3725 | PDE Loss:  -5.1833 | Function Loss:  -3.4454\n",
      "Total loss:  -3.373 | PDE Loss:  -5.1845 | Function Loss:  -3.4458\n",
      "Total loss:  -3.3734 | PDE Loss:  -5.1857 | Function Loss:  -3.4461\n",
      "Total loss:  -3.3738 | PDE Loss:  -5.1877 | Function Loss:  -3.4462\n",
      "Total loss:  -3.3741 | PDE Loss:  -5.1899 | Function Loss:  -3.4462\n",
      "Total loss:  -3.3744 | PDE Loss:  -5.1922 | Function Loss:  -3.446\n",
      "Total loss:  -3.3746 | PDE Loss:  -5.195 | Function Loss:  -3.4458\n",
      "Total loss:  -3.3748 | PDE Loss:  -5.1979 | Function Loss:  -3.4455\n",
      "Total loss:  -3.375 | PDE Loss:  -5.201 | Function Loss:  -3.4452\n",
      "Total loss:  -3.3752 | PDE Loss:  -5.2039 | Function Loss:  -3.4449\n",
      "Total loss:  -3.3755 | PDE Loss:  -5.2071 | Function Loss:  -3.4447\n",
      "Total loss:  -3.3757 | PDE Loss:  -5.2101 | Function Loss:  -3.4444\n",
      "Total loss:  -3.3759 | PDE Loss:  -5.2108 | Function Loss:  -3.4446\n",
      "Total loss:  -3.3761 | PDE Loss:  -5.212 | Function Loss:  -3.4446\n",
      "Total loss:  -3.3764 | PDE Loss:  -5.2122 | Function Loss:  -3.4449\n",
      "Total loss:  -3.3766 | PDE Loss:  -5.2116 | Function Loss:  -3.4453\n",
      "Total loss:  -3.3768 | PDE Loss:  -5.2119 | Function Loss:  -3.4455\n",
      "Total loss:  -3.377 | PDE Loss:  -5.2112 | Function Loss:  -3.4458\n",
      "Total loss:  -3.3772 | PDE Loss:  -5.2111 | Function Loss:  -3.4461\n",
      "Total loss:  -3.3774 | PDE Loss:  -5.2119 | Function Loss:  -3.4462\n",
      "Total loss:  -3.3776 | PDE Loss:  -5.2116 | Function Loss:  -3.4465\n",
      "Total loss:  -3.3778 | PDE Loss:  -5.2125 | Function Loss:  -3.4465\n",
      "Total loss:  -3.378 | PDE Loss:  -5.2137 | Function Loss:  -3.4465\n",
      "Total loss:  -3.3781 | PDE Loss:  -5.2141 | Function Loss:  -3.4466\n",
      "Total loss:  -3.3783 | PDE Loss:  -5.2139 | Function Loss:  -3.4469\n",
      "Total loss:  -3.3785 | PDE Loss:  -5.213 | Function Loss:  -3.4473\n",
      "Total loss:  -3.3787 | PDE Loss:  -5.212 | Function Loss:  -3.4477\n",
      "Total loss:  -3.379 | PDE Loss:  -5.2107 | Function Loss:  -3.4482\n",
      "Total loss:  -3.3792 | PDE Loss:  -5.2087 | Function Loss:  -3.4488\n",
      "Total loss:  -3.3792 | PDE Loss:  -5.205 | Function Loss:  -3.4495\n",
      "Total loss:  -3.3795 | PDE Loss:  -5.2056 | Function Loss:  -3.4497\n",
      "Total loss:  -3.3797 | PDE Loss:  -5.2058 | Function Loss:  -3.4499\n",
      "Total loss:  -3.3798 | PDE Loss:  -5.2057 | Function Loss:  -3.4501\n",
      "Total loss:  -3.38 | PDE Loss:  -5.2051 | Function Loss:  -3.4504\n",
      "Total loss:  -3.3802 | PDE Loss:  -5.204 | Function Loss:  -3.4508\n",
      "Total loss:  -3.3804 | PDE Loss:  -5.2025 | Function Loss:  -3.4513\n",
      "Total loss:  -3.3806 | PDE Loss:  -5.2011 | Function Loss:  -3.4517\n",
      "Total loss:  -3.3808 | PDE Loss:  -5.1986 | Function Loss:  -3.4524\n",
      "Total loss:  -3.3809 | PDE Loss:  -5.1984 | Function Loss:  -3.4526\n",
      "Total loss:  -3.381 | PDE Loss:  -5.1978 | Function Loss:  -3.4528\n",
      "Total loss:  -3.3811 | PDE Loss:  -5.1979 | Function Loss:  -3.4529\n",
      "Total loss:  -3.3812 | PDE Loss:  -5.198 | Function Loss:  -3.453\n",
      "Total loss:  -3.3813 | PDE Loss:  -5.1985 | Function Loss:  -3.4531\n",
      "Total loss:  -3.3815 | PDE Loss:  -5.1992 | Function Loss:  -3.4532\n",
      "Total loss:  -3.3817 | PDE Loss:  -5.2005 | Function Loss:  -3.4532\n",
      "Total loss:  -3.382 | PDE Loss:  -5.2019 | Function Loss:  -3.4533\n",
      "Total loss:  -3.3822 | PDE Loss:  -5.2031 | Function Loss:  -3.4533\n",
      "Total loss:  -3.3825 | PDE Loss:  -5.2039 | Function Loss:  -3.4535\n",
      "Total loss:  -3.3828 | PDE Loss:  -5.2052 | Function Loss:  -3.4536\n",
      "Total loss:  -3.383 | PDE Loss:  -5.2052 | Function Loss:  -3.4539\n",
      "Total loss:  -3.3833 | PDE Loss:  -5.2053 | Function Loss:  -3.4542\n",
      "Total loss:  -3.3836 | PDE Loss:  -5.2043 | Function Loss:  -3.4547\n",
      "Total loss:  -3.3838 | PDE Loss:  -5.2041 | Function Loss:  -3.4551\n",
      "Total loss:  -3.3842 | PDE Loss:  -5.203 | Function Loss:  -3.4556\n",
      "Total loss:  -3.3845 | PDE Loss:  -5.202 | Function Loss:  -3.4562\n",
      "Total loss:  -3.3849 | PDE Loss:  -5.2019 | Function Loss:  -3.4567\n",
      "Total loss:  -3.3852 | PDE Loss:  -5.201 | Function Loss:  -3.4572\n",
      "Total loss:  -3.3854 | PDE Loss:  -5.2021 | Function Loss:  -3.4573\n",
      "Total loss:  -3.3856 | PDE Loss:  -5.2027 | Function Loss:  -3.4574\n",
      "Total loss:  -3.3858 | PDE Loss:  -5.2044 | Function Loss:  -3.4574\n",
      "Total loss:  -3.3861 | PDE Loss:  -5.2066 | Function Loss:  -3.4573\n",
      "Total loss:  -3.3865 | PDE Loss:  -5.2097 | Function Loss:  -3.4572\n",
      "Total loss:  -3.3864 | PDE Loss:  -5.21 | Function Loss:  -3.457\n",
      "Total loss:  -3.3867 | PDE Loss:  -5.2105 | Function Loss:  -3.4573\n",
      "Total loss:  -3.387 | PDE Loss:  -5.2124 | Function Loss:  -3.4573\n",
      "Total loss:  -3.3873 | PDE Loss:  -5.2141 | Function Loss:  -3.4573\n",
      "Total loss:  -3.3875 | PDE Loss:  -5.2147 | Function Loss:  -3.4575\n",
      "Total loss:  -3.3878 | PDE Loss:  -5.2149 | Function Loss:  -3.4578\n",
      "Total loss:  -3.3879 | PDE Loss:  -5.2139 | Function Loss:  -3.4581\n",
      "Total loss:  -3.3881 | PDE Loss:  -5.2129 | Function Loss:  -3.4585\n",
      "Total loss:  -3.3883 | PDE Loss:  -5.2113 | Function Loss:  -3.459\n",
      "Total loss:  -3.3884 | PDE Loss:  -5.2097 | Function Loss:  -3.4594\n",
      "Total loss:  -3.3886 | PDE Loss:  -5.2083 | Function Loss:  -3.4599\n",
      "Total loss:  -3.3887 | PDE Loss:  -5.2073 | Function Loss:  -3.4603\n",
      "Total loss:  -3.389 | PDE Loss:  -5.2072 | Function Loss:  -3.4605\n",
      "Total loss:  -3.3892 | PDE Loss:  -5.2078 | Function Loss:  -3.4607\n",
      "Total loss:  -3.3894 | PDE Loss:  -5.2096 | Function Loss:  -3.4606\n",
      "Total loss:  -3.3896 | PDE Loss:  -5.2115 | Function Loss:  -3.4606\n",
      "Total loss:  -3.3898 | PDE Loss:  -5.2141 | Function Loss:  -3.4603\n",
      "Total loss:  -3.3901 | PDE Loss:  -5.2168 | Function Loss:  -3.4602\n",
      "Total loss:  -3.3904 | PDE Loss:  -5.2193 | Function Loss:  -3.4601\n",
      "Total loss:  -3.3906 | PDE Loss:  -5.221 | Function Loss:  -3.4601\n",
      "Total loss:  -3.3909 | PDE Loss:  -5.222 | Function Loss:  -3.4603\n",
      "Total loss:  -3.3912 | PDE Loss:  -5.2222 | Function Loss:  -3.4605\n",
      "Total loss:  -3.3914 | PDE Loss:  -5.2217 | Function Loss:  -3.4609\n",
      "Total loss:  -3.3918 | PDE Loss:  -5.2206 | Function Loss:  -3.4615\n",
      "Total loss:  -3.3921 | PDE Loss:  -5.2186 | Function Loss:  -3.4622\n",
      "Total loss:  -3.3924 | PDE Loss:  -5.2169 | Function Loss:  -3.4628\n",
      "Total loss:  -3.3926 | PDE Loss:  -5.2158 | Function Loss:  -3.4632\n",
      "Total loss:  -3.3928 | PDE Loss:  -5.2153 | Function Loss:  -3.4636\n",
      "Total loss:  -3.393 | PDE Loss:  -5.2149 | Function Loss:  -3.464\n",
      "Total loss:  -3.3934 | PDE Loss:  -5.2153 | Function Loss:  -3.4643\n",
      "Total loss:  -3.3937 | PDE Loss:  -5.2147 | Function Loss:  -3.4648\n",
      "Total loss:  -3.3939 | PDE Loss:  -5.2149 | Function Loss:  -3.465\n",
      "Total loss:  -3.3943 | PDE Loss:  -5.2153 | Function Loss:  -3.4654\n",
      "Total loss:  -3.3948 | PDE Loss:  -5.2147 | Function Loss:  -3.466\n",
      "Total loss:  -3.3951 | PDE Loss:  -5.2142 | Function Loss:  -3.4665\n",
      "Total loss:  -3.3954 | PDE Loss:  -5.2135 | Function Loss:  -3.467\n",
      "Total loss:  -3.3957 | PDE Loss:  -5.2124 | Function Loss:  -3.4676\n",
      "Total loss:  -3.3962 | PDE Loss:  -5.2105 | Function Loss:  -3.4685\n",
      "Total loss:  -3.3965 | PDE Loss:  -5.2091 | Function Loss:  -3.4691\n",
      "Total loss:  -3.3969 | PDE Loss:  -5.2069 | Function Loss:  -3.47\n",
      "Total loss:  -3.3974 | PDE Loss:  -5.2049 | Function Loss:  -3.4709\n",
      "Total loss:  -3.3978 | PDE Loss:  -5.203 | Function Loss:  -3.4718\n",
      "Total loss:  -3.3982 | PDE Loss:  -5.2021 | Function Loss:  -3.4724\n",
      "Total loss:  -3.3986 | PDE Loss:  -5.1995 | Function Loss:  -3.4733\n",
      "Total loss:  -3.3988 | PDE Loss:  -5.2004 | Function Loss:  -3.4734\n",
      "Total loss:  -3.399 | PDE Loss:  -5.2009 | Function Loss:  -3.4736\n",
      "Total loss:  -3.3993 | PDE Loss:  -5.2019 | Function Loss:  -3.4737\n",
      "Total loss:  -3.3996 | PDE Loss:  -5.2032 | Function Loss:  -3.4738\n",
      "Total loss:  -3.3998 | PDE Loss:  -5.2045 | Function Loss:  -3.4739\n",
      "Total loss:  -3.4001 | PDE Loss:  -5.2053 | Function Loss:  -3.4741\n",
      "Total loss:  -3.4003 | PDE Loss:  -5.2061 | Function Loss:  -3.4742\n",
      "Total loss:  -3.4005 | PDE Loss:  -5.2069 | Function Loss:  -3.4743\n",
      "Total loss:  -3.4006 | PDE Loss:  -5.207 | Function Loss:  -3.4744\n",
      "Total loss:  -3.4007 | PDE Loss:  -5.2071 | Function Loss:  -3.4745\n",
      "Total loss:  -3.4008 | PDE Loss:  -5.2067 | Function Loss:  -3.4746\n",
      "Total loss:  -3.4009 | PDE Loss:  -5.2063 | Function Loss:  -3.4748\n",
      "Total loss:  -3.4011 | PDE Loss:  -5.2055 | Function Loss:  -3.4752\n",
      "Total loss:  -3.4012 | PDE Loss:  -5.2048 | Function Loss:  -3.4755\n",
      "Total loss:  -3.4013 | PDE Loss:  -5.2024 | Function Loss:  -3.4761\n",
      "Total loss:  -3.4015 | PDE Loss:  -5.2023 | Function Loss:  -3.4763\n",
      "Total loss:  -3.4016 | PDE Loss:  -5.2019 | Function Loss:  -3.4765\n",
      "Total loss:  -3.4018 | PDE Loss:  -5.2015 | Function Loss:  -3.4768\n",
      "Total loss:  -3.402 | PDE Loss:  -5.2018 | Function Loss:  -3.477\n",
      "Total loss:  -3.4022 | PDE Loss:  -5.202 | Function Loss:  -3.4771\n",
      "Total loss:  -3.4023 | PDE Loss:  -5.2024 | Function Loss:  -3.4773\n",
      "Total loss:  -3.4025 | PDE Loss:  -5.2032 | Function Loss:  -3.4773\n",
      "Total loss:  -3.4026 | PDE Loss:  -5.2033 | Function Loss:  -3.4775\n",
      "Total loss:  -3.4028 | PDE Loss:  -5.2036 | Function Loss:  -3.4776\n",
      "Total loss:  -3.403 | PDE Loss:  -5.203 | Function Loss:  -3.4779\n",
      "Total loss:  -3.4031 | PDE Loss:  -5.2029 | Function Loss:  -3.4781\n",
      "Total loss:  -3.4033 | PDE Loss:  -5.2009 | Function Loss:  -3.4787\n",
      "Total loss:  -3.4035 | PDE Loss:  -5.2007 | Function Loss:  -3.479\n",
      "Total loss:  -3.4037 | PDE Loss:  -5.1991 | Function Loss:  -3.4795\n",
      "Total loss:  -3.4039 | PDE Loss:  -5.1985 | Function Loss:  -3.4798\n",
      "Total loss:  -3.4041 | PDE Loss:  -5.1979 | Function Loss:  -3.4802\n",
      "Total loss:  -3.4043 | PDE Loss:  -5.1986 | Function Loss:  -3.4803\n",
      "Total loss:  -3.4045 | PDE Loss:  -5.1994 | Function Loss:  -3.4805\n",
      "Total loss:  -3.4048 | PDE Loss:  -5.202 | Function Loss:  -3.4803\n",
      "Total loss:  -3.405 | PDE Loss:  -5.2042 | Function Loss:  -3.48\n",
      "Total loss:  -3.4052 | PDE Loss:  -5.2069 | Function Loss:  -3.4798\n",
      "Total loss:  -3.4053 | PDE Loss:  -5.2097 | Function Loss:  -3.4795\n",
      "Total loss:  -3.4056 | PDE Loss:  -5.2124 | Function Loss:  -3.4793\n",
      "Total loss:  -3.4058 | PDE Loss:  -5.2147 | Function Loss:  -3.4791\n",
      "Total loss:  -3.406 | PDE Loss:  -5.2165 | Function Loss:  -3.479\n",
      "Total loss:  -3.4063 | PDE Loss:  -5.2185 | Function Loss:  -3.4789\n",
      "Total loss:  -3.4065 | PDE Loss:  -5.2193 | Function Loss:  -3.4791\n",
      "Total loss:  -3.4067 | PDE Loss:  -5.22 | Function Loss:  -3.4792\n",
      "Total loss:  -3.407 | PDE Loss:  -5.222 | Function Loss:  -3.4792\n",
      "Total loss:  -3.4073 | PDE Loss:  -5.2238 | Function Loss:  -3.4792\n",
      "Total loss:  -3.4076 | PDE Loss:  -5.2256 | Function Loss:  -3.4792\n",
      "Total loss:  -3.4078 | PDE Loss:  -5.2274 | Function Loss:  -3.4792\n",
      "Total loss:  -3.408 | PDE Loss:  -5.2286 | Function Loss:  -3.4792\n",
      "Total loss:  -3.4082 | PDE Loss:  -5.2302 | Function Loss:  -3.4791\n",
      "Total loss:  -3.4084 | PDE Loss:  -5.231 | Function Loss:  -3.4792\n",
      "Total loss:  -3.4087 | PDE Loss:  -5.2323 | Function Loss:  -3.4793\n",
      "Total loss:  -3.4089 | PDE Loss:  -5.2314 | Function Loss:  -3.4798\n",
      "Total loss:  -3.4092 | PDE Loss:  -5.231 | Function Loss:  -3.4802\n",
      "Total loss:  -3.4094 | PDE Loss:  -5.2309 | Function Loss:  -3.4804\n",
      "Total loss:  -3.4096 | PDE Loss:  -5.2292 | Function Loss:  -3.481\n",
      "Total loss:  -3.4098 | PDE Loss:  -5.228 | Function Loss:  -3.4814\n",
      "Total loss:  -3.41 | PDE Loss:  -5.2268 | Function Loss:  -3.4819\n",
      "Total loss:  -3.4102 | PDE Loss:  -5.2255 | Function Loss:  -3.4823\n",
      "Total loss:  -3.4104 | PDE Loss:  -5.2247 | Function Loss:  -3.4826\n",
      "Total loss:  -3.4105 | PDE Loss:  -5.2239 | Function Loss:  -3.483\n",
      "Total loss:  -3.4107 | PDE Loss:  -5.2234 | Function Loss:  -3.4833\n",
      "Total loss:  -3.4109 | PDE Loss:  -5.2236 | Function Loss:  -3.4835\n",
      "Total loss:  -3.4111 | PDE Loss:  -5.2227 | Function Loss:  -3.4839\n",
      "Total loss:  -3.4113 | PDE Loss:  -5.2226 | Function Loss:  -3.4841\n",
      "Total loss:  -3.4114 | PDE Loss:  -5.223 | Function Loss:  -3.4842\n",
      "Total loss:  -3.4115 | PDE Loss:  -5.2233 | Function Loss:  -3.4843\n",
      "Total loss:  -3.4116 | PDE Loss:  -5.2235 | Function Loss:  -3.4843\n",
      "Total loss:  -3.4117 | PDE Loss:  -5.2242 | Function Loss:  -3.4844\n",
      "Total loss:  -3.4119 | PDE Loss:  -5.224 | Function Loss:  -3.4846\n",
      "Total loss:  -3.412 | PDE Loss:  -5.2242 | Function Loss:  -3.4847\n",
      "Total loss:  -3.4122 | PDE Loss:  -5.2237 | Function Loss:  -3.485\n",
      "Total loss:  -3.4123 | PDE Loss:  -5.2231 | Function Loss:  -3.4852\n",
      "Total loss:  -3.4125 | PDE Loss:  -5.223 | Function Loss:  -3.4855\n",
      "Total loss:  -3.4127 | PDE Loss:  -5.2224 | Function Loss:  -3.4859\n",
      "Total loss:  -3.413 | PDE Loss:  -5.2229 | Function Loss:  -3.4861\n",
      "Total loss:  -3.4133 | PDE Loss:  -5.2244 | Function Loss:  -3.4861\n",
      "Total loss:  -3.4136 | PDE Loss:  -5.2271 | Function Loss:  -3.486\n",
      "Total loss:  -3.4138 | PDE Loss:  -5.2294 | Function Loss:  -3.4859\n",
      "Total loss:  -3.414 | PDE Loss:  -5.2333 | Function Loss:  -3.4854\n",
      "Total loss:  -3.4142 | PDE Loss:  -5.2365 | Function Loss:  -3.485\n",
      "Total loss:  -3.4143 | PDE Loss:  -5.2402 | Function Loss:  -3.4845\n",
      "Total loss:  -3.4144 | PDE Loss:  -5.2433 | Function Loss:  -3.4841\n",
      "Total loss:  -3.4146 | PDE Loss:  -5.2476 | Function Loss:  -3.4836\n",
      "Total loss:  -3.4147 | PDE Loss:  -5.2496 | Function Loss:  -3.4833\n",
      "Total loss:  -3.4148 | PDE Loss:  -5.2511 | Function Loss:  -3.4833\n",
      "Total loss:  -3.415 | PDE Loss:  -5.2523 | Function Loss:  -3.4833\n",
      "Total loss:  -3.4152 | PDE Loss:  -5.2526 | Function Loss:  -3.4835\n",
      "Total loss:  -3.4155 | PDE Loss:  -5.2536 | Function Loss:  -3.4836\n",
      "Total loss:  -3.4157 | PDE Loss:  -5.2533 | Function Loss:  -3.4839\n",
      "Total loss:  -3.4159 | PDE Loss:  -5.2529 | Function Loss:  -3.4842\n",
      "Total loss:  -3.4162 | PDE Loss:  -5.2524 | Function Loss:  -3.4846\n",
      "Total loss:  -3.4164 | PDE Loss:  -5.2516 | Function Loss:  -3.4851\n",
      "Total loss:  -3.4166 | PDE Loss:  -5.2508 | Function Loss:  -3.4854\n",
      "Total loss:  -3.4168 | PDE Loss:  -5.2505 | Function Loss:  -3.4857\n",
      "Total loss:  -3.417 | PDE Loss:  -5.2497 | Function Loss:  -3.4861\n",
      "Total loss:  -3.4173 | PDE Loss:  -5.2501 | Function Loss:  -3.4864\n",
      "Total loss:  -3.4176 | PDE Loss:  -5.2493 | Function Loss:  -3.4868\n",
      "Total loss:  -3.4179 | PDE Loss:  -5.2505 | Function Loss:  -3.487\n",
      "Total loss:  -3.4181 | PDE Loss:  -5.2505 | Function Loss:  -3.4873\n",
      "Total loss:  -3.4183 | PDE Loss:  -5.2514 | Function Loss:  -3.4873\n",
      "Total loss:  -3.4185 | PDE Loss:  -5.2522 | Function Loss:  -3.4873\n",
      "Total loss:  -3.4186 | PDE Loss:  -5.2531 | Function Loss:  -3.4874\n",
      "Total loss:  -3.4188 | PDE Loss:  -5.2542 | Function Loss:  -3.4874\n",
      "Total loss:  -3.4191 | PDE Loss:  -5.2563 | Function Loss:  -3.4873\n",
      "Total loss:  -3.4193 | PDE Loss:  -5.2563 | Function Loss:  -3.4876\n",
      "Total loss:  -3.4196 | PDE Loss:  -5.2579 | Function Loss:  -3.4877\n",
      "Total loss:  -3.4199 | PDE Loss:  -5.2595 | Function Loss:  -3.4878\n",
      "Total loss:  -3.4203 | PDE Loss:  -5.2596 | Function Loss:  -3.4882\n",
      "Total loss:  -3.4205 | PDE Loss:  -5.2587 | Function Loss:  -3.4887\n",
      "Total loss:  -3.4207 | PDE Loss:  -5.2581 | Function Loss:  -3.489\n",
      "Total loss:  -3.4209 | PDE Loss:  -5.2569 | Function Loss:  -3.4893\n",
      "Total loss:  -3.4211 | PDE Loss:  -5.2555 | Function Loss:  -3.4899\n",
      "Total loss:  -3.4214 | PDE Loss:  -5.254 | Function Loss:  -3.4905\n",
      "Total loss:  -3.4217 | PDE Loss:  -5.2521 | Function Loss:  -3.4911\n",
      "Total loss:  -3.422 | PDE Loss:  -5.2508 | Function Loss:  -3.4917\n",
      "Total loss:  -3.4223 | PDE Loss:  -5.2495 | Function Loss:  -3.4923\n",
      "Total loss:  -3.4225 | PDE Loss:  -5.249 | Function Loss:  -3.4927\n",
      "Total loss:  -3.4229 | PDE Loss:  -5.2484 | Function Loss:  -3.4931\n",
      "Total loss:  -3.4232 | PDE Loss:  -5.2487 | Function Loss:  -3.4935\n",
      "Total loss:  -3.4236 | PDE Loss:  -5.2488 | Function Loss:  -3.494\n",
      "Total loss:  -3.424 | PDE Loss:  -5.2493 | Function Loss:  -3.4943\n",
      "Total loss:  -3.4243 | PDE Loss:  -5.2499 | Function Loss:  -3.4946\n",
      "Total loss:  -3.4235 | PDE Loss:  -5.2421 | Function Loss:  -3.495\n",
      "Total loss:  -3.4245 | PDE Loss:  -5.2486 | Function Loss:  -3.495\n",
      "Total loss:  -3.4247 | PDE Loss:  -5.2495 | Function Loss:  -3.4951\n",
      "Total loss:  -3.4249 | PDE Loss:  -5.2501 | Function Loss:  -3.4953\n",
      "Total loss:  -3.4252 | PDE Loss:  -5.2505 | Function Loss:  -3.4956\n",
      "Total loss:  -3.4255 | PDE Loss:  -5.2511 | Function Loss:  -3.4958\n",
      "Total loss:  -3.4258 | PDE Loss:  -5.2507 | Function Loss:  -3.4962\n",
      "Total loss:  -3.4261 | PDE Loss:  -5.2507 | Function Loss:  -3.4965\n",
      "Total loss:  -3.4264 | PDE Loss:  -5.2499 | Function Loss:  -3.497\n",
      "Total loss:  -3.4266 | PDE Loss:  -5.249 | Function Loss:  -3.4975\n",
      "Total loss:  -3.427 | PDE Loss:  -5.2473 | Function Loss:  -3.4982\n",
      "Total loss:  -3.4273 | PDE Loss:  -5.2458 | Function Loss:  -3.4989\n",
      "Total loss:  -3.4277 | PDE Loss:  -5.2436 | Function Loss:  -3.4997\n",
      "Total loss:  -3.428 | PDE Loss:  -5.2423 | Function Loss:  -3.5004\n",
      "Total loss:  -3.4284 | PDE Loss:  -5.2399 | Function Loss:  -3.5012\n",
      "Total loss:  -3.4287 | PDE Loss:  -5.24 | Function Loss:  -3.5015\n",
      "Total loss:  -3.4289 | PDE Loss:  -5.241 | Function Loss:  -3.5017\n",
      "Total loss:  -3.4293 | PDE Loss:  -5.2426 | Function Loss:  -3.5017\n",
      "Total loss:  -3.4295 | PDE Loss:  -5.2445 | Function Loss:  -3.5017\n",
      "Total loss:  -3.4298 | PDE Loss:  -5.2461 | Function Loss:  -3.5017\n",
      "Total loss:  -3.43 | PDE Loss:  -5.2479 | Function Loss:  -3.5016\n",
      "Total loss:  -3.4302 | PDE Loss:  -5.2497 | Function Loss:  -3.5016\n",
      "Total loss:  -3.4304 | PDE Loss:  -5.2521 | Function Loss:  -3.5014\n",
      "Total loss:  -3.4307 | PDE Loss:  -5.2543 | Function Loss:  -3.5013\n",
      "Total loss:  -3.4309 | PDE Loss:  -5.258 | Function Loss:  -3.501\n",
      "Total loss:  -3.4312 | PDE Loss:  -5.2603 | Function Loss:  -3.5009\n",
      "Total loss:  -3.4315 | PDE Loss:  -5.2641 | Function Loss:  -3.5005\n",
      "Total loss:  -3.4317 | PDE Loss:  -5.266 | Function Loss:  -3.5005\n",
      "Total loss:  -3.4319 | PDE Loss:  -5.2677 | Function Loss:  -3.5005\n",
      "Total loss:  -3.4321 | PDE Loss:  -5.2688 | Function Loss:  -3.5005\n",
      "Total loss:  -3.4323 | PDE Loss:  -5.2686 | Function Loss:  -3.5008\n",
      "Total loss:  -3.4326 | PDE Loss:  -5.2683 | Function Loss:  -3.5011\n",
      "Total loss:  -3.4328 | PDE Loss:  -5.2668 | Function Loss:  -3.5016\n",
      "Total loss:  -3.4331 | PDE Loss:  -5.2657 | Function Loss:  -3.5022\n",
      "Total loss:  -3.4335 | PDE Loss:  -5.2632 | Function Loss:  -3.503\n",
      "Total loss:  -3.4337 | PDE Loss:  -5.2604 | Function Loss:  -3.5038\n",
      "Total loss:  -3.434 | PDE Loss:  -5.2588 | Function Loss:  -3.5045\n",
      "Total loss:  -3.4343 | PDE Loss:  -5.258 | Function Loss:  -3.505\n",
      "Total loss:  -3.4346 | PDE Loss:  -5.2573 | Function Loss:  -3.5054\n",
      "Total loss:  -3.4348 | PDE Loss:  -5.2574 | Function Loss:  -3.5056\n",
      "Total loss:  -3.435 | PDE Loss:  -5.2583 | Function Loss:  -3.5057\n",
      "Total loss:  -3.4353 | PDE Loss:  -5.2595 | Function Loss:  -3.5058\n",
      "Total loss:  -3.4356 | PDE Loss:  -5.2607 | Function Loss:  -3.506\n",
      "Total loss:  -3.4359 | PDE Loss:  -5.2619 | Function Loss:  -3.5061\n",
      "Total loss:  -3.4361 | PDE Loss:  -5.2621 | Function Loss:  -3.5064\n",
      "Total loss:  -3.4364 | PDE Loss:  -5.263 | Function Loss:  -3.5065\n",
      "Total loss:  -3.4366 | PDE Loss:  -5.263 | Function Loss:  -3.5068\n",
      "Total loss:  -3.4369 | PDE Loss:  -5.2631 | Function Loss:  -3.5071\n",
      "Total loss:  -3.4373 | PDE Loss:  -5.263 | Function Loss:  -3.5075\n",
      "Total loss:  -3.4377 | PDE Loss:  -5.2638 | Function Loss:  -3.5079\n",
      "Total loss:  -3.4382 | PDE Loss:  -5.2636 | Function Loss:  -3.5085\n",
      "Total loss:  -3.4386 | PDE Loss:  -5.2647 | Function Loss:  -3.5088\n",
      "Total loss:  -3.4389 | PDE Loss:  -5.2658 | Function Loss:  -3.509\n",
      "Total loss:  -3.4393 | PDE Loss:  -5.2664 | Function Loss:  -3.5093\n",
      "Total loss:  -3.4396 | PDE Loss:  -5.267 | Function Loss:  -3.5096\n",
      "Total loss:  -3.4399 | PDE Loss:  -5.2674 | Function Loss:  -3.5098\n",
      "Total loss:  -3.4402 | PDE Loss:  -5.2679 | Function Loss:  -3.5101\n",
      "Total loss:  -3.4406 | PDE Loss:  -5.2682 | Function Loss:  -3.5105\n",
      "Total loss:  -3.4409 | PDE Loss:  -5.2695 | Function Loss:  -3.5106\n",
      "Total loss:  -3.4411 | PDE Loss:  -5.2698 | Function Loss:  -3.5108\n",
      "Total loss:  -3.4413 | PDE Loss:  -5.2707 | Function Loss:  -3.5109\n",
      "Total loss:  -3.4415 | PDE Loss:  -5.2712 | Function Loss:  -3.511\n",
      "Total loss:  -3.4416 | PDE Loss:  -5.2716 | Function Loss:  -3.5111\n",
      "Total loss:  -3.4417 | PDE Loss:  -5.2725 | Function Loss:  -3.5111\n",
      "Total loss:  -3.4418 | PDE Loss:  -5.2728 | Function Loss:  -3.5111\n",
      "Total loss:  -3.4419 | PDE Loss:  -5.2734 | Function Loss:  -3.5112\n",
      "Total loss:  -3.4421 | PDE Loss:  -5.2734 | Function Loss:  -3.5114\n",
      "Total loss:  -3.4423 | PDE Loss:  -5.2736 | Function Loss:  -3.5116\n",
      "Total loss:  -3.4425 | PDE Loss:  -5.2731 | Function Loss:  -3.5119\n",
      "Total loss:  -3.4426 | PDE Loss:  -5.2722 | Function Loss:  -3.5122\n",
      "Total loss:  -3.4427 | PDE Loss:  -5.2714 | Function Loss:  -3.5125\n",
      "Total loss:  -3.4429 | PDE Loss:  -5.2693 | Function Loss:  -3.5131\n",
      "Total loss:  -3.4432 | PDE Loss:  -5.2675 | Function Loss:  -3.5137\n",
      "Total loss:  -3.4434 | PDE Loss:  -5.265 | Function Loss:  -3.5144\n",
      "Total loss:  -3.4436 | PDE Loss:  -5.2622 | Function Loss:  -3.5151\n",
      "Total loss:  -3.4437 | PDE Loss:  -5.2599 | Function Loss:  -3.5157\n",
      "Total loss:  -3.4438 | PDE Loss:  -5.2583 | Function Loss:  -3.5161\n",
      "Total loss:  -3.444 | PDE Loss:  -5.2562 | Function Loss:  -3.5167\n",
      "Total loss:  -3.4442 | PDE Loss:  -5.2545 | Function Loss:  -3.5173\n",
      "Total loss:  -3.4444 | PDE Loss:  -5.253 | Function Loss:  -3.5177\n",
      "Total loss:  -3.4446 | PDE Loss:  -5.2508 | Function Loss:  -3.5184\n",
      "Total loss:  -3.4447 | PDE Loss:  -5.2507 | Function Loss:  -3.5186\n",
      "Total loss:  -3.4449 | PDE Loss:  -5.25 | Function Loss:  -3.5189\n",
      "Total loss:  -3.445 | PDE Loss:  -5.2495 | Function Loss:  -3.5191\n",
      "Total loss:  -3.4452 | PDE Loss:  -5.249 | Function Loss:  -3.5194\n",
      "Total loss:  -3.4453 | PDE Loss:  -5.2485 | Function Loss:  -3.5197\n",
      "Total loss:  -3.4455 | PDE Loss:  -5.2474 | Function Loss:  -3.5201\n",
      "Total loss:  -3.4457 | PDE Loss:  -5.2459 | Function Loss:  -3.5206\n",
      "Total loss:  -3.4459 | PDE Loss:  -5.2445 | Function Loss:  -3.5211\n",
      "Total loss:  -3.446 | PDE Loss:  -5.2429 | Function Loss:  -3.5216\n",
      "Total loss:  -3.4462 | PDE Loss:  -5.2418 | Function Loss:  -3.522\n",
      "Total loss:  -3.4464 | PDE Loss:  -5.2401 | Function Loss:  -3.5225\n",
      "Total loss:  -3.4466 | PDE Loss:  -5.2396 | Function Loss:  -3.5229\n",
      "Total loss:  -3.4468 | PDE Loss:  -5.2383 | Function Loss:  -3.5234\n",
      "Total loss:  -3.4471 | PDE Loss:  -5.2385 | Function Loss:  -3.5236\n",
      "Total loss:  -3.4473 | PDE Loss:  -5.2386 | Function Loss:  -3.5239\n",
      "Total loss:  -3.4476 | PDE Loss:  -5.2381 | Function Loss:  -3.5244\n",
      "Total loss:  -3.4478 | PDE Loss:  -5.2387 | Function Loss:  -3.5244\n",
      "Total loss:  -3.4479 | PDE Loss:  -5.2391 | Function Loss:  -3.5246\n",
      "Total loss:  -3.4481 | PDE Loss:  -5.2391 | Function Loss:  -3.5247\n",
      "Total loss:  -3.4483 | PDE Loss:  -5.2388 | Function Loss:  -3.525\n",
      "Total loss:  -3.4485 | PDE Loss:  -5.2379 | Function Loss:  -3.5254\n",
      "Total loss:  -3.4487 | PDE Loss:  -5.237 | Function Loss:  -3.5258\n",
      "Total loss:  -3.4489 | PDE Loss:  -5.236 | Function Loss:  -3.5263\n",
      "Total loss:  -3.4491 | PDE Loss:  -5.2353 | Function Loss:  -3.5267\n",
      "Total loss:  -3.4493 | PDE Loss:  -5.2351 | Function Loss:  -3.527\n",
      "Total loss:  -3.4495 | PDE Loss:  -5.2346 | Function Loss:  -3.5274\n",
      "Total loss:  -3.4498 | PDE Loss:  -5.2358 | Function Loss:  -3.5274\n",
      "Total loss:  -3.45 | PDE Loss:  -5.236 | Function Loss:  -3.5277\n",
      "Total loss:  -3.4503 | PDE Loss:  -5.2375 | Function Loss:  -3.5277\n",
      "Total loss:  -3.4506 | PDE Loss:  -5.2393 | Function Loss:  -3.5276\n",
      "Total loss:  -3.4509 | PDE Loss:  -5.2403 | Function Loss:  -3.5278\n",
      "Total loss:  -3.4511 | PDE Loss:  -5.2422 | Function Loss:  -3.5278\n",
      "Total loss:  -3.4514 | PDE Loss:  -5.243 | Function Loss:  -3.528\n",
      "Total loss:  -3.4517 | PDE Loss:  -5.2439 | Function Loss:  -3.5281\n",
      "Total loss:  -3.4519 | PDE Loss:  -5.2439 | Function Loss:  -3.5284\n",
      "Total loss:  -3.4521 | PDE Loss:  -5.2443 | Function Loss:  -3.5285\n",
      "Total loss:  -3.4522 | PDE Loss:  -5.2436 | Function Loss:  -3.5288\n",
      "Total loss:  -3.4523 | PDE Loss:  -5.2432 | Function Loss:  -3.529\n",
      "Total loss:  -3.4525 | PDE Loss:  -5.2426 | Function Loss:  -3.5293\n",
      "Total loss:  -3.4527 | PDE Loss:  -5.2415 | Function Loss:  -3.5298\n",
      "Total loss:  -3.4529 | PDE Loss:  -5.2409 | Function Loss:  -3.5302\n",
      "Total loss:  -3.4531 | PDE Loss:  -5.2402 | Function Loss:  -3.5305\n",
      "Total loss:  -3.4532 | PDE Loss:  -5.2396 | Function Loss:  -3.5308\n",
      "Total loss:  -3.4534 | PDE Loss:  -5.2394 | Function Loss:  -3.531\n",
      "Total loss:  -3.4535 | PDE Loss:  -5.2387 | Function Loss:  -3.5313\n",
      "Total loss:  -3.4536 | PDE Loss:  -5.238 | Function Loss:  -3.5316\n",
      "Total loss:  -3.4538 | PDE Loss:  -5.2364 | Function Loss:  -3.532\n",
      "Total loss:  -3.4539 | PDE Loss:  -5.2334 | Function Loss:  -3.5328\n",
      "Total loss:  -3.454 | PDE Loss:  -5.2326 | Function Loss:  -3.5331\n",
      "Total loss:  -3.4542 | PDE Loss:  -5.2312 | Function Loss:  -3.5336\n",
      "Total loss:  -3.4544 | PDE Loss:  -5.2299 | Function Loss:  -3.5341\n",
      "Total loss:  -3.4547 | PDE Loss:  -5.2285 | Function Loss:  -3.5347\n",
      "Total loss:  -3.4549 | PDE Loss:  -5.2279 | Function Loss:  -3.5351\n",
      "Total loss:  -3.4551 | PDE Loss:  -5.2276 | Function Loss:  -3.5354\n",
      "Total loss:  -3.4553 | PDE Loss:  -5.2276 | Function Loss:  -3.5357\n",
      "Total loss:  -3.4556 | PDE Loss:  -5.2284 | Function Loss:  -3.5358\n",
      "Total loss:  -3.4557 | PDE Loss:  -5.2296 | Function Loss:  -3.5358\n",
      "Total loss:  -3.4559 | PDE Loss:  -5.2304 | Function Loss:  -3.5358\n",
      "Total loss:  -3.456 | PDE Loss:  -5.2316 | Function Loss:  -3.5358\n",
      "Total loss:  -3.4561 | PDE Loss:  -5.2316 | Function Loss:  -3.5359\n",
      "Total loss:  -3.4562 | PDE Loss:  -5.2322 | Function Loss:  -3.5359\n",
      "Total loss:  -3.4563 | PDE Loss:  -5.2323 | Function Loss:  -3.5359\n",
      "Total loss:  -3.4564 | PDE Loss:  -5.2326 | Function Loss:  -3.536\n",
      "Total loss:  -3.4565 | PDE Loss:  -5.233 | Function Loss:  -3.5361\n",
      "Total loss:  -3.4567 | PDE Loss:  -5.2335 | Function Loss:  -3.5361\n",
      "Total loss:  -3.4568 | PDE Loss:  -5.2344 | Function Loss:  -3.5361\n",
      "Total loss:  -3.457 | PDE Loss:  -5.2352 | Function Loss:  -3.5362\n",
      "Total loss:  -3.4572 | PDE Loss:  -5.2367 | Function Loss:  -3.5361\n",
      "Total loss:  -3.4574 | PDE Loss:  -5.2377 | Function Loss:  -3.5361\n",
      "Total loss:  -3.4575 | PDE Loss:  -5.239 | Function Loss:  -3.5361\n",
      "Total loss:  -3.4577 | PDE Loss:  -5.2399 | Function Loss:  -3.5361\n",
      "Total loss:  -3.4578 | PDE Loss:  -5.2409 | Function Loss:  -3.536\n",
      "Total loss:  -3.4579 | PDE Loss:  -5.2417 | Function Loss:  -3.536\n",
      "Total loss:  -3.458 | PDE Loss:  -5.2423 | Function Loss:  -3.536\n",
      "Total loss:  -3.4582 | PDE Loss:  -5.2431 | Function Loss:  -3.536\n",
      "Total loss:  -3.4583 | PDE Loss:  -5.2436 | Function Loss:  -3.536\n",
      "Total loss:  -3.4584 | PDE Loss:  -5.2449 | Function Loss:  -3.536\n",
      "Total loss:  -3.4585 | PDE Loss:  -5.2464 | Function Loss:  -3.5358\n",
      "Total loss:  -3.4587 | PDE Loss:  -5.2466 | Function Loss:  -3.536\n",
      "Total loss:  -3.4589 | PDE Loss:  -5.2479 | Function Loss:  -3.5359\n",
      "Total loss:  -3.459 | PDE Loss:  -5.2485 | Function Loss:  -3.536\n",
      "Total loss:  -3.4592 | PDE Loss:  -5.2504 | Function Loss:  -3.5358\n",
      "Total loss:  -3.4593 | PDE Loss:  -5.25 | Function Loss:  -3.536\n",
      "Total loss:  -3.4594 | PDE Loss:  -5.2528 | Function Loss:  -3.5356\n",
      "Total loss:  -3.4595 | PDE Loss:  -5.254 | Function Loss:  -3.5355\n",
      "Total loss:  -3.4597 | PDE Loss:  -5.2552 | Function Loss:  -3.5355\n",
      "Total loss:  -3.4599 | PDE Loss:  -5.2562 | Function Loss:  -3.5355\n",
      "Total loss:  -3.46 | PDE Loss:  -5.2564 | Function Loss:  -3.5357\n",
      "Total loss:  -3.4602 | PDE Loss:  -5.2562 | Function Loss:  -3.5359\n",
      "Total loss:  -3.4604 | PDE Loss:  -5.2568 | Function Loss:  -3.536\n",
      "Total loss:  -3.4605 | PDE Loss:  -5.2561 | Function Loss:  -3.5363\n",
      "Total loss:  -3.4607 | PDE Loss:  -5.2573 | Function Loss:  -3.5362\n",
      "Total loss:  -3.4609 | PDE Loss:  -5.2561 | Function Loss:  -3.5367\n",
      "Total loss:  -3.461 | PDE Loss:  -5.2566 | Function Loss:  -3.5368\n",
      "Total loss:  -3.4613 | PDE Loss:  -5.2569 | Function Loss:  -3.537\n",
      "Total loss:  -3.4615 | PDE Loss:  -5.257 | Function Loss:  -3.5373\n",
      "Total loss:  -3.4618 | PDE Loss:  -5.258 | Function Loss:  -3.5375\n",
      "Total loss:  -3.4622 | PDE Loss:  -5.2584 | Function Loss:  -3.5378\n",
      "Total loss:  -3.4624 | PDE Loss:  -5.2592 | Function Loss:  -3.5379\n",
      "Total loss:  -3.4627 | PDE Loss:  -5.2607 | Function Loss:  -3.538\n",
      "Total loss:  -3.4632 | PDE Loss:  -5.2638 | Function Loss:  -3.538\n",
      "Total loss:  -3.4635 | PDE Loss:  -5.2675 | Function Loss:  -3.5377\n",
      "Total loss:  -3.4637 | PDE Loss:  -5.27 | Function Loss:  -3.5375\n",
      "Total loss:  -3.4639 | PDE Loss:  -5.2728 | Function Loss:  -3.5372\n",
      "Total loss:  -3.4642 | PDE Loss:  -5.2765 | Function Loss:  -3.5369\n",
      "Total loss:  -3.4646 | PDE Loss:  -5.2804 | Function Loss:  -3.5366\n",
      "Total loss:  -3.4649 | PDE Loss:  -5.2856 | Function Loss:  -3.5361\n",
      "Total loss:  -3.4653 | PDE Loss:  -5.2881 | Function Loss:  -3.536\n",
      "Total loss:  -3.4657 | PDE Loss:  -5.2905 | Function Loss:  -3.5361\n",
      "Total loss:  -3.466 | PDE Loss:  -5.2918 | Function Loss:  -3.5362\n",
      "Total loss:  -3.4662 | PDE Loss:  -5.2923 | Function Loss:  -3.5364\n",
      "Total loss:  -3.4663 | PDE Loss:  -5.2923 | Function Loss:  -3.5365\n",
      "Total loss:  -3.4664 | PDE Loss:  -5.2927 | Function Loss:  -3.5366\n",
      "Total loss:  -3.4666 | PDE Loss:  -5.293 | Function Loss:  -3.5368\n",
      "Total loss:  -3.4668 | PDE Loss:  -5.2931 | Function Loss:  -3.537\n",
      "Total loss:  -3.467 | PDE Loss:  -5.2932 | Function Loss:  -3.5372\n",
      "Total loss:  -3.4673 | PDE Loss:  -5.2931 | Function Loss:  -3.5376\n",
      "Total loss:  -3.4676 | PDE Loss:  -5.2929 | Function Loss:  -3.5379\n",
      "Total loss:  -3.4678 | PDE Loss:  -5.2924 | Function Loss:  -3.5383\n",
      "Total loss:  -3.468 | PDE Loss:  -5.292 | Function Loss:  -3.5386\n",
      "Total loss:  -3.4682 | PDE Loss:  -5.2914 | Function Loss:  -3.5389\n",
      "Total loss:  -3.4684 | PDE Loss:  -5.2909 | Function Loss:  -3.5392\n",
      "Total loss:  -3.4686 | PDE Loss:  -5.2907 | Function Loss:  -3.5395\n",
      "Total loss:  -3.4687 | PDE Loss:  -5.291 | Function Loss:  -3.5396\n",
      "Total loss:  -3.469 | PDE Loss:  -5.2907 | Function Loss:  -3.5399\n",
      "Total loss:  -3.4692 | PDE Loss:  -5.2921 | Function Loss:  -3.54\n",
      "Total loss:  -3.4696 | PDE Loss:  -5.2927 | Function Loss:  -3.5403\n",
      "Total loss:  -3.4699 | PDE Loss:  -5.2941 | Function Loss:  -3.5404\n",
      "Total loss:  -3.4702 | PDE Loss:  -5.2945 | Function Loss:  -3.5408\n",
      "Total loss:  -3.4706 | PDE Loss:  -5.2955 | Function Loss:  -3.541\n",
      "Total loss:  -3.471 | PDE Loss:  -5.2959 | Function Loss:  -3.5414\n",
      "Total loss:  -3.4713 | PDE Loss:  -5.296 | Function Loss:  -3.5417\n",
      "Total loss:  -3.4716 | PDE Loss:  -5.2954 | Function Loss:  -3.5422\n",
      "Total loss:  -3.4719 | PDE Loss:  -5.2954 | Function Loss:  -3.5426\n",
      "Total loss:  -3.4722 | PDE Loss:  -5.2945 | Function Loss:  -3.5431\n",
      "Total loss:  -3.4726 | PDE Loss:  -5.2945 | Function Loss:  -3.5435\n",
      "Total loss:  -3.4729 | PDE Loss:  -5.294 | Function Loss:  -3.544\n",
      "Total loss:  -3.4732 | PDE Loss:  -5.2941 | Function Loss:  -3.5444\n",
      "Total loss:  -3.4736 | PDE Loss:  -5.295 | Function Loss:  -3.5446\n",
      "Total loss:  -3.4738 | PDE Loss:  -5.2959 | Function Loss:  -3.5447\n",
      "Total loss:  -3.4739 | PDE Loss:  -5.297 | Function Loss:  -3.5446\n",
      "Total loss:  -3.474 | PDE Loss:  -5.2986 | Function Loss:  -3.5445\n",
      "Total loss:  -3.4742 | PDE Loss:  -5.2998 | Function Loss:  -3.5445\n",
      "Total loss:  -3.4743 | PDE Loss:  -5.3017 | Function Loss:  -3.5443\n",
      "Total loss:  -3.4745 | PDE Loss:  -5.3033 | Function Loss:  -3.5443\n",
      "Total loss:  -3.4745 | PDE Loss:  -5.3133 | Function Loss:  -3.5425\n",
      "Total loss:  -3.4747 | PDE Loss:  -5.3082 | Function Loss:  -3.5436\n",
      "Total loss:  -3.4748 | PDE Loss:  -5.3087 | Function Loss:  -3.5437\n",
      "Total loss:  -3.475 | PDE Loss:  -5.3089 | Function Loss:  -3.5438\n",
      "Total loss:  -3.4751 | PDE Loss:  -5.3085 | Function Loss:  -3.544\n",
      "Total loss:  -3.4752 | PDE Loss:  -5.3082 | Function Loss:  -3.5442\n",
      "Total loss:  -3.4752 | PDE Loss:  -5.3082 | Function Loss:  -3.5443\n",
      "Total loss:  -3.4753 | PDE Loss:  -5.3082 | Function Loss:  -3.5444\n",
      "Total loss:  -3.4754 | PDE Loss:  -5.3085 | Function Loss:  -3.5444\n",
      "Total loss:  -3.4755 | PDE Loss:  -5.3086 | Function Loss:  -3.5445\n",
      "Total loss:  -3.4756 | PDE Loss:  -5.3094 | Function Loss:  -3.5445\n",
      "Total loss:  -3.4757 | PDE Loss:  -5.3102 | Function Loss:  -3.5445\n",
      "Total loss:  -3.4758 | PDE Loss:  -5.3108 | Function Loss:  -3.5445\n",
      "Total loss:  -3.476 | PDE Loss:  -5.3123 | Function Loss:  -3.5444\n",
      "Total loss:  -3.4761 | PDE Loss:  -5.3128 | Function Loss:  -3.5445\n",
      "Total loss:  -3.4763 | PDE Loss:  -5.3135 | Function Loss:  -3.5446\n",
      "Total loss:  -3.4764 | PDE Loss:  -5.3125 | Function Loss:  -3.5449\n",
      "Total loss:  -3.4766 | PDE Loss:  -5.3136 | Function Loss:  -3.5449\n",
      "Total loss:  -3.4767 | PDE Loss:  -5.314 | Function Loss:  -3.545\n",
      "Total loss:  -3.4769 | PDE Loss:  -5.3145 | Function Loss:  -3.5451\n",
      "Total loss:  -3.4771 | PDE Loss:  -5.315 | Function Loss:  -3.5452\n",
      "Total loss:  -3.4772 | PDE Loss:  -5.3158 | Function Loss:  -3.5453\n",
      "Total loss:  -3.4775 | PDE Loss:  -5.3168 | Function Loss:  -3.5454\n",
      "Total loss:  -3.4777 | PDE Loss:  -5.3178 | Function Loss:  -3.5454\n",
      "Total loss:  -3.4778 | PDE Loss:  -5.3188 | Function Loss:  -3.5455\n",
      "Total loss:  -3.4781 | PDE Loss:  -5.32 | Function Loss:  -3.5456\n",
      "Total loss:  -3.4785 | PDE Loss:  -5.3215 | Function Loss:  -3.5458\n",
      "Total loss:  -3.4789 | PDE Loss:  -5.3215 | Function Loss:  -3.5462\n",
      "Total loss:  -3.4793 | PDE Loss:  -5.3203 | Function Loss:  -3.5469\n",
      "Total loss:  -3.4797 | PDE Loss:  -5.317 | Function Loss:  -3.548\n",
      "Total loss:  -3.4801 | PDE Loss:  -5.314 | Function Loss:  -3.549\n",
      "Total loss:  -3.4805 | PDE Loss:  -5.3091 | Function Loss:  -3.5502\n",
      "Total loss:  -3.4808 | PDE Loss:  -5.3061 | Function Loss:  -3.5511\n",
      "Total loss:  -3.481 | PDE Loss:  -5.3037 | Function Loss:  -3.5518\n",
      "Total loss:  -3.4812 | PDE Loss:  -5.3025 | Function Loss:  -3.5522\n",
      "Total loss:  -3.4814 | PDE Loss:  -5.3016 | Function Loss:  -3.5526\n",
      "Total loss:  -3.4815 | PDE Loss:  -5.3015 | Function Loss:  -3.5528\n",
      "Total loss:  -3.4817 | PDE Loss:  -5.3015 | Function Loss:  -3.553\n",
      "Total loss:  -3.4819 | PDE Loss:  -5.3024 | Function Loss:  -3.5531\n",
      "Total loss:  -3.4821 | PDE Loss:  -5.3029 | Function Loss:  -3.5532\n",
      "Total loss:  -3.4823 | PDE Loss:  -5.3039 | Function Loss:  -3.5532\n",
      "Total loss:  -3.4824 | PDE Loss:  -5.3044 | Function Loss:  -3.5533\n",
      "Total loss:  -3.4826 | PDE Loss:  -5.3052 | Function Loss:  -3.5534\n",
      "Total loss:  -3.4828 | PDE Loss:  -5.3055 | Function Loss:  -3.5536\n",
      "Total loss:  -3.483 | PDE Loss:  -5.3044 | Function Loss:  -3.554\n",
      "Total loss:  -3.4831 | PDE Loss:  -5.3047 | Function Loss:  -3.5541\n",
      "Total loss:  -3.4833 | PDE Loss:  -5.3046 | Function Loss:  -3.5543\n",
      "Total loss:  -3.4836 | PDE Loss:  -5.3037 | Function Loss:  -3.5548\n",
      "Total loss:  -3.4838 | PDE Loss:  -5.3032 | Function Loss:  -3.5552\n",
      "Total loss:  -3.484 | PDE Loss:  -5.3028 | Function Loss:  -3.5555\n",
      "Total loss:  -3.4842 | PDE Loss:  -5.3033 | Function Loss:  -3.5556\n",
      "Total loss:  -3.4844 | PDE Loss:  -5.3031 | Function Loss:  -3.5559\n",
      "Total loss:  -3.4846 | PDE Loss:  -5.3042 | Function Loss:  -3.5559\n",
      "Total loss:  -3.4847 | PDE Loss:  -5.306 | Function Loss:  -3.5558\n",
      "Total loss:  -3.4849 | PDE Loss:  -5.3076 | Function Loss:  -3.5557\n",
      "Total loss:  -3.4851 | PDE Loss:  -5.3091 | Function Loss:  -3.5556\n",
      "Total loss:  -3.4852 | PDE Loss:  -5.3099 | Function Loss:  -3.5556\n",
      "Total loss:  -3.4853 | PDE Loss:  -5.3102 | Function Loss:  -3.5557\n",
      "Total loss:  -3.4854 | PDE Loss:  -5.3102 | Function Loss:  -3.5558\n",
      "Total loss:  -3.4855 | PDE Loss:  -5.3092 | Function Loss:  -3.5562\n",
      "Total loss:  -3.4857 | PDE Loss:  -5.3086 | Function Loss:  -3.5564\n",
      "Total loss:  -3.4858 | PDE Loss:  -5.3069 | Function Loss:  -3.5569\n",
      "Total loss:  -3.4859 | PDE Loss:  -5.3058 | Function Loss:  -3.5572\n",
      "Total loss:  -3.486 | PDE Loss:  -5.305 | Function Loss:  -3.5575\n",
      "Total loss:  -3.4861 | PDE Loss:  -5.3041 | Function Loss:  -3.5578\n",
      "Total loss:  -3.4862 | PDE Loss:  -5.3038 | Function Loss:  -3.558\n",
      "Total loss:  -3.4864 | PDE Loss:  -5.3035 | Function Loss:  -3.5581\n",
      "Total loss:  -3.4865 | PDE Loss:  -5.3037 | Function Loss:  -3.5583\n",
      "Total loss:  -3.4863 | PDE Loss:  -5.3043 | Function Loss:  -3.5579\n",
      "Total loss:  -3.4866 | PDE Loss:  -5.3042 | Function Loss:  -3.5583\n",
      "Total loss:  -3.4867 | PDE Loss:  -5.3049 | Function Loss:  -3.5583\n",
      "Total loss:  -3.4869 | PDE Loss:  -5.306 | Function Loss:  -3.5584\n",
      "Total loss:  -3.4871 | PDE Loss:  -5.307 | Function Loss:  -3.5584\n",
      "Total loss:  -3.4873 | PDE Loss:  -5.3085 | Function Loss:  -3.5584\n",
      "Total loss:  -3.4876 | PDE Loss:  -5.3097 | Function Loss:  -3.5584\n",
      "Total loss:  -3.4878 | PDE Loss:  -5.311 | Function Loss:  -3.5585\n",
      "Total loss:  -3.4881 | PDE Loss:  -5.3132 | Function Loss:  -3.5584\n",
      "Total loss:  -3.4883 | PDE Loss:  -5.3142 | Function Loss:  -3.5585\n",
      "Total loss:  -3.4886 | PDE Loss:  -5.3154 | Function Loss:  -3.5587\n",
      "Total loss:  -3.489 | PDE Loss:  -5.3171 | Function Loss:  -3.5588\n",
      "Total loss:  -3.4892 | PDE Loss:  -5.3183 | Function Loss:  -3.5589\n",
      "Total loss:  -3.4895 | PDE Loss:  -5.3195 | Function Loss:  -3.559\n",
      "Total loss:  -3.4897 | PDE Loss:  -5.3208 | Function Loss:  -3.5591\n",
      "Total loss:  -3.4901 | PDE Loss:  -5.3221 | Function Loss:  -3.5592\n",
      "Total loss:  -3.4905 | PDE Loss:  -5.3239 | Function Loss:  -3.5594\n",
      "Total loss:  -3.491 | PDE Loss:  -5.3254 | Function Loss:  -3.5597\n",
      "Total loss:  -3.4915 | PDE Loss:  -5.3272 | Function Loss:  -3.56\n",
      "Total loss:  -3.492 | PDE Loss:  -5.3288 | Function Loss:  -3.5603\n",
      "Total loss:  -3.4924 | PDE Loss:  -5.3299 | Function Loss:  -3.5606\n",
      "Total loss:  -3.4926 | PDE Loss:  -5.3307 | Function Loss:  -3.5607\n",
      "Total loss:  -3.4928 | PDE Loss:  -5.3313 | Function Loss:  -3.5608\n",
      "Total loss:  -3.493 | PDE Loss:  -5.3324 | Function Loss:  -3.5609\n",
      "Total loss:  -3.4932 | PDE Loss:  -5.3344 | Function Loss:  -3.5608\n",
      "Total loss:  -3.4935 | PDE Loss:  -5.3371 | Function Loss:  -3.5607\n",
      "Total loss:  -3.4938 | PDE Loss:  -5.3395 | Function Loss:  -3.5606\n",
      "Total loss:  -3.494 | PDE Loss:  -5.3415 | Function Loss:  -3.5606\n",
      "Total loss:  -3.4943 | PDE Loss:  -5.3442 | Function Loss:  -3.5605\n",
      "Total loss:  -3.4946 | PDE Loss:  -5.3456 | Function Loss:  -3.5606\n",
      "Total loss:  -3.4949 | PDE Loss:  -5.3481 | Function Loss:  -3.5605\n",
      "Total loss:  -3.4952 | PDE Loss:  -5.3497 | Function Loss:  -3.5606\n",
      "Total loss:  -3.4957 | PDE Loss:  -5.3504 | Function Loss:  -3.5611\n",
      "Total loss:  -3.4961 | PDE Loss:  -5.3523 | Function Loss:  -3.5613\n",
      "Total loss:  -3.4965 | PDE Loss:  -5.3525 | Function Loss:  -3.5617\n",
      "Total loss:  -3.4969 | PDE Loss:  -5.3525 | Function Loss:  -3.5622\n",
      "Total loss:  -3.4973 | PDE Loss:  -5.3521 | Function Loss:  -3.5626\n",
      "Total loss:  -3.4975 | PDE Loss:  -5.3513 | Function Loss:  -3.5631\n",
      "Total loss:  -3.4978 | PDE Loss:  -5.3507 | Function Loss:  -3.5634\n",
      "Total loss:  -3.4981 | PDE Loss:  -5.3502 | Function Loss:  -3.5639\n",
      "Total loss:  -3.4984 | PDE Loss:  -5.3496 | Function Loss:  -3.5643\n",
      "Total loss:  -3.4986 | PDE Loss:  -5.3496 | Function Loss:  -3.5646\n",
      "Total loss:  -3.4989 | PDE Loss:  -5.3499 | Function Loss:  -3.5648\n",
      "Total loss:  -3.4991 | PDE Loss:  -5.351 | Function Loss:  -3.565\n",
      "Total loss:  -3.4994 | PDE Loss:  -5.3523 | Function Loss:  -3.565\n",
      "Total loss:  -3.4996 | PDE Loss:  -5.3533 | Function Loss:  -3.5651\n",
      "Total loss:  -3.4997 | PDE Loss:  -5.3558 | Function Loss:  -3.5649\n",
      "Total loss:  -3.5 | PDE Loss:  -5.3564 | Function Loss:  -3.5651\n",
      "Total loss:  -3.5002 | PDE Loss:  -5.3564 | Function Loss:  -3.5654\n",
      "Total loss:  -3.5005 | PDE Loss:  -5.3564 | Function Loss:  -3.5656\n",
      "Total loss:  -3.5006 | PDE Loss:  -5.3561 | Function Loss:  -3.5659\n",
      "Total loss:  -3.5009 | PDE Loss:  -5.3559 | Function Loss:  -3.5662\n",
      "Total loss:  -3.5011 | PDE Loss:  -5.3558 | Function Loss:  -3.5665\n",
      "Total loss:  -3.5016 | PDE Loss:  -5.3556 | Function Loss:  -3.567\n",
      "Total loss:  -3.5019 | PDE Loss:  -5.3554 | Function Loss:  -3.5675\n",
      "Total loss:  -3.5023 | PDE Loss:  -5.3558 | Function Loss:  -3.5678\n",
      "Total loss:  -3.5026 | PDE Loss:  -5.356 | Function Loss:  -3.5682\n",
      "Total loss:  -3.5029 | PDE Loss:  -5.3566 | Function Loss:  -3.5684\n",
      "Total loss:  -3.5031 | PDE Loss:  -5.3563 | Function Loss:  -3.5687\n",
      "Total loss:  -3.5032 | PDE Loss:  -5.3565 | Function Loss:  -3.5688\n",
      "Total loss:  -3.5033 | PDE Loss:  -5.3565 | Function Loss:  -3.569\n",
      "Total loss:  -3.5035 | PDE Loss:  -5.3563 | Function Loss:  -3.5692\n",
      "Total loss:  -3.5038 | PDE Loss:  -5.3558 | Function Loss:  -3.5696\n",
      "Total loss:  -3.5042 | PDE Loss:  -5.3547 | Function Loss:  -3.5702\n",
      "Total loss:  -3.5045 | PDE Loss:  -5.354 | Function Loss:  -3.5707\n",
      "Total loss:  -3.5047 | PDE Loss:  -5.3531 | Function Loss:  -3.5711\n",
      "Total loss:  -3.5049 | PDE Loss:  -5.3529 | Function Loss:  -3.5714\n",
      "Total loss:  -3.5051 | PDE Loss:  -5.3523 | Function Loss:  -3.5717\n",
      "Total loss:  -3.5053 | PDE Loss:  -5.3529 | Function Loss:  -3.5718\n",
      "Total loss:  -3.5054 | PDE Loss:  -5.3535 | Function Loss:  -3.5719\n",
      "Total loss:  -3.5056 | PDE Loss:  -5.3546 | Function Loss:  -3.5719\n",
      "Total loss:  -3.5058 | PDE Loss:  -5.3565 | Function Loss:  -3.5718\n",
      "Total loss:  -3.5059 | PDE Loss:  -5.3585 | Function Loss:  -3.5716\n",
      "Total loss:  -3.506 | PDE Loss:  -5.3603 | Function Loss:  -3.5715\n",
      "Total loss:  -3.5061 | PDE Loss:  -5.3614 | Function Loss:  -3.5713\n",
      "Total loss:  -3.5061 | PDE Loss:  -5.3618 | Function Loss:  -3.5713\n",
      "Total loss:  -3.5062 | PDE Loss:  -5.3623 | Function Loss:  -3.5713\n",
      "Total loss:  -3.5062 | PDE Loss:  -5.3628 | Function Loss:  -3.5713\n",
      "Total loss:  -3.5063 | PDE Loss:  -5.3623 | Function Loss:  -3.5715\n",
      "Total loss:  -3.5064 | PDE Loss:  -5.3622 | Function Loss:  -3.5716\n",
      "Total loss:  -3.5065 | PDE Loss:  -5.3609 | Function Loss:  -3.572\n",
      "Total loss:  -3.5067 | PDE Loss:  -5.3592 | Function Loss:  -3.5725\n",
      "Total loss:  -3.5069 | PDE Loss:  -5.3576 | Function Loss:  -3.5729\n",
      "Total loss:  -3.5071 | PDE Loss:  -5.3555 | Function Loss:  -3.5735\n",
      "Total loss:  -3.5072 | PDE Loss:  -5.3552 | Function Loss:  -3.5737\n",
      "Total loss:  -3.5073 | PDE Loss:  -5.3553 | Function Loss:  -3.5738\n",
      "Total loss:  -3.5075 | PDE Loss:  -5.3559 | Function Loss:  -3.5739\n",
      "Total loss:  -3.5076 | PDE Loss:  -5.3571 | Function Loss:  -3.5738\n",
      "Total loss:  -3.5077 | PDE Loss:  -5.3583 | Function Loss:  -3.5737\n",
      "Total loss:  -3.5079 | PDE Loss:  -5.3601 | Function Loss:  -3.5736\n",
      "Total loss:  -3.508 | PDE Loss:  -5.3621 | Function Loss:  -3.5735\n",
      "Total loss:  -3.5082 | PDE Loss:  -5.3647 | Function Loss:  -3.5733\n",
      "Total loss:  -3.5083 | PDE Loss:  -5.3672 | Function Loss:  -3.573\n",
      "Total loss:  -3.5085 | PDE Loss:  -5.3702 | Function Loss:  -3.5727\n",
      "Total loss:  -3.5086 | PDE Loss:  -5.3725 | Function Loss:  -3.5725\n",
      "Total loss:  -3.5088 | PDE Loss:  -5.3752 | Function Loss:  -3.5722\n",
      "Total loss:  -3.509 | PDE Loss:  -5.3765 | Function Loss:  -3.5723\n",
      "Total loss:  -3.5091 | PDE Loss:  -5.3796 | Function Loss:  -3.572\n",
      "Total loss:  -3.5093 | PDE Loss:  -5.382 | Function Loss:  -3.5719\n",
      "Total loss:  -3.5095 | PDE Loss:  -5.3837 | Function Loss:  -3.5718\n",
      "Total loss:  -3.5097 | PDE Loss:  -5.3849 | Function Loss:  -3.5718\n",
      "Total loss:  -3.5098 | PDE Loss:  -5.3851 | Function Loss:  -3.572\n",
      "Total loss:  -3.51 | PDE Loss:  -5.3849 | Function Loss:  -3.5721\n",
      "Total loss:  -3.5101 | PDE Loss:  -5.3845 | Function Loss:  -3.5723\n",
      "Total loss:  -3.5102 | PDE Loss:  -5.3842 | Function Loss:  -3.5725\n",
      "Total loss:  -3.5103 | PDE Loss:  -5.3838 | Function Loss:  -3.5727\n",
      "Total loss:  -3.5104 | PDE Loss:  -5.3836 | Function Loss:  -3.5729\n",
      "Total loss:  -3.5106 | PDE Loss:  -5.3835 | Function Loss:  -3.573\n",
      "Total loss:  -3.5107 | PDE Loss:  -5.3836 | Function Loss:  -3.5731\n",
      "Total loss:  -3.5108 | PDE Loss:  -5.3834 | Function Loss:  -3.5733\n",
      "Total loss:  -3.5109 | PDE Loss:  -5.3835 | Function Loss:  -3.5734\n",
      "Total loss:  -3.511 | PDE Loss:  -5.3834 | Function Loss:  -3.5736\n",
      "Total loss:  -3.5112 | PDE Loss:  -5.3836 | Function Loss:  -3.5737\n",
      "Total loss:  -3.5113 | PDE Loss:  -5.3834 | Function Loss:  -3.5739\n",
      "Total loss:  -3.5115 | PDE Loss:  -5.3834 | Function Loss:  -3.5741\n",
      "Total loss:  -3.5118 | PDE Loss:  -5.3831 | Function Loss:  -3.5745\n",
      "Total loss:  -3.512 | PDE Loss:  -5.383 | Function Loss:  -3.5748\n",
      "Total loss:  -3.5122 | PDE Loss:  -5.3822 | Function Loss:  -3.5751\n",
      "Total loss:  -3.5125 | PDE Loss:  -5.382 | Function Loss:  -3.5755\n",
      "Total loss:  -3.5127 | PDE Loss:  -5.3805 | Function Loss:  -3.576\n",
      "Total loss:  -3.5129 | PDE Loss:  -5.3796 | Function Loss:  -3.5763\n",
      "Total loss:  -3.5131 | PDE Loss:  -5.3789 | Function Loss:  -3.5767\n",
      "Total loss:  -3.5133 | PDE Loss:  -5.378 | Function Loss:  -3.5771\n",
      "Total loss:  -3.5136 | PDE Loss:  -5.3774 | Function Loss:  -3.5775\n",
      "Total loss:  -3.5138 | PDE Loss:  -5.3761 | Function Loss:  -3.578\n",
      "Total loss:  -3.514 | PDE Loss:  -5.3757 | Function Loss:  -3.5782\n",
      "Total loss:  -3.5142 | PDE Loss:  -5.3758 | Function Loss:  -3.5785\n",
      "Total loss:  -3.5145 | PDE Loss:  -5.3755 | Function Loss:  -3.5789\n",
      "Total loss:  -3.5147 | PDE Loss:  -5.376 | Function Loss:  -3.5791\n",
      "Total loss:  -3.5149 | PDE Loss:  -5.3765 | Function Loss:  -3.5792\n",
      "Total loss:  -3.5151 | PDE Loss:  -5.3773 | Function Loss:  -3.5793\n",
      "Total loss:  -3.5154 | PDE Loss:  -5.3784 | Function Loss:  -3.5794\n",
      "Total loss:  -3.5157 | PDE Loss:  -5.38 | Function Loss:  -3.5796\n",
      "Total loss:  -3.516 | PDE Loss:  -5.3809 | Function Loss:  -3.5797\n",
      "Total loss:  -3.5163 | PDE Loss:  -5.3822 | Function Loss:  -3.5799\n",
      "Total loss:  -3.5166 | PDE Loss:  -5.3832 | Function Loss:  -3.5801\n",
      "Total loss:  -3.5169 | PDE Loss:  -5.3837 | Function Loss:  -3.5804\n",
      "Total loss:  -3.5172 | PDE Loss:  -5.3838 | Function Loss:  -3.5807\n",
      "Total loss:  -3.5174 | PDE Loss:  -5.3837 | Function Loss:  -3.581\n",
      "Total loss:  -3.5176 | PDE Loss:  -5.3832 | Function Loss:  -3.5813\n",
      "Total loss:  -3.5179 | PDE Loss:  -5.3826 | Function Loss:  -3.5816\n",
      "Total loss:  -3.5181 | PDE Loss:  -5.3818 | Function Loss:  -3.5821\n",
      "Total loss:  -3.5183 | PDE Loss:  -5.3811 | Function Loss:  -3.5823\n",
      "Total loss:  -3.5185 | PDE Loss:  -5.3809 | Function Loss:  -3.5826\n",
      "Total loss:  -3.5187 | PDE Loss:  -5.3809 | Function Loss:  -3.5828\n",
      "Total loss:  -3.5189 | PDE Loss:  -5.3813 | Function Loss:  -3.583\n",
      "Total loss:  -3.5191 | PDE Loss:  -5.3825 | Function Loss:  -3.5831\n",
      "Total loss:  -3.5194 | PDE Loss:  -5.3823 | Function Loss:  -3.5834\n",
      "Total loss:  -3.5196 | PDE Loss:  -5.3841 | Function Loss:  -3.5834\n",
      "Total loss:  -3.5199 | PDE Loss:  -5.3861 | Function Loss:  -3.5834\n",
      "Total loss:  -3.5201 | PDE Loss:  -5.3884 | Function Loss:  -3.5833\n",
      "Total loss:  -3.5203 | PDE Loss:  -5.39 | Function Loss:  -3.5833\n",
      "Total loss:  -3.5205 | PDE Loss:  -5.3913 | Function Loss:  -3.5833\n",
      "Total loss:  -3.5207 | PDE Loss:  -5.3926 | Function Loss:  -3.5833\n",
      "Total loss:  -3.5209 | PDE Loss:  -5.393 | Function Loss:  -3.5835\n",
      "Total loss:  -3.5211 | PDE Loss:  -5.3939 | Function Loss:  -3.5835\n",
      "Total loss:  -3.5213 | PDE Loss:  -5.3938 | Function Loss:  -3.5838\n",
      "Total loss:  -3.5215 | PDE Loss:  -5.3957 | Function Loss:  -3.5838\n",
      "Total loss:  -3.5217 | PDE Loss:  -5.3955 | Function Loss:  -3.584\n",
      "Total loss:  -3.5219 | PDE Loss:  -5.3951 | Function Loss:  -3.5844\n",
      "Total loss:  -3.5222 | PDE Loss:  -5.3958 | Function Loss:  -3.5846\n",
      "Total loss:  -3.5225 | PDE Loss:  -5.3965 | Function Loss:  -3.5848\n",
      "Total loss:  -3.5228 | PDE Loss:  -5.3977 | Function Loss:  -3.5849\n",
      "Total loss:  -3.523 | PDE Loss:  -5.399 | Function Loss:  -3.585\n",
      "Total loss:  -3.5233 | PDE Loss:  -5.4007 | Function Loss:  -3.5851\n",
      "Total loss:  -3.5236 | PDE Loss:  -5.4028 | Function Loss:  -3.5851\n",
      "Total loss:  -3.524 | PDE Loss:  -5.4057 | Function Loss:  -3.5852\n",
      "Total loss:  -3.5244 | PDE Loss:  -5.4085 | Function Loss:  -3.5852\n",
      "Total loss:  -3.5247 | PDE Loss:  -5.4103 | Function Loss:  -3.5853\n",
      "Total loss:  -3.5251 | PDE Loss:  -5.4124 | Function Loss:  -3.5854\n",
      "Total loss:  -3.5254 | PDE Loss:  -5.4128 | Function Loss:  -3.5856\n",
      "Total loss:  -3.5256 | PDE Loss:  -5.4127 | Function Loss:  -3.5859\n",
      "Total loss:  -3.5258 | PDE Loss:  -5.4123 | Function Loss:  -3.5863\n",
      "Total loss:  -3.5261 | PDE Loss:  -5.4123 | Function Loss:  -3.5866\n",
      "Total loss:  -3.5265 | PDE Loss:  -5.4121 | Function Loss:  -3.5871\n",
      "Total loss:  -3.5269 | PDE Loss:  -5.412 | Function Loss:  -3.5875\n",
      "Total loss:  -3.5273 | PDE Loss:  -5.4125 | Function Loss:  -3.588\n",
      "Total loss:  -3.5277 | PDE Loss:  -5.4122 | Function Loss:  -3.5884\n",
      "Total loss:  -3.5279 | PDE Loss:  -5.4127 | Function Loss:  -3.5886\n",
      "Total loss:  -3.5281 | PDE Loss:  -5.4126 | Function Loss:  -3.5888\n",
      "Total loss:  -3.5283 | PDE Loss:  -5.4129 | Function Loss:  -3.589\n",
      "Total loss:  -3.5286 | PDE Loss:  -5.413 | Function Loss:  -3.5893\n",
      "Total loss:  -3.5288 | PDE Loss:  -5.4136 | Function Loss:  -3.5895\n",
      "Total loss:  -3.5292 | PDE Loss:  -5.4135 | Function Loss:  -3.5899\n",
      "Total loss:  -3.5294 | PDE Loss:  -5.4138 | Function Loss:  -3.5902\n",
      "Total loss:  -3.5296 | PDE Loss:  -5.4134 | Function Loss:  -3.5905\n",
      "Total loss:  -3.5298 | PDE Loss:  -5.4134 | Function Loss:  -3.5907\n",
      "Total loss:  -3.53 | PDE Loss:  -5.413 | Function Loss:  -3.591\n",
      "Total loss:  -3.5302 | PDE Loss:  -5.4132 | Function Loss:  -3.5912\n",
      "Total loss:  -3.5304 | PDE Loss:  -5.4136 | Function Loss:  -3.5913\n",
      "Total loss:  -3.5306 | PDE Loss:  -5.4144 | Function Loss:  -3.5914\n",
      "Total loss:  -3.5308 | PDE Loss:  -5.4154 | Function Loss:  -3.5915\n",
      "Total loss:  -3.5309 | PDE Loss:  -5.4164 | Function Loss:  -3.5915\n",
      "Total loss:  -3.5311 | PDE Loss:  -5.4172 | Function Loss:  -3.5916\n",
      "Total loss:  -3.5313 | PDE Loss:  -5.4183 | Function Loss:  -3.5916\n",
      "Total loss:  -3.5314 | PDE Loss:  -5.4174 | Function Loss:  -3.5919\n",
      "Total loss:  -3.5317 | PDE Loss:  -5.4188 | Function Loss:  -3.592\n",
      "Total loss:  -3.5319 | PDE Loss:  -5.4195 | Function Loss:  -3.5921\n",
      "Total loss:  -3.5321 | PDE Loss:  -5.4197 | Function Loss:  -3.5924\n",
      "Total loss:  -3.5325 | PDE Loss:  -5.4181 | Function Loss:  -3.593\n",
      "Total loss:  -3.5328 | PDE Loss:  -5.4184 | Function Loss:  -3.5933\n",
      "Total loss:  -3.5329 | PDE Loss:  -5.4167 | Function Loss:  -3.5938\n",
      "Total loss:  -3.5331 | PDE Loss:  -5.4152 | Function Loss:  -3.5942\n",
      "Total loss:  -3.5332 | PDE Loss:  -5.414 | Function Loss:  -3.5945\n",
      "Total loss:  -3.5334 | PDE Loss:  -5.4132 | Function Loss:  -3.5948\n",
      "Total loss:  -3.5335 | PDE Loss:  -5.4127 | Function Loss:  -3.595\n",
      "Total loss:  -3.5336 | PDE Loss:  -5.4123 | Function Loss:  -3.5952\n",
      "Total loss:  -3.5337 | PDE Loss:  -5.4119 | Function Loss:  -3.5954\n",
      "Total loss:  -3.5338 | PDE Loss:  -5.4119 | Function Loss:  -3.5955\n",
      "Total loss:  -3.5339 | PDE Loss:  -5.4116 | Function Loss:  -3.5957\n",
      "Total loss:  -3.534 | PDE Loss:  -5.4119 | Function Loss:  -3.5957\n",
      "Total loss:  -3.5341 | PDE Loss:  -5.412 | Function Loss:  -3.5958\n",
      "Total loss:  -3.5342 | PDE Loss:  -5.4122 | Function Loss:  -3.5959\n",
      "Total loss:  -3.5344 | PDE Loss:  -5.4126 | Function Loss:  -3.596\n",
      "Total loss:  -3.5345 | PDE Loss:  -5.4127 | Function Loss:  -3.5962\n",
      "Total loss:  -3.5347 | PDE Loss:  -5.4128 | Function Loss:  -3.5964\n",
      "Total loss:  -3.535 | PDE Loss:  -5.4141 | Function Loss:  -3.5966\n",
      "Total loss:  -3.5353 | PDE Loss:  -5.4137 | Function Loss:  -3.597\n",
      "Total loss:  -3.5356 | PDE Loss:  -5.4141 | Function Loss:  -3.5972\n",
      "Total loss:  -3.536 | PDE Loss:  -5.4146 | Function Loss:  -3.5976\n",
      "Total loss:  -3.5364 | PDE Loss:  -5.4154 | Function Loss:  -3.5979\n",
      "Total loss:  -3.5368 | PDE Loss:  -5.4151 | Function Loss:  -3.5984\n",
      "Total loss:  -3.537 | PDE Loss:  -5.4159 | Function Loss:  -3.5986\n",
      "Total loss:  -3.5373 | PDE Loss:  -5.4167 | Function Loss:  -3.5987\n",
      "Total loss:  -3.5375 | PDE Loss:  -5.4174 | Function Loss:  -3.5989\n",
      "Total loss:  -3.5376 | PDE Loss:  -5.4182 | Function Loss:  -3.5989\n",
      "Total loss:  -3.5378 | PDE Loss:  -5.4185 | Function Loss:  -3.5991\n",
      "Total loss:  -3.5379 | PDE Loss:  -5.4191 | Function Loss:  -3.5992\n",
      "Total loss:  -3.5381 | PDE Loss:  -5.4195 | Function Loss:  -3.5992\n",
      "Total loss:  -3.5382 | PDE Loss:  -5.4201 | Function Loss:  -3.5993\n",
      "Total loss:  -3.5384 | PDE Loss:  -5.4209 | Function Loss:  -3.5994\n",
      "Total loss:  -3.5385 | PDE Loss:  -5.4219 | Function Loss:  -3.5994\n",
      "Total loss:  -3.5386 | PDE Loss:  -5.4225 | Function Loss:  -3.5995\n",
      "Total loss:  -3.5387 | PDE Loss:  -5.4229 | Function Loss:  -3.5995\n",
      "Total loss:  -3.5388 | PDE Loss:  -5.4234 | Function Loss:  -3.5995\n",
      "Total loss:  -3.5389 | PDE Loss:  -5.4234 | Function Loss:  -3.5997\n",
      "Total loss:  -3.5391 | PDE Loss:  -5.4232 | Function Loss:  -3.5999\n",
      "Total loss:  -3.5392 | PDE Loss:  -5.4225 | Function Loss:  -3.6001\n",
      "Total loss:  -3.5393 | PDE Loss:  -5.4217 | Function Loss:  -3.6004\n",
      "Total loss:  -3.5394 | PDE Loss:  -5.4202 | Function Loss:  -3.6007\n",
      "Total loss:  -3.5395 | PDE Loss:  -5.4194 | Function Loss:  -3.6009\n",
      "Total loss:  -3.5396 | PDE Loss:  -5.4189 | Function Loss:  -3.6011\n",
      "Total loss:  -3.5397 | PDE Loss:  -5.4187 | Function Loss:  -3.6013\n",
      "Total loss:  -3.5398 | PDE Loss:  -5.4186 | Function Loss:  -3.6014\n",
      "Total loss:  -3.5399 | PDE Loss:  -5.4189 | Function Loss:  -3.6014\n",
      "Total loss:  -3.5399 | PDE Loss:  -5.4192 | Function Loss:  -3.6014\n",
      "Total loss:  -3.54 | PDE Loss:  -5.4198 | Function Loss:  -3.6014\n",
      "Total loss:  -3.5401 | PDE Loss:  -5.4209 | Function Loss:  -3.6014\n",
      "Total loss:  -3.5402 | PDE Loss:  -5.4211 | Function Loss:  -3.6014\n",
      "Total loss:  -3.5402 | PDE Loss:  -5.4218 | Function Loss:  -3.6014\n",
      "Total loss:  -3.5403 | PDE Loss:  -5.4224 | Function Loss:  -3.6014\n",
      "Total loss:  -3.5404 | PDE Loss:  -5.4233 | Function Loss:  -3.6014\n",
      "Total loss:  -3.5406 | PDE Loss:  -5.4246 | Function Loss:  -3.6013\n",
      "Total loss:  -3.5407 | PDE Loss:  -5.4262 | Function Loss:  -3.6013\n",
      "Total loss:  -3.5408 | PDE Loss:  -5.4273 | Function Loss:  -3.6012\n",
      "Total loss:  -3.541 | PDE Loss:  -5.4283 | Function Loss:  -3.6013\n",
      "Total loss:  -3.5411 | PDE Loss:  -5.4291 | Function Loss:  -3.6013\n",
      "Total loss:  -3.5413 | PDE Loss:  -5.4294 | Function Loss:  -3.6014\n",
      "Total loss:  -3.5414 | PDE Loss:  -5.4298 | Function Loss:  -3.6016\n",
      "Total loss:  -3.5416 | PDE Loss:  -5.4298 | Function Loss:  -3.6018\n",
      "Total loss:  -3.5418 | PDE Loss:  -5.4303 | Function Loss:  -3.6019\n",
      "Total loss:  -3.542 | PDE Loss:  -5.4304 | Function Loss:  -3.6021\n",
      "Total loss:  -3.5422 | PDE Loss:  -5.4317 | Function Loss:  -3.6022\n",
      "Total loss:  -3.5424 | PDE Loss:  -5.4329 | Function Loss:  -3.6023\n",
      "Total loss:  -3.5427 | PDE Loss:  -5.4351 | Function Loss:  -3.6023\n",
      "Total loss:  -3.543 | PDE Loss:  -5.4371 | Function Loss:  -3.6023\n",
      "Total loss:  -3.5433 | PDE Loss:  -5.4394 | Function Loss:  -3.6023\n",
      "Total loss:  -3.5435 | PDE Loss:  -5.441 | Function Loss:  -3.6023\n",
      "Total loss:  -3.5438 | PDE Loss:  -5.4429 | Function Loss:  -3.6024\n",
      "Total loss:  -3.5442 | PDE Loss:  -5.4441 | Function Loss:  -3.6026\n",
      "Total loss:  -3.5445 | PDE Loss:  -5.4451 | Function Loss:  -3.6029\n",
      "Total loss:  -3.5448 | PDE Loss:  -5.4447 | Function Loss:  -3.6032\n",
      "Total loss:  -3.5451 | PDE Loss:  -5.4443 | Function Loss:  -3.6036\n",
      "Total loss:  -3.5453 | PDE Loss:  -5.4432 | Function Loss:  -3.6041\n",
      "Total loss:  -3.5456 | PDE Loss:  -5.4421 | Function Loss:  -3.6045\n",
      "Total loss:  -3.5457 | PDE Loss:  -5.4405 | Function Loss:  -3.6049\n",
      "Total loss:  -3.5459 | PDE Loss:  -5.4397 | Function Loss:  -3.6052\n",
      "Total loss:  -3.546 | PDE Loss:  -5.4387 | Function Loss:  -3.6055\n",
      "Total loss:  -3.5461 | PDE Loss:  -5.4386 | Function Loss:  -3.6056\n",
      "Total loss:  -3.5462 | PDE Loss:  -5.4385 | Function Loss:  -3.6058\n",
      "Total loss:  -3.5464 | PDE Loss:  -5.4389 | Function Loss:  -3.6059\n",
      "Total loss:  -3.5465 | PDE Loss:  -5.4398 | Function Loss:  -3.6059\n",
      "Total loss:  -3.5466 | PDE Loss:  -5.4409 | Function Loss:  -3.6059\n",
      "Total loss:  -3.5467 | PDE Loss:  -5.442 | Function Loss:  -3.6058\n",
      "Total loss:  -3.5468 | PDE Loss:  -5.443 | Function Loss:  -3.6058\n",
      "Total loss:  -3.5469 | PDE Loss:  -5.444 | Function Loss:  -3.6058\n",
      "Total loss:  -3.547 | PDE Loss:  -5.4442 | Function Loss:  -3.6058\n",
      "Total loss:  -3.5471 | PDE Loss:  -5.4452 | Function Loss:  -3.6058\n",
      "Total loss:  -3.5472 | PDE Loss:  -5.4454 | Function Loss:  -3.6059\n",
      "Total loss:  -3.5472 | PDE Loss:  -5.4455 | Function Loss:  -3.6059\n",
      "Total loss:  -3.5473 | PDE Loss:  -5.4455 | Function Loss:  -3.606\n",
      "Total loss:  -3.5474 | PDE Loss:  -5.4452 | Function Loss:  -3.6061\n",
      "Total loss:  -3.5474 | PDE Loss:  -5.4451 | Function Loss:  -3.6062\n",
      "Total loss:  -3.5475 | PDE Loss:  -5.4447 | Function Loss:  -3.6064\n",
      "Total loss:  -3.5476 | PDE Loss:  -5.4443 | Function Loss:  -3.6066\n",
      "Total loss:  -3.5478 | PDE Loss:  -5.4436 | Function Loss:  -3.6068\n",
      "Total loss:  -3.5479 | PDE Loss:  -5.4425 | Function Loss:  -3.6072\n",
      "Total loss:  -3.5481 | PDE Loss:  -5.441 | Function Loss:  -3.6076\n",
      "Total loss:  -3.5482 | PDE Loss:  -5.4396 | Function Loss:  -3.6079\n",
      "Total loss:  -3.5484 | PDE Loss:  -5.4382 | Function Loss:  -3.6083\n",
      "Total loss:  -3.5486 | PDE Loss:  -5.4366 | Function Loss:  -3.6087\n",
      "Total loss:  -3.5487 | PDE Loss:  -5.4354 | Function Loss:  -3.6091\n",
      "Total loss:  -3.5489 | PDE Loss:  -5.4342 | Function Loss:  -3.6095\n",
      "Total loss:  -3.5491 | PDE Loss:  -5.433 | Function Loss:  -3.6099\n",
      "Total loss:  -3.5494 | PDE Loss:  -5.4314 | Function Loss:  -3.6105\n",
      "Total loss:  -3.5497 | PDE Loss:  -5.4301 | Function Loss:  -3.611\n",
      "Total loss:  -3.5499 | PDE Loss:  -5.4287 | Function Loss:  -3.6115\n",
      "Total loss:  -3.5502 | PDE Loss:  -5.4283 | Function Loss:  -3.6118\n",
      "Total loss:  -3.5505 | PDE Loss:  -5.4276 | Function Loss:  -3.6123\n",
      "Total loss:  -3.5507 | PDE Loss:  -5.4279 | Function Loss:  -3.6125\n",
      "Total loss:  -3.5508 | PDE Loss:  -5.4275 | Function Loss:  -3.6127\n",
      "Total loss:  -3.551 | PDE Loss:  -5.4272 | Function Loss:  -3.6129\n",
      "Total loss:  -3.551 | PDE Loss:  -5.4263 | Function Loss:  -3.6132\n",
      "Total loss:  -3.5512 | PDE Loss:  -5.425 | Function Loss:  -3.6135\n",
      "Total loss:  -3.5513 | PDE Loss:  -5.4226 | Function Loss:  -3.614\n",
      "Total loss:  -3.5514 | PDE Loss:  -5.4203 | Function Loss:  -3.6145\n",
      "Total loss:  -3.5514 | PDE Loss:  -5.4189 | Function Loss:  -3.6148\n",
      "Total loss:  -3.5515 | PDE Loss:  -5.4166 | Function Loss:  -3.6152\n",
      "Total loss:  -3.5516 | PDE Loss:  -5.4146 | Function Loss:  -3.6157\n",
      "Total loss:  -3.5517 | PDE Loss:  -5.4126 | Function Loss:  -3.6161\n",
      "Total loss:  -3.5518 | PDE Loss:  -5.4107 | Function Loss:  -3.6165\n",
      "Total loss:  -3.5519 | PDE Loss:  -5.4086 | Function Loss:  -3.617\n",
      "Total loss:  -3.5521 | PDE Loss:  -5.4064 | Function Loss:  -3.6175\n",
      "Total loss:  -3.5523 | PDE Loss:  -5.4045 | Function Loss:  -3.6181\n",
      "Total loss:  -3.5525 | PDE Loss:  -5.4034 | Function Loss:  -3.6184\n",
      "Total loss:  -3.5526 | PDE Loss:  -5.4024 | Function Loss:  -3.6188\n",
      "Total loss:  -3.5528 | PDE Loss:  -5.4022 | Function Loss:  -3.6191\n",
      "Total loss:  -3.553 | PDE Loss:  -5.4028 | Function Loss:  -3.6192\n",
      "Total loss:  -3.5532 | PDE Loss:  -5.4033 | Function Loss:  -3.6193\n",
      "Total loss:  -3.5533 | PDE Loss:  -5.4044 | Function Loss:  -3.6193\n",
      "Total loss:  -3.5535 | PDE Loss:  -5.4046 | Function Loss:  -3.6194\n",
      "Total loss:  -3.5536 | PDE Loss:  -5.4046 | Function Loss:  -3.6195\n",
      "Total loss:  -3.5536 | PDE Loss:  -5.4042 | Function Loss:  -3.6197\n",
      "Total loss:  -3.5538 | PDE Loss:  -5.4032 | Function Loss:  -3.62\n",
      "Total loss:  -3.5539 | PDE Loss:  -5.4017 | Function Loss:  -3.6204\n",
      "Total loss:  -3.554 | PDE Loss:  -5.3996 | Function Loss:  -3.6209\n",
      "Total loss:  -3.5542 | PDE Loss:  -5.3982 | Function Loss:  -3.6213\n",
      "Total loss:  -3.5544 | PDE Loss:  -5.396 | Function Loss:  -3.6219\n",
      "Total loss:  -3.5545 | PDE Loss:  -5.3945 | Function Loss:  -3.6223\n",
      "Total loss:  -3.5547 | PDE Loss:  -5.3923 | Function Loss:  -3.6229\n",
      "Total loss:  -3.5548 | PDE Loss:  -5.3907 | Function Loss:  -3.6233\n",
      "Total loss:  -3.5551 | PDE Loss:  -5.3885 | Function Loss:  -3.624\n",
      "Total loss:  -3.5554 | PDE Loss:  -5.3861 | Function Loss:  -3.6247\n",
      "Total loss:  -3.5553 | PDE Loss:  -5.3794 | Function Loss:  -3.6259\n",
      "Total loss:  -3.5555 | PDE Loss:  -5.3833 | Function Loss:  -3.6254\n",
      "Total loss:  -3.5558 | PDE Loss:  -5.3815 | Function Loss:  -3.626\n",
      "Total loss:  -3.5561 | PDE Loss:  -5.3793 | Function Loss:  -3.6267\n",
      "Total loss:  -3.5563 | PDE Loss:  -5.3766 | Function Loss:  -3.6275\n",
      "Total loss:  -3.5565 | PDE Loss:  -5.3742 | Function Loss:  -3.6282\n",
      "Total loss:  -3.5566 | PDE Loss:  -5.3717 | Function Loss:  -3.6288\n",
      "Total loss:  -3.5568 | PDE Loss:  -5.3695 | Function Loss:  -3.6293\n",
      "Total loss:  -3.5569 | PDE Loss:  -5.3672 | Function Loss:  -3.6299\n",
      "Total loss:  -3.5571 | PDE Loss:  -5.3652 | Function Loss:  -3.6305\n",
      "Total loss:  -3.5573 | PDE Loss:  -5.3629 | Function Loss:  -3.6312\n",
      "Total loss:  -3.5573 | PDE Loss:  -5.3596 | Function Loss:  -3.6318\n",
      "Total loss:  -3.5574 | PDE Loss:  -5.3614 | Function Loss:  -3.6316\n",
      "Total loss:  -3.5576 | PDE Loss:  -5.3612 | Function Loss:  -3.6318\n",
      "Total loss:  -3.5578 | PDE Loss:  -5.3609 | Function Loss:  -3.6321\n",
      "Total loss:  -3.558 | PDE Loss:  -5.3609 | Function Loss:  -3.6324\n",
      "Total loss:  -3.5584 | PDE Loss:  -5.3609 | Function Loss:  -3.6328\n",
      "Total loss:  -3.5586 | PDE Loss:  -5.3617 | Function Loss:  -3.633\n",
      "Total loss:  -3.5589 | PDE Loss:  -5.362 | Function Loss:  -3.6332\n",
      "Total loss:  -3.5591 | PDE Loss:  -5.3629 | Function Loss:  -3.6333\n",
      "Total loss:  -3.5592 | PDE Loss:  -5.3633 | Function Loss:  -3.6334\n",
      "Total loss:  -3.5594 | PDE Loss:  -5.3639 | Function Loss:  -3.6335\n",
      "Total loss:  -3.5595 | PDE Loss:  -5.3645 | Function Loss:  -3.6335\n",
      "Total loss:  -3.5596 | PDE Loss:  -5.3647 | Function Loss:  -3.6336\n",
      "Total loss:  -3.5597 | PDE Loss:  -5.3652 | Function Loss:  -3.6337\n",
      "Total loss:  -3.5599 | PDE Loss:  -5.3651 | Function Loss:  -3.6339\n",
      "Total loss:  -3.56 | PDE Loss:  -5.3653 | Function Loss:  -3.6339\n",
      "Total loss:  -3.5601 | PDE Loss:  -5.3647 | Function Loss:  -3.6342\n",
      "Total loss:  -3.5603 | PDE Loss:  -5.3644 | Function Loss:  -3.6344\n",
      "Total loss:  -3.5604 | PDE Loss:  -5.3641 | Function Loss:  -3.6346\n",
      "Total loss:  -3.5605 | PDE Loss:  -5.363 | Function Loss:  -3.635\n",
      "Total loss:  -3.5607 | PDE Loss:  -5.3621 | Function Loss:  -3.6354\n",
      "Total loss:  -3.5608 | PDE Loss:  -5.3613 | Function Loss:  -3.6356\n",
      "Total loss:  -3.5609 | PDE Loss:  -5.3601 | Function Loss:  -3.636\n",
      "Total loss:  -3.5611 | PDE Loss:  -5.359 | Function Loss:  -3.6364\n",
      "Total loss:  -3.5612 | PDE Loss:  -5.3575 | Function Loss:  -3.6368\n",
      "Total loss:  -3.5613 | PDE Loss:  -5.3565 | Function Loss:  -3.6372\n",
      "Total loss:  -3.5615 | PDE Loss:  -5.3554 | Function Loss:  -3.6376\n",
      "Total loss:  -3.5616 | PDE Loss:  -5.3559 | Function Loss:  -3.6376\n",
      "Total loss:  -3.5617 | PDE Loss:  -5.3551 | Function Loss:  -3.6379\n",
      "Total loss:  -3.5618 | PDE Loss:  -5.3551 | Function Loss:  -3.638\n",
      "Total loss:  -3.5618 | PDE Loss:  -5.3553 | Function Loss:  -3.638\n",
      "Total loss:  -3.5619 | PDE Loss:  -5.3558 | Function Loss:  -3.638\n",
      "Total loss:  -3.5619 | PDE Loss:  -5.3564 | Function Loss:  -3.6379\n",
      "Total loss:  -3.5619 | PDE Loss:  -5.357 | Function Loss:  -3.6378\n",
      "Total loss:  -3.562 | PDE Loss:  -5.3579 | Function Loss:  -3.6377\n",
      "Total loss:  -3.5621 | PDE Loss:  -5.3582 | Function Loss:  -3.6377\n",
      "Total loss:  -3.5621 | PDE Loss:  -5.3584 | Function Loss:  -3.6378\n",
      "Total loss:  -3.5622 | PDE Loss:  -5.3581 | Function Loss:  -3.6379\n",
      "Total loss:  -3.5622 | PDE Loss:  -5.3571 | Function Loss:  -3.6382\n",
      "Total loss:  -3.5623 | PDE Loss:  -5.3564 | Function Loss:  -3.6384\n",
      "Total loss:  -3.5623 | PDE Loss:  -5.355 | Function Loss:  -3.6387\n",
      "Total loss:  -3.5624 | PDE Loss:  -5.354 | Function Loss:  -3.639\n",
      "Total loss:  -3.5625 | PDE Loss:  -5.3525 | Function Loss:  -3.6394\n",
      "Total loss:  -3.5626 | PDE Loss:  -5.3516 | Function Loss:  -3.6397\n",
      "Total loss:  -3.5627 | PDE Loss:  -5.3507 | Function Loss:  -3.64\n",
      "Total loss:  -3.5628 | PDE Loss:  -5.35 | Function Loss:  -3.6403\n",
      "Total loss:  -3.563 | PDE Loss:  -5.3499 | Function Loss:  -3.6404\n",
      "Total loss:  -3.5631 | PDE Loss:  -5.35 | Function Loss:  -3.6405\n",
      "Total loss:  -3.5632 | PDE Loss:  -5.3503 | Function Loss:  -3.6406\n",
      "Total loss:  -3.5632 | PDE Loss:  -5.3507 | Function Loss:  -3.6406\n",
      "Total loss:  -3.5633 | PDE Loss:  -5.351 | Function Loss:  -3.6406\n",
      "Total loss:  -3.5634 | PDE Loss:  -5.3512 | Function Loss:  -3.6407\n",
      "Total loss:  -3.5635 | PDE Loss:  -5.3514 | Function Loss:  -3.6408\n",
      "Total loss:  -3.5636 | PDE Loss:  -5.3517 | Function Loss:  -3.6408\n",
      "Total loss:  -3.5637 | PDE Loss:  -5.3514 | Function Loss:  -3.641\n",
      "Total loss:  -3.5638 | PDE Loss:  -5.351 | Function Loss:  -3.6412\n",
      "Total loss:  -3.5639 | PDE Loss:  -5.3505 | Function Loss:  -3.6414\n",
      "Total loss:  -3.564 | PDE Loss:  -5.35 | Function Loss:  -3.6416\n",
      "Total loss:  -3.5641 | PDE Loss:  -5.3493 | Function Loss:  -3.6419\n",
      "Total loss:  -3.5642 | PDE Loss:  -5.3486 | Function Loss:  -3.6421\n",
      "Total loss:  -3.5643 | PDE Loss:  -5.3475 | Function Loss:  -3.6425\n",
      "Total loss:  -3.5645 | PDE Loss:  -5.3467 | Function Loss:  -3.6428\n",
      "Total loss:  -3.5647 | PDE Loss:  -5.3454 | Function Loss:  -3.6433\n",
      "Total loss:  -3.5649 | PDE Loss:  -5.3434 | Function Loss:  -3.6439\n",
      "Total loss:  -3.5651 | PDE Loss:  -5.3428 | Function Loss:  -3.6444\n",
      "Total loss:  -3.5653 | PDE Loss:  -5.3423 | Function Loss:  -3.6447\n",
      "Total loss:  -3.5656 | PDE Loss:  -5.3427 | Function Loss:  -3.645\n",
      "Total loss:  -3.5658 | PDE Loss:  -5.343 | Function Loss:  -3.6452\n",
      "Total loss:  -3.566 | PDE Loss:  -5.3439 | Function Loss:  -3.6452\n",
      "Total loss:  -3.5661 | PDE Loss:  -5.3444 | Function Loss:  -3.6453\n",
      "Total loss:  -3.5662 | PDE Loss:  -5.3449 | Function Loss:  -3.6453\n",
      "Total loss:  -3.5663 | PDE Loss:  -5.3452 | Function Loss:  -3.6454\n",
      "Total loss:  -3.5664 | PDE Loss:  -5.3456 | Function Loss:  -3.6454\n",
      "Total loss:  -3.5665 | PDE Loss:  -5.3447 | Function Loss:  -3.6457\n",
      "Total loss:  -3.5667 | PDE Loss:  -5.3455 | Function Loss:  -3.6457\n",
      "Total loss:  -3.5668 | PDE Loss:  -5.3444 | Function Loss:  -3.646\n",
      "Total loss:  -3.5669 | PDE Loss:  -5.3444 | Function Loss:  -3.6462\n",
      "Total loss:  -3.567 | PDE Loss:  -5.3435 | Function Loss:  -3.6465\n",
      "Total loss:  -3.5671 | PDE Loss:  -5.3431 | Function Loss:  -3.6467\n",
      "Total loss:  -3.5672 | PDE Loss:  -5.3421 | Function Loss:  -3.647\n",
      "Total loss:  -3.5673 | PDE Loss:  -5.3414 | Function Loss:  -3.6473\n",
      "Total loss:  -3.5674 | PDE Loss:  -5.3407 | Function Loss:  -3.6476\n",
      "Total loss:  -3.5675 | PDE Loss:  -5.3391 | Function Loss:  -3.648\n",
      "Total loss:  -3.5677 | PDE Loss:  -5.3395 | Function Loss:  -3.6482\n",
      "Total loss:  -3.5678 | PDE Loss:  -5.3399 | Function Loss:  -3.6482\n",
      "Total loss:  -3.5679 | PDE Loss:  -5.3412 | Function Loss:  -3.6481\n",
      "Total loss:  -3.568 | PDE Loss:  -5.3415 | Function Loss:  -3.6481\n",
      "Total loss:  -3.5681 | PDE Loss:  -5.343 | Function Loss:  -3.648\n",
      "Total loss:  -3.5683 | PDE Loss:  -5.3441 | Function Loss:  -3.6479\n",
      "Total loss:  -3.5685 | PDE Loss:  -5.3465 | Function Loss:  -3.6477\n",
      "Total loss:  -3.5687 | PDE Loss:  -5.3481 | Function Loss:  -3.6476\n",
      "Total loss:  -3.5688 | PDE Loss:  -5.3493 | Function Loss:  -3.6475\n",
      "Total loss:  -3.569 | PDE Loss:  -5.3507 | Function Loss:  -3.6475\n",
      "Total loss:  -3.5692 | PDE Loss:  -5.3511 | Function Loss:  -3.6476\n",
      "Total loss:  -3.5694 | PDE Loss:  -5.3524 | Function Loss:  -3.6476\n",
      "Total loss:  -3.5696 | PDE Loss:  -5.3511 | Function Loss:  -3.6481\n",
      "Total loss:  -3.5698 | PDE Loss:  -5.3507 | Function Loss:  -3.6484\n",
      "Total loss:  -3.5702 | PDE Loss:  -5.3496 | Function Loss:  -3.6491\n",
      "Total loss:  -3.5705 | PDE Loss:  -5.3474 | Function Loss:  -3.6499\n",
      "Total loss:  -3.5706 | PDE Loss:  -5.3467 | Function Loss:  -3.6502\n",
      "Total loss:  -3.5708 | PDE Loss:  -5.3457 | Function Loss:  -3.6506\n",
      "Total loss:  -3.5709 | PDE Loss:  -5.3455 | Function Loss:  -3.6508\n",
      "Total loss:  -3.571 | PDE Loss:  -5.3448 | Function Loss:  -3.6511\n",
      "Total loss:  -3.5712 | PDE Loss:  -5.3448 | Function Loss:  -3.6513\n",
      "Total loss:  -3.5713 | PDE Loss:  -5.3442 | Function Loss:  -3.6516\n",
      "Total loss:  -3.5715 | PDE Loss:  -5.344 | Function Loss:  -3.6519\n",
      "Total loss:  -3.5718 | PDE Loss:  -5.3429 | Function Loss:  -3.6524\n",
      "Total loss:  -3.572 | PDE Loss:  -5.3431 | Function Loss:  -3.6526\n",
      "Total loss:  -3.5722 | PDE Loss:  -5.3419 | Function Loss:  -3.653\n",
      "Total loss:  -3.5723 | PDE Loss:  -5.341 | Function Loss:  -3.6534\n",
      "Total loss:  -3.5726 | PDE Loss:  -5.3394 | Function Loss:  -3.654\n",
      "Total loss:  -3.5728 | PDE Loss:  -5.3387 | Function Loss:  -3.6544\n",
      "Total loss:  -3.573 | PDE Loss:  -5.3361 | Function Loss:  -3.6552\n",
      "Total loss:  -3.5732 | PDE Loss:  -5.3355 | Function Loss:  -3.6556\n",
      "Total loss:  -3.5734 | PDE Loss:  -5.3343 | Function Loss:  -3.656\n",
      "Total loss:  -3.5736 | PDE Loss:  -5.3343 | Function Loss:  -3.6563\n",
      "Total loss:  -3.5737 | PDE Loss:  -5.3342 | Function Loss:  -3.6565\n",
      "Total loss:  -3.5738 | PDE Loss:  -5.335 | Function Loss:  -3.6565\n",
      "Total loss:  -3.5739 | PDE Loss:  -5.3362 | Function Loss:  -3.6563\n",
      "Total loss:  -3.574 | PDE Loss:  -5.3375 | Function Loss:  -3.6562\n",
      "Total loss:  -3.5741 | PDE Loss:  -5.3396 | Function Loss:  -3.6559\n",
      "Total loss:  -3.5741 | PDE Loss:  -5.341 | Function Loss:  -3.6556\n",
      "Total loss:  -3.5742 | PDE Loss:  -5.3425 | Function Loss:  -3.6554\n",
      "Total loss:  -3.5743 | PDE Loss:  -5.3451 | Function Loss:  -3.655\n",
      "Total loss:  -3.5744 | PDE Loss:  -5.3467 | Function Loss:  -3.6548\n",
      "Total loss:  -3.5746 | PDE Loss:  -5.3498 | Function Loss:  -3.6543\n",
      "Total loss:  -3.5747 | PDE Loss:  -5.3506 | Function Loss:  -3.6543\n",
      "Total loss:  -3.5749 | PDE Loss:  -5.3529 | Function Loss:  -3.6541\n",
      "Total loss:  -3.575 | PDE Loss:  -5.3534 | Function Loss:  -3.6542\n",
      "Total loss:  -3.5753 | PDE Loss:  -5.354 | Function Loss:  -3.6544\n",
      "Total loss:  -3.5756 | PDE Loss:  -5.3539 | Function Loss:  -3.6547\n",
      "Total loss:  -3.576 | PDE Loss:  -5.3544 | Function Loss:  -3.6551\n",
      "Total loss:  -3.5764 | PDE Loss:  -5.3547 | Function Loss:  -3.6555\n",
      "Total loss:  -3.5768 | PDE Loss:  -5.3548 | Function Loss:  -3.656\n",
      "Total loss:  -3.5773 | PDE Loss:  -5.3564 | Function Loss:  -3.6563\n",
      "Total loss:  -3.5777 | PDE Loss:  -5.3567 | Function Loss:  -3.6567\n",
      "Total loss:  -3.5782 | PDE Loss:  -5.358 | Function Loss:  -3.657\n",
      "Total loss:  -3.5786 | PDE Loss:  -5.3586 | Function Loss:  -3.6574\n",
      "Total loss:  -3.5789 | PDE Loss:  -5.3605 | Function Loss:  -3.6574\n",
      "Total loss:  -3.5792 | PDE Loss:  -5.3606 | Function Loss:  -3.6577\n",
      "Total loss:  -3.5793 | PDE Loss:  -5.3611 | Function Loss:  -3.6578\n",
      "Total loss:  -3.5795 | PDE Loss:  -5.3611 | Function Loss:  -3.6581\n",
      "Total loss:  -3.5797 | PDE Loss:  -5.3604 | Function Loss:  -3.6584\n",
      "Total loss:  -3.5799 | PDE Loss:  -5.3598 | Function Loss:  -3.6588\n",
      "Total loss:  -3.5801 | PDE Loss:  -5.3584 | Function Loss:  -3.6592\n",
      "Total loss:  -3.5802 | PDE Loss:  -5.3571 | Function Loss:  -3.6597\n",
      "Total loss:  -3.5803 | PDE Loss:  -5.3558 | Function Loss:  -3.66\n",
      "Total loss:  -3.5805 | PDE Loss:  -5.3543 | Function Loss:  -3.6605\n",
      "Total loss:  -3.5805 | PDE Loss:  -5.3538 | Function Loss:  -3.6607\n",
      "Total loss:  -3.5807 | PDE Loss:  -5.3536 | Function Loss:  -3.6609\n",
      "Total loss:  -3.5808 | PDE Loss:  -5.3536 | Function Loss:  -3.6611\n",
      "Total loss:  -3.5809 | PDE Loss:  -5.3539 | Function Loss:  -3.6611\n",
      "Total loss:  -3.581 | PDE Loss:  -5.3546 | Function Loss:  -3.6611\n",
      "Total loss:  -3.5811 | PDE Loss:  -5.3551 | Function Loss:  -3.6611\n",
      "Total loss:  -3.5813 | PDE Loss:  -5.3563 | Function Loss:  -3.6611\n",
      "Total loss:  -3.5814 | PDE Loss:  -5.3568 | Function Loss:  -3.6612\n",
      "Total loss:  -3.5815 | PDE Loss:  -5.3572 | Function Loss:  -3.6612\n",
      "Total loss:  -3.5817 | PDE Loss:  -5.3574 | Function Loss:  -3.6613\n",
      "Total loss:  -3.5818 | PDE Loss:  -5.357 | Function Loss:  -3.6615\n",
      "Total loss:  -3.5819 | PDE Loss:  -5.3573 | Function Loss:  -3.6616\n",
      "Total loss:  -3.582 | PDE Loss:  -5.3564 | Function Loss:  -3.6619\n",
      "Total loss:  -3.5821 | PDE Loss:  -5.3566 | Function Loss:  -3.662\n",
      "Total loss:  -3.5822 | PDE Loss:  -5.3567 | Function Loss:  -3.6621\n",
      "Total loss:  -3.5823 | PDE Loss:  -5.3571 | Function Loss:  -3.6621\n",
      "Total loss:  -3.5824 | PDE Loss:  -5.3578 | Function Loss:  -3.6621\n",
      "Total loss:  -3.5825 | PDE Loss:  -5.3586 | Function Loss:  -3.6621\n",
      "Total loss:  -3.5826 | PDE Loss:  -5.3598 | Function Loss:  -3.662\n",
      "Total loss:  -3.5827 | PDE Loss:  -5.3612 | Function Loss:  -3.6618\n",
      "Total loss:  -3.5829 | PDE Loss:  -5.3631 | Function Loss:  -3.6616\n",
      "Total loss:  -3.583 | PDE Loss:  -5.3649 | Function Loss:  -3.6614\n",
      "Total loss:  -3.5831 | PDE Loss:  -5.3671 | Function Loss:  -3.6611\n",
      "Total loss:  -3.5832 | PDE Loss:  -5.369 | Function Loss:  -3.6609\n",
      "Total loss:  -3.5834 | PDE Loss:  -5.3715 | Function Loss:  -3.6606\n",
      "Total loss:  -3.5835 | PDE Loss:  -5.3738 | Function Loss:  -3.6603\n",
      "Total loss:  -3.5837 | PDE Loss:  -5.3765 | Function Loss:  -3.66\n",
      "Total loss:  -3.5838 | PDE Loss:  -5.3782 | Function Loss:  -3.6598\n",
      "Total loss:  -3.584 | PDE Loss:  -5.3797 | Function Loss:  -3.6597\n",
      "Total loss:  -3.5841 | PDE Loss:  -5.3803 | Function Loss:  -3.6598\n",
      "Total loss:  -3.5842 | PDE Loss:  -5.3806 | Function Loss:  -3.6599\n",
      "Total loss:  -3.5844 | PDE Loss:  -5.3804 | Function Loss:  -3.6601\n",
      "Total loss:  -3.5845 | PDE Loss:  -5.3796 | Function Loss:  -3.6604\n",
      "Total loss:  -3.5846 | PDE Loss:  -5.3789 | Function Loss:  -3.6606\n",
      "Total loss:  -3.5847 | PDE Loss:  -5.3781 | Function Loss:  -3.6609\n",
      "Total loss:  -3.5848 | PDE Loss:  -5.3773 | Function Loss:  -3.6612\n",
      "Total loss:  -3.5849 | PDE Loss:  -5.3767 | Function Loss:  -3.6615\n",
      "Total loss:  -3.5851 | PDE Loss:  -5.3759 | Function Loss:  -3.6618\n",
      "Total loss:  -3.5852 | PDE Loss:  -5.3756 | Function Loss:  -3.662\n",
      "Total loss:  -3.5854 | PDE Loss:  -5.3756 | Function Loss:  -3.6622\n",
      "Total loss:  -3.5855 | PDE Loss:  -5.3756 | Function Loss:  -3.6624\n",
      "Total loss:  -3.5857 | PDE Loss:  -5.3761 | Function Loss:  -3.6625\n",
      "Total loss:  -3.5858 | PDE Loss:  -5.3763 | Function Loss:  -3.6626\n",
      "Total loss:  -3.5859 | PDE Loss:  -5.3768 | Function Loss:  -3.6626\n",
      "Total loss:  -3.586 | PDE Loss:  -5.377 | Function Loss:  -3.6627\n",
      "Total loss:  -3.5861 | PDE Loss:  -5.3771 | Function Loss:  -3.6628\n",
      "Total loss:  -3.5862 | PDE Loss:  -5.377 | Function Loss:  -3.6629\n",
      "Total loss:  -3.5863 | PDE Loss:  -5.3769 | Function Loss:  -3.663\n",
      "Total loss:  -3.5865 | PDE Loss:  -5.3768 | Function Loss:  -3.6633\n",
      "Total loss:  -3.5867 | PDE Loss:  -5.3765 | Function Loss:  -3.6636\n",
      "Total loss:  -3.5869 | PDE Loss:  -5.3748 | Function Loss:  -3.6641\n",
      "Total loss:  -3.5871 | PDE Loss:  -5.3752 | Function Loss:  -3.6643\n",
      "Total loss:  -3.5873 | PDE Loss:  -5.3758 | Function Loss:  -3.6645\n",
      "Total loss:  -3.5875 | PDE Loss:  -5.3766 | Function Loss:  -3.6645\n",
      "Total loss:  -3.5876 | PDE Loss:  -5.3773 | Function Loss:  -3.6646\n",
      "Total loss:  -3.5877 | PDE Loss:  -5.3779 | Function Loss:  -3.6646\n",
      "Total loss:  -3.5878 | PDE Loss:  -5.3785 | Function Loss:  -3.6645\n",
      "Total loss:  -3.5879 | PDE Loss:  -5.3789 | Function Loss:  -3.6646\n",
      "Total loss:  -3.588 | PDE Loss:  -5.3791 | Function Loss:  -3.6647\n",
      "Total loss:  -3.5881 | PDE Loss:  -5.3788 | Function Loss:  -3.6649\n",
      "Total loss:  -3.5883 | PDE Loss:  -5.3787 | Function Loss:  -3.665\n",
      "Total loss:  -3.5884 | PDE Loss:  -5.3776 | Function Loss:  -3.6654\n",
      "Total loss:  -3.5885 | PDE Loss:  -5.3774 | Function Loss:  -3.6655\n",
      "Total loss:  -3.5886 | PDE Loss:  -5.3771 | Function Loss:  -3.6657\n",
      "Total loss:  -3.5887 | PDE Loss:  -5.3771 | Function Loss:  -3.6658\n",
      "Total loss:  -3.5888 | PDE Loss:  -5.3766 | Function Loss:  -3.6661\n",
      "Total loss:  -3.5889 | PDE Loss:  -5.3777 | Function Loss:  -3.666\n",
      "Total loss:  -3.589 | PDE Loss:  -5.3775 | Function Loss:  -3.6661\n",
      "Total loss:  -3.5891 | PDE Loss:  -5.3781 | Function Loss:  -3.6662\n",
      "Total loss:  -3.5893 | PDE Loss:  -5.3788 | Function Loss:  -3.6662\n",
      "Total loss:  -3.5894 | PDE Loss:  -5.3798 | Function Loss:  -3.6662\n",
      "Total loss:  -3.5895 | PDE Loss:  -5.3806 | Function Loss:  -3.6662\n",
      "Total loss:  -3.5896 | PDE Loss:  -5.3815 | Function Loss:  -3.6661\n",
      "Total loss:  -3.5897 | PDE Loss:  -5.3821 | Function Loss:  -3.6661\n",
      "Total loss:  -3.5898 | PDE Loss:  -5.3826 | Function Loss:  -3.6662\n",
      "Total loss:  -3.59 | PDE Loss:  -5.3837 | Function Loss:  -3.6661\n",
      "Total loss:  -3.5901 | PDE Loss:  -5.3842 | Function Loss:  -3.6662\n",
      "Total loss:  -3.5903 | PDE Loss:  -5.3846 | Function Loss:  -3.6663\n",
      "Total loss:  -3.5904 | PDE Loss:  -5.3849 | Function Loss:  -3.6664\n",
      "Total loss:  -3.5905 | PDE Loss:  -5.3848 | Function Loss:  -3.6665\n",
      "Total loss:  -3.5906 | PDE Loss:  -5.3849 | Function Loss:  -3.6666\n",
      "Total loss:  -3.5907 | PDE Loss:  -5.3852 | Function Loss:  -3.6667\n",
      "Total loss:  -3.5908 | PDE Loss:  -5.3853 | Function Loss:  -3.6668\n",
      "Total loss:  -3.5909 | PDE Loss:  -5.3858 | Function Loss:  -3.6668\n",
      "Total loss:  -3.591 | PDE Loss:  -5.3863 | Function Loss:  -3.6669\n",
      "Total loss:  -3.5912 | PDE Loss:  -5.3875 | Function Loss:  -3.6669\n",
      "Total loss:  -3.5914 | PDE Loss:  -5.389 | Function Loss:  -3.6669\n",
      "Total loss:  -3.5917 | PDE Loss:  -5.3907 | Function Loss:  -3.6668\n",
      "Total loss:  -3.5919 | PDE Loss:  -5.3926 | Function Loss:  -3.6667\n",
      "Total loss:  -3.5921 | PDE Loss:  -5.3948 | Function Loss:  -3.6665\n",
      "Total loss:  -3.5923 | PDE Loss:  -5.3956 | Function Loss:  -3.6666\n",
      "Total loss:  -3.5925 | PDE Loss:  -5.3965 | Function Loss:  -3.6667\n",
      "Total loss:  -3.5927 | PDE Loss:  -5.3968 | Function Loss:  -3.6669\n",
      "Total loss:  -3.5929 | PDE Loss:  -5.3967 | Function Loss:  -3.6671\n",
      "Total loss:  -3.593 | PDE Loss:  -5.3968 | Function Loss:  -3.6673\n",
      "Total loss:  -3.5932 | PDE Loss:  -5.3963 | Function Loss:  -3.6676\n",
      "Total loss:  -3.5934 | PDE Loss:  -5.3963 | Function Loss:  -3.6678\n",
      "Total loss:  -3.5935 | PDE Loss:  -5.3959 | Function Loss:  -3.6681\n",
      "Total loss:  -3.5937 | PDE Loss:  -5.3956 | Function Loss:  -3.6683\n",
      "Total loss:  -3.5939 | PDE Loss:  -5.3954 | Function Loss:  -3.6686\n",
      "Total loss:  -3.5941 | PDE Loss:  -5.3953 | Function Loss:  -3.6688\n",
      "Total loss:  -3.5943 | PDE Loss:  -5.3952 | Function Loss:  -3.669\n",
      "Total loss:  -3.5944 | PDE Loss:  -5.3951 | Function Loss:  -3.6692\n",
      "Total loss:  -3.5946 | PDE Loss:  -5.395 | Function Loss:  -3.6695\n",
      "Total loss:  -3.5947 | PDE Loss:  -5.3939 | Function Loss:  -3.6698\n",
      "Total loss:  -3.5949 | PDE Loss:  -5.3933 | Function Loss:  -3.6702\n",
      "Total loss:  -3.5951 | PDE Loss:  -5.3925 | Function Loss:  -3.6705\n",
      "Total loss:  -3.5953 | PDE Loss:  -5.3921 | Function Loss:  -3.6708\n",
      "Total loss:  -3.5955 | PDE Loss:  -5.3924 | Function Loss:  -3.6711\n",
      "Total loss:  -3.5958 | PDE Loss:  -5.3925 | Function Loss:  -3.6713\n",
      "Total loss:  -3.596 | PDE Loss:  -5.394 | Function Loss:  -3.6713\n",
      "Total loss:  -3.5962 | PDE Loss:  -5.3956 | Function Loss:  -3.6713\n",
      "Total loss:  -3.5965 | PDE Loss:  -5.3984 | Function Loss:  -3.6711\n",
      "Total loss:  -3.5969 | PDE Loss:  -5.4029 | Function Loss:  -3.6707\n",
      "Total loss:  -3.5973 | PDE Loss:  -5.4073 | Function Loss:  -3.6704\n",
      "Total loss:  -3.5978 | PDE Loss:  -5.4141 | Function Loss:  -3.6698\n",
      "Total loss:  -3.5983 | PDE Loss:  -5.4185 | Function Loss:  -3.6695\n",
      "Total loss:  -3.5987 | PDE Loss:  -5.4224 | Function Loss:  -3.6693\n",
      "Total loss:  -3.5992 | PDE Loss:  -5.4262 | Function Loss:  -3.6692\n",
      "Total loss:  -3.5996 | PDE Loss:  -5.4276 | Function Loss:  -3.6694\n",
      "Total loss:  -3.6 | PDE Loss:  -5.4266 | Function Loss:  -3.6702\n",
      "Total loss:  -3.6005 | PDE Loss:  -5.4256 | Function Loss:  -3.6708\n",
      "Total loss:  -3.6008 | PDE Loss:  -5.4229 | Function Loss:  -3.6718\n",
      "Total loss:  -3.6012 | PDE Loss:  -5.4202 | Function Loss:  -3.6727\n",
      "Total loss:  -3.6016 | PDE Loss:  -5.4179 | Function Loss:  -3.6735\n",
      "Total loss:  -3.6019 | PDE Loss:  -5.4173 | Function Loss:  -3.674\n",
      "Total loss:  -3.6023 | PDE Loss:  -5.4179 | Function Loss:  -3.6744\n",
      "Total loss:  -3.6027 | PDE Loss:  -5.4197 | Function Loss:  -3.6745\n",
      "Total loss:  -3.6031 | PDE Loss:  -5.4224 | Function Loss:  -3.6745\n",
      "Total loss:  -3.6034 | PDE Loss:  -5.4262 | Function Loss:  -3.6742\n",
      "Total loss:  -3.6037 | PDE Loss:  -5.4291 | Function Loss:  -3.674\n",
      "Total loss:  -3.6038 | PDE Loss:  -5.4317 | Function Loss:  -3.6737\n",
      "Total loss:  -3.604 | PDE Loss:  -5.4329 | Function Loss:  -3.6737\n",
      "Total loss:  -3.6042 | PDE Loss:  -5.4344 | Function Loss:  -3.6737\n",
      "Total loss:  -3.6044 | PDE Loss:  -5.4352 | Function Loss:  -3.6737\n",
      "Total loss:  -3.6046 | PDE Loss:  -5.4365 | Function Loss:  -3.6738\n",
      "Total loss:  -3.6048 | PDE Loss:  -5.4358 | Function Loss:  -3.6742\n",
      "Total loss:  -3.605 | PDE Loss:  -5.4352 | Function Loss:  -3.6745\n",
      "Total loss:  -3.6052 | PDE Loss:  -5.4347 | Function Loss:  -3.6748\n",
      "Total loss:  -3.6054 | PDE Loss:  -5.4337 | Function Loss:  -3.6752\n",
      "Total loss:  -3.6056 | PDE Loss:  -5.4336 | Function Loss:  -3.6754\n",
      "Total loss:  -3.6057 | PDE Loss:  -5.4339 | Function Loss:  -3.6756\n",
      "Total loss:  -3.6059 | PDE Loss:  -5.4351 | Function Loss:  -3.6755\n",
      "Total loss:  -3.606 | PDE Loss:  -5.4372 | Function Loss:  -3.6754\n",
      "Total loss:  -3.6062 | PDE Loss:  -5.44 | Function Loss:  -3.6751\n",
      "Total loss:  -3.6064 | PDE Loss:  -5.4434 | Function Loss:  -3.6747\n",
      "Total loss:  -3.6066 | PDE Loss:  -5.4465 | Function Loss:  -3.6744\n",
      "Total loss:  -3.6067 | PDE Loss:  -5.4496 | Function Loss:  -3.674\n",
      "Total loss:  -3.6069 | PDE Loss:  -5.4522 | Function Loss:  -3.6738\n",
      "Total loss:  -3.607 | PDE Loss:  -5.4544 | Function Loss:  -3.6736\n",
      "Total loss:  -3.6072 | PDE Loss:  -5.4559 | Function Loss:  -3.6735\n",
      "Total loss:  -3.6073 | PDE Loss:  -5.4569 | Function Loss:  -3.6735\n",
      "Total loss:  -3.6074 | PDE Loss:  -5.457 | Function Loss:  -3.6736\n",
      "Total loss:  -3.6076 | PDE Loss:  -5.457 | Function Loss:  -3.6738\n",
      "Total loss:  -3.6077 | PDE Loss:  -5.4559 | Function Loss:  -3.6742\n",
      "Total loss:  -3.6079 | PDE Loss:  -5.4549 | Function Loss:  -3.6745\n",
      "Total loss:  -3.608 | PDE Loss:  -5.4545 | Function Loss:  -3.6748\n",
      "Total loss:  -3.6082 | PDE Loss:  -5.4538 | Function Loss:  -3.675\n",
      "Total loss:  -3.6083 | PDE Loss:  -5.4543 | Function Loss:  -3.6751\n",
      "Total loss:  -3.6085 | PDE Loss:  -5.4543 | Function Loss:  -3.6753\n",
      "Total loss:  -3.6087 | PDE Loss:  -5.4565 | Function Loss:  -3.6752\n",
      "Total loss:  -3.6089 | PDE Loss:  -5.458 | Function Loss:  -3.6752\n",
      "Total loss:  -3.6091 | PDE Loss:  -5.4595 | Function Loss:  -3.6751\n",
      "Total loss:  -3.6092 | PDE Loss:  -5.4614 | Function Loss:  -3.675\n",
      "Total loss:  -3.6093 | PDE Loss:  -5.4625 | Function Loss:  -3.6749\n",
      "Total loss:  -3.6094 | PDE Loss:  -5.4633 | Function Loss:  -3.6749\n",
      "Total loss:  -3.6095 | PDE Loss:  -5.464 | Function Loss:  -3.6749\n",
      "Total loss:  -3.6096 | PDE Loss:  -5.4646 | Function Loss:  -3.6749\n",
      "Total loss:  -3.6098 | PDE Loss:  -5.4657 | Function Loss:  -3.675\n",
      "Total loss:  -3.6099 | PDE Loss:  -5.4649 | Function Loss:  -3.6753\n",
      "Total loss:  -3.6101 | PDE Loss:  -5.4657 | Function Loss:  -3.6753\n",
      "Total loss:  -3.6104 | PDE Loss:  -5.466 | Function Loss:  -3.6756\n",
      "Total loss:  -3.6106 | PDE Loss:  -5.4659 | Function Loss:  -3.6758\n",
      "Total loss:  -3.6107 | PDE Loss:  -5.4652 | Function Loss:  -3.6761\n",
      "Total loss:  -3.6109 | PDE Loss:  -5.4642 | Function Loss:  -3.6765\n",
      "Total loss:  -3.611 | PDE Loss:  -5.463 | Function Loss:  -3.6768\n",
      "Total loss:  -3.6111 | PDE Loss:  -5.4615 | Function Loss:  -3.6772\n",
      "Total loss:  -3.6113 | PDE Loss:  -5.4598 | Function Loss:  -3.6776\n",
      "Total loss:  -3.6113 | PDE Loss:  -5.4562 | Function Loss:  -3.6783\n",
      "Total loss:  -3.6115 | PDE Loss:  -5.4562 | Function Loss:  -3.6785\n",
      "Total loss:  -3.6116 | PDE Loss:  -5.4565 | Function Loss:  -3.6786\n",
      "Total loss:  -3.6118 | PDE Loss:  -5.4565 | Function Loss:  -3.6788\n",
      "Total loss:  -3.6119 | PDE Loss:  -5.4569 | Function Loss:  -3.6788\n",
      "Total loss:  -3.6119 | PDE Loss:  -5.4572 | Function Loss:  -3.6789\n",
      "Total loss:  -3.612 | PDE Loss:  -5.4578 | Function Loss:  -3.6789\n",
      "Total loss:  -3.6121 | PDE Loss:  -5.4587 | Function Loss:  -3.6788\n",
      "Total loss:  -3.6122 | PDE Loss:  -5.4588 | Function Loss:  -3.6789\n",
      "Total loss:  -3.6124 | PDE Loss:  -5.4616 | Function Loss:  -3.6787\n",
      "Total loss:  -3.6125 | PDE Loss:  -5.4618 | Function Loss:  -3.6787\n",
      "Total loss:  -3.6127 | PDE Loss:  -5.4616 | Function Loss:  -3.6791\n",
      "Total loss:  -3.613 | PDE Loss:  -5.46 | Function Loss:  -3.6796\n",
      "Total loss:  -3.6132 | PDE Loss:  -5.4574 | Function Loss:  -3.6803\n",
      "Total loss:  -3.6134 | PDE Loss:  -5.4538 | Function Loss:  -3.6811\n",
      "Total loss:  -3.6135 | PDE Loss:  -5.4503 | Function Loss:  -3.6819\n",
      "Total loss:  -3.6137 | PDE Loss:  -5.447 | Function Loss:  -3.6826\n",
      "Total loss:  -3.6138 | PDE Loss:  -5.444 | Function Loss:  -3.6833\n",
      "Total loss:  -3.614 | PDE Loss:  -5.4415 | Function Loss:  -3.6839\n",
      "Total loss:  -3.6142 | PDE Loss:  -5.4386 | Function Loss:  -3.6846\n",
      "Total loss:  -3.6143 | PDE Loss:  -5.4382 | Function Loss:  -3.6849\n",
      "Total loss:  -3.6145 | PDE Loss:  -5.4384 | Function Loss:  -3.6851\n",
      "Total loss:  -3.6147 | PDE Loss:  -5.4396 | Function Loss:  -3.6851\n",
      "Total loss:  -3.6149 | PDE Loss:  -5.4408 | Function Loss:  -3.6851\n",
      "Total loss:  -3.6151 | PDE Loss:  -5.4432 | Function Loss:  -3.6849\n",
      "Total loss:  -3.6153 | PDE Loss:  -5.4451 | Function Loss:  -3.6848\n",
      "Total loss:  -3.6154 | PDE Loss:  -5.4472 | Function Loss:  -3.6846\n",
      "Total loss:  -3.6156 | PDE Loss:  -5.4492 | Function Loss:  -3.6845\n",
      "Total loss:  -3.6158 | PDE Loss:  -5.4503 | Function Loss:  -3.6845\n",
      "Total loss:  -3.616 | PDE Loss:  -5.4527 | Function Loss:  -3.6843\n",
      "Total loss:  -3.6161 | PDE Loss:  -5.4528 | Function Loss:  -3.6844\n",
      "Total loss:  -3.6162 | PDE Loss:  -5.4529 | Function Loss:  -3.6846\n",
      "Total loss:  -3.6164 | PDE Loss:  -5.4521 | Function Loss:  -3.6849\n",
      "Total loss:  -3.6165 | PDE Loss:  -5.4515 | Function Loss:  -3.6851\n",
      "Total loss:  -3.6167 | PDE Loss:  -5.451 | Function Loss:  -3.6854\n",
      "Total loss:  -3.6169 | PDE Loss:  -5.4507 | Function Loss:  -3.6858\n",
      "Total loss:  -3.6171 | PDE Loss:  -5.4511 | Function Loss:  -3.6859\n",
      "Total loss:  -3.6173 | PDE Loss:  -5.452 | Function Loss:  -3.686\n",
      "Total loss:  -3.6175 | PDE Loss:  -5.4536 | Function Loss:  -3.6859\n",
      "Total loss:  -3.6177 | PDE Loss:  -5.4543 | Function Loss:  -3.6861\n",
      "Total loss:  -3.6179 | PDE Loss:  -5.456 | Function Loss:  -3.686\n",
      "Total loss:  -3.6181 | PDE Loss:  -5.457 | Function Loss:  -3.686\n",
      "Total loss:  -3.6182 | PDE Loss:  -5.458 | Function Loss:  -3.686\n",
      "Total loss:  -3.6182 | PDE Loss:  -5.4587 | Function Loss:  -3.686\n",
      "Total loss:  -3.6184 | PDE Loss:  -5.4594 | Function Loss:  -3.686\n",
      "Total loss:  -3.6186 | PDE Loss:  -5.4601 | Function Loss:  -3.6861\n",
      "Total loss:  -3.6188 | PDE Loss:  -5.4609 | Function Loss:  -3.6862\n",
      "Total loss:  -3.619 | PDE Loss:  -5.462 | Function Loss:  -3.6863\n",
      "Total loss:  -3.6192 | PDE Loss:  -5.462 | Function Loss:  -3.6865\n",
      "Total loss:  -3.6194 | PDE Loss:  -5.4638 | Function Loss:  -3.6865\n",
      "Total loss:  -3.6195 | PDE Loss:  -5.4629 | Function Loss:  -3.6868\n",
      "Total loss:  -3.6196 | PDE Loss:  -5.4628 | Function Loss:  -3.6869\n",
      "Total loss:  -3.6198 | PDE Loss:  -5.4625 | Function Loss:  -3.6871\n",
      "Total loss:  -3.6199 | PDE Loss:  -5.4627 | Function Loss:  -3.6872\n",
      "Total loss:  -3.6201 | PDE Loss:  -5.4637 | Function Loss:  -3.6872\n",
      "Total loss:  -3.6202 | PDE Loss:  -5.4639 | Function Loss:  -3.6874\n",
      "Total loss:  -3.6203 | PDE Loss:  -5.4649 | Function Loss:  -3.6874\n",
      "Total loss:  -3.6204 | PDE Loss:  -5.4659 | Function Loss:  -3.6873\n",
      "Total loss:  -3.6206 | PDE Loss:  -5.467 | Function Loss:  -3.6873\n",
      "Total loss:  -3.6207 | PDE Loss:  -5.4679 | Function Loss:  -3.6873\n",
      "Total loss:  -3.6209 | PDE Loss:  -5.4686 | Function Loss:  -3.6874\n",
      "Total loss:  -3.621 | PDE Loss:  -5.4692 | Function Loss:  -3.6874\n",
      "Total loss:  -3.6212 | PDE Loss:  -5.4697 | Function Loss:  -3.6876\n",
      "Total loss:  -3.6214 | PDE Loss:  -5.4701 | Function Loss:  -3.6878\n",
      "Total loss:  -3.6217 | PDE Loss:  -5.4711 | Function Loss:  -3.6879\n",
      "Total loss:  -3.622 | PDE Loss:  -5.4716 | Function Loss:  -3.6882\n",
      "Total loss:  -3.6223 | PDE Loss:  -5.4734 | Function Loss:  -3.6883\n",
      "Total loss:  -3.6225 | PDE Loss:  -5.4739 | Function Loss:  -3.6884\n",
      "Total loss:  -3.6227 | PDE Loss:  -5.4763 | Function Loss:  -3.6883\n",
      "Total loss:  -3.6229 | PDE Loss:  -5.4765 | Function Loss:  -3.6885\n",
      "Total loss:  -3.6231 | PDE Loss:  -5.4802 | Function Loss:  -3.6881\n",
      "Total loss:  -3.6233 | PDE Loss:  -5.48 | Function Loss:  -3.6883\n",
      "Total loss:  -3.6235 | PDE Loss:  -5.48 | Function Loss:  -3.6885\n",
      "Total loss:  -3.6237 | PDE Loss:  -5.4803 | Function Loss:  -3.6888\n",
      "Total loss:  -3.6239 | PDE Loss:  -5.4797 | Function Loss:  -3.6891\n",
      "Total loss:  -3.624 | PDE Loss:  -5.4801 | Function Loss:  -3.6892\n",
      "Total loss:  -3.6241 | PDE Loss:  -5.4799 | Function Loss:  -3.6893\n",
      "Total loss:  -3.6242 | PDE Loss:  -5.4802 | Function Loss:  -3.6894\n",
      "Total loss:  -3.6243 | PDE Loss:  -5.4807 | Function Loss:  -3.6893\n",
      "Total loss:  -3.6244 | PDE Loss:  -5.4815 | Function Loss:  -3.6893\n",
      "Total loss:  -3.6245 | PDE Loss:  -5.4827 | Function Loss:  -3.6893\n",
      "Total loss:  -3.6246 | PDE Loss:  -5.4838 | Function Loss:  -3.6892\n",
      "Total loss:  -3.6247 | PDE Loss:  -5.4847 | Function Loss:  -3.6892\n",
      "Total loss:  -3.6249 | PDE Loss:  -5.4851 | Function Loss:  -3.6893\n",
      "Total loss:  -3.6251 | PDE Loss:  -5.4875 | Function Loss:  -3.6892\n",
      "Total loss:  -3.6252 | PDE Loss:  -5.4871 | Function Loss:  -3.6895\n",
      "Total loss:  -3.6255 | PDE Loss:  -5.4872 | Function Loss:  -3.6897\n",
      "Total loss:  -3.6258 | PDE Loss:  -5.4858 | Function Loss:  -3.6903\n",
      "Total loss:  -3.6261 | PDE Loss:  -5.4848 | Function Loss:  -3.6908\n",
      "Total loss:  -3.6264 | PDE Loss:  -5.4837 | Function Loss:  -3.6913\n",
      "Total loss:  -3.6266 | PDE Loss:  -5.4819 | Function Loss:  -3.6919\n",
      "Total loss:  -3.6269 | PDE Loss:  -5.4818 | Function Loss:  -3.6922\n",
      "Total loss:  -3.6271 | PDE Loss:  -5.4804 | Function Loss:  -3.6927\n",
      "Total loss:  -3.6272 | PDE Loss:  -5.4807 | Function Loss:  -3.6928\n",
      "Total loss:  -3.6274 | PDE Loss:  -5.4808 | Function Loss:  -3.693\n",
      "Total loss:  -3.6276 | PDE Loss:  -5.4817 | Function Loss:  -3.693\n",
      "Total loss:  -3.6278 | PDE Loss:  -5.482 | Function Loss:  -3.6933\n",
      "Total loss:  -3.6281 | PDE Loss:  -5.4839 | Function Loss:  -3.6933\n",
      "Total loss:  -3.6283 | PDE Loss:  -5.4854 | Function Loss:  -3.6933\n",
      "Total loss:  -3.6286 | PDE Loss:  -5.4867 | Function Loss:  -3.6934\n",
      "Total loss:  -3.6289 | PDE Loss:  -5.4889 | Function Loss:  -3.6934\n",
      "Total loss:  -3.6293 | PDE Loss:  -5.4921 | Function Loss:  -3.6933\n",
      "Total loss:  -3.6296 | PDE Loss:  -5.4941 | Function Loss:  -3.6933\n",
      "Total loss:  -3.6298 | PDE Loss:  -5.4951 | Function Loss:  -3.6935\n",
      "Total loss:  -3.63 | PDE Loss:  -5.4959 | Function Loss:  -3.6936\n",
      "Total loss:  -3.6302 | PDE Loss:  -5.4956 | Function Loss:  -3.6939\n",
      "Total loss:  -3.6304 | PDE Loss:  -5.4956 | Function Loss:  -3.694\n",
      "Total loss:  -3.6305 | PDE Loss:  -5.4946 | Function Loss:  -3.6944\n",
      "Total loss:  -3.6308 | PDE Loss:  -5.4943 | Function Loss:  -3.6947\n",
      "Total loss:  -3.6309 | PDE Loss:  -5.4942 | Function Loss:  -3.6949\n",
      "Total loss:  -3.6311 | PDE Loss:  -5.4941 | Function Loss:  -3.6952\n",
      "Total loss:  -3.6314 | PDE Loss:  -5.4953 | Function Loss:  -3.6953\n",
      "Total loss:  -3.6313 | PDE Loss:  -5.4967 | Function Loss:  -3.695\n",
      "Total loss:  -3.6315 | PDE Loss:  -5.4964 | Function Loss:  -3.6952\n",
      "Total loss:  -3.6317 | PDE Loss:  -5.4973 | Function Loss:  -3.6953\n",
      "Total loss:  -3.6318 | PDE Loss:  -5.499 | Function Loss:  -3.6952\n",
      "Total loss:  -3.6319 | PDE Loss:  -5.5007 | Function Loss:  -3.6951\n",
      "Total loss:  -3.6321 | PDE Loss:  -5.5026 | Function Loss:  -3.6949\n",
      "Total loss:  -3.6322 | PDE Loss:  -5.5046 | Function Loss:  -3.6948\n",
      "Total loss:  -3.6324 | PDE Loss:  -5.5067 | Function Loss:  -3.6946\n",
      "Total loss:  -3.6326 | PDE Loss:  -5.5091 | Function Loss:  -3.6945\n",
      "Total loss:  -3.6328 | PDE Loss:  -5.5108 | Function Loss:  -3.6945\n",
      "Total loss:  -3.633 | PDE Loss:  -5.5129 | Function Loss:  -3.6944\n",
      "Total loss:  -3.6333 | PDE Loss:  -5.5139 | Function Loss:  -3.6946\n",
      "Total loss:  -3.6336 | PDE Loss:  -5.516 | Function Loss:  -3.6947\n",
      "Total loss:  -3.634 | PDE Loss:  -5.5168 | Function Loss:  -3.695\n",
      "Total loss:  -3.6344 | PDE Loss:  -5.5184 | Function Loss:  -3.6951\n",
      "Total loss:  -3.6345 | PDE Loss:  -5.5173 | Function Loss:  -3.6954\n",
      "Total loss:  -3.6347 | PDE Loss:  -5.5169 | Function Loss:  -3.6958\n",
      "Total loss:  -3.6349 | PDE Loss:  -5.5179 | Function Loss:  -3.6958\n",
      "Total loss:  -3.635 | PDE Loss:  -5.5171 | Function Loss:  -3.6961\n",
      "Total loss:  -3.6351 | PDE Loss:  -5.5172 | Function Loss:  -3.6962\n",
      "Total loss:  -3.6353 | PDE Loss:  -5.5168 | Function Loss:  -3.6964\n",
      "Total loss:  -3.6354 | PDE Loss:  -5.5169 | Function Loss:  -3.6966\n",
      "Total loss:  -3.6355 | PDE Loss:  -5.5169 | Function Loss:  -3.6967\n",
      "Total loss:  -3.6357 | PDE Loss:  -5.5168 | Function Loss:  -3.6969\n",
      "Total loss:  -3.6359 | PDE Loss:  -5.5177 | Function Loss:  -3.697\n",
      "Total loss:  -3.6361 | PDE Loss:  -5.5178 | Function Loss:  -3.6972\n",
      "Total loss:  -3.6362 | PDE Loss:  -5.5186 | Function Loss:  -3.6972\n",
      "Total loss:  -3.6364 | PDE Loss:  -5.5189 | Function Loss:  -3.6974\n",
      "Total loss:  -3.6365 | PDE Loss:  -5.5211 | Function Loss:  -3.6972\n",
      "Total loss:  -3.6366 | PDE Loss:  -5.5206 | Function Loss:  -3.6974\n",
      "Total loss:  -3.6367 | PDE Loss:  -5.5223 | Function Loss:  -3.6973\n",
      "Total loss:  -3.6368 | PDE Loss:  -5.5225 | Function Loss:  -3.6973\n",
      "Total loss:  -3.6369 | PDE Loss:  -5.5234 | Function Loss:  -3.6973\n",
      "Total loss:  -3.637 | PDE Loss:  -5.5233 | Function Loss:  -3.6975\n",
      "Total loss:  -3.6372 | PDE Loss:  -5.5235 | Function Loss:  -3.6976\n",
      "Total loss:  -3.6373 | PDE Loss:  -5.5235 | Function Loss:  -3.6978\n",
      "Total loss:  -3.6375 | PDE Loss:  -5.5224 | Function Loss:  -3.6981\n",
      "Total loss:  -3.6377 | PDE Loss:  -5.5211 | Function Loss:  -3.6986\n",
      "Total loss:  -3.6378 | PDE Loss:  -5.5197 | Function Loss:  -3.6989\n",
      "Total loss:  -3.6379 | PDE Loss:  -5.5178 | Function Loss:  -3.6993\n",
      "Total loss:  -3.638 | PDE Loss:  -5.5167 | Function Loss:  -3.6996\n",
      "Total loss:  -3.6381 | PDE Loss:  -5.5149 | Function Loss:  -3.7\n",
      "Total loss:  -3.6383 | PDE Loss:  -5.5139 | Function Loss:  -3.7003\n",
      "Total loss:  -3.6384 | PDE Loss:  -5.5124 | Function Loss:  -3.7007\n",
      "Total loss:  -3.6385 | PDE Loss:  -5.5121 | Function Loss:  -3.7008\n",
      "Total loss:  -3.6385 | PDE Loss:  -5.5113 | Function Loss:  -3.701\n",
      "Total loss:  -3.6386 | PDE Loss:  -5.5112 | Function Loss:  -3.7011\n",
      "Total loss:  -3.6386 | PDE Loss:  -5.5111 | Function Loss:  -3.7012\n",
      "Total loss:  -3.6387 | PDE Loss:  -5.511 | Function Loss:  -3.7013\n",
      "Total loss:  -3.6388 | PDE Loss:  -5.511 | Function Loss:  -3.7014\n",
      "Total loss:  -3.6389 | PDE Loss:  -5.5108 | Function Loss:  -3.7015\n",
      "Total loss:  -3.639 | PDE Loss:  -5.5108 | Function Loss:  -3.7017\n",
      "Total loss:  -3.6391 | PDE Loss:  -5.5085 | Function Loss:  -3.7021\n",
      "Total loss:  -3.6391 | PDE Loss:  -5.5088 | Function Loss:  -3.7021\n",
      "Total loss:  -3.6393 | PDE Loss:  -5.5092 | Function Loss:  -3.7022\n",
      "Total loss:  -3.6394 | PDE Loss:  -5.5094 | Function Loss:  -3.7023\n",
      "Total loss:  -3.6395 | PDE Loss:  -5.5105 | Function Loss:  -3.7023\n",
      "Total loss:  -3.6396 | PDE Loss:  -5.511 | Function Loss:  -3.7024\n",
      "Total loss:  -3.6398 | PDE Loss:  -5.5122 | Function Loss:  -3.7023\n",
      "Total loss:  -3.6399 | PDE Loss:  -5.5134 | Function Loss:  -3.7023\n",
      "Total loss:  -3.6401 | PDE Loss:  -5.5152 | Function Loss:  -3.7023\n",
      "Total loss:  -3.6403 | PDE Loss:  -5.5168 | Function Loss:  -3.7022\n",
      "Total loss:  -3.6404 | PDE Loss:  -5.5175 | Function Loss:  -3.7022\n",
      "Total loss:  -3.6406 | PDE Loss:  -5.5182 | Function Loss:  -3.7023\n",
      "Total loss:  -3.6407 | PDE Loss:  -5.5188 | Function Loss:  -3.7024\n",
      "Total loss:  -3.6409 | PDE Loss:  -5.5196 | Function Loss:  -3.7025\n",
      "Total loss:  -3.6411 | PDE Loss:  -5.5209 | Function Loss:  -3.7025\n",
      "Total loss:  -3.6412 | PDE Loss:  -5.5217 | Function Loss:  -3.7025\n",
      "Total loss:  -3.6414 | PDE Loss:  -5.5234 | Function Loss:  -3.7024\n",
      "Total loss:  -3.6415 | PDE Loss:  -5.5252 | Function Loss:  -3.7024\n",
      "Total loss:  -3.6417 | PDE Loss:  -5.5269 | Function Loss:  -3.7023\n",
      "Total loss:  -3.642 | PDE Loss:  -5.5305 | Function Loss:  -3.7021\n",
      "Total loss:  -3.6422 | PDE Loss:  -5.5313 | Function Loss:  -3.7022\n",
      "Total loss:  -3.6423 | PDE Loss:  -5.5329 | Function Loss:  -3.7022\n",
      "Total loss:  -3.6426 | PDE Loss:  -5.5342 | Function Loss:  -3.7022\n",
      "Total loss:  -3.6428 | PDE Loss:  -5.5347 | Function Loss:  -3.7024\n",
      "Total loss:  -3.6429 | PDE Loss:  -5.5351 | Function Loss:  -3.7025\n",
      "Total loss:  -3.6431 | PDE Loss:  -5.5353 | Function Loss:  -3.7027\n",
      "Total loss:  -3.6433 | PDE Loss:  -5.5347 | Function Loss:  -3.7029\n",
      "Total loss:  -3.6434 | PDE Loss:  -5.5349 | Function Loss:  -3.703\n",
      "Total loss:  -3.6435 | PDE Loss:  -5.5345 | Function Loss:  -3.7032\n",
      "Total loss:  -3.6436 | PDE Loss:  -5.5347 | Function Loss:  -3.7033\n",
      "Total loss:  -3.6437 | PDE Loss:  -5.5347 | Function Loss:  -3.7034\n",
      "Total loss:  -3.6437 | PDE Loss:  -5.535 | Function Loss:  -3.7035\n",
      "Total loss:  -3.6438 | PDE Loss:  -5.5354 | Function Loss:  -3.7035\n",
      "Total loss:  -3.6439 | PDE Loss:  -5.5357 | Function Loss:  -3.7035\n",
      "Total loss:  -3.6439 | PDE Loss:  -5.5362 | Function Loss:  -3.7035\n",
      "Total loss:  -3.644 | PDE Loss:  -5.5367 | Function Loss:  -3.7035\n",
      "Total loss:  -3.6442 | PDE Loss:  -5.5379 | Function Loss:  -3.7035\n",
      "Total loss:  -3.6444 | PDE Loss:  -5.5395 | Function Loss:  -3.7035\n",
      "Total loss:  -3.6445 | PDE Loss:  -5.5408 | Function Loss:  -3.7035\n",
      "Total loss:  -3.6447 | PDE Loss:  -5.5429 | Function Loss:  -3.7034\n",
      "Total loss:  -3.6449 | PDE Loss:  -5.5451 | Function Loss:  -3.7033\n",
      "Total loss:  -3.6451 | PDE Loss:  -5.5464 | Function Loss:  -3.7033\n",
      "Total loss:  -3.6452 | PDE Loss:  -5.5471 | Function Loss:  -3.7033\n",
      "Total loss:  -3.6452 | PDE Loss:  -5.5469 | Function Loss:  -3.7034\n",
      "Total loss:  -3.6453 | PDE Loss:  -5.5472 | Function Loss:  -3.7035\n",
      "Total loss:  -3.6454 | PDE Loss:  -5.5468 | Function Loss:  -3.7036\n",
      "Total loss:  -3.6455 | PDE Loss:  -5.5467 | Function Loss:  -3.7038\n",
      "Total loss:  -3.6456 | PDE Loss:  -5.5462 | Function Loss:  -3.704\n",
      "Total loss:  -3.6458 | PDE Loss:  -5.5458 | Function Loss:  -3.7042\n",
      "Total loss:  -3.646 | PDE Loss:  -5.5449 | Function Loss:  -3.7046\n",
      "Total loss:  -3.6461 | PDE Loss:  -5.544 | Function Loss:  -3.7048\n",
      "Total loss:  -3.6463 | PDE Loss:  -5.5437 | Function Loss:  -3.7051\n",
      "Total loss:  -3.6465 | PDE Loss:  -5.5422 | Function Loss:  -3.7056\n",
      "Total loss:  -3.6467 | PDE Loss:  -5.5419 | Function Loss:  -3.7059\n",
      "Total loss:  -3.6469 | PDE Loss:  -5.5413 | Function Loss:  -3.7062\n",
      "Total loss:  -3.6471 | PDE Loss:  -5.5408 | Function Loss:  -3.7065\n",
      "Total loss:  -3.6474 | PDE Loss:  -5.5403 | Function Loss:  -3.7068\n",
      "Total loss:  -3.6477 | PDE Loss:  -5.5395 | Function Loss:  -3.7073\n",
      "Total loss:  -3.648 | PDE Loss:  -5.5386 | Function Loss:  -3.7078\n",
      "Total loss:  -3.6483 | PDE Loss:  -5.537 | Function Loss:  -3.7083\n",
      "Total loss:  -3.6485 | PDE Loss:  -5.5364 | Function Loss:  -3.7087\n",
      "Total loss:  -3.6486 | PDE Loss:  -5.5366 | Function Loss:  -3.7088\n",
      "Total loss:  -3.6488 | PDE Loss:  -5.5364 | Function Loss:  -3.7091\n",
      "Total loss:  -3.649 | PDE Loss:  -5.5379 | Function Loss:  -3.7091\n",
      "Total loss:  -3.6492 | PDE Loss:  -5.5395 | Function Loss:  -3.7091\n",
      "Total loss:  -3.6494 | PDE Loss:  -5.5407 | Function Loss:  -3.7091\n",
      "Total loss:  -3.6495 | PDE Loss:  -5.5416 | Function Loss:  -3.709\n",
      "Total loss:  -3.6496 | PDE Loss:  -5.5423 | Function Loss:  -3.7091\n",
      "Total loss:  -3.6497 | PDE Loss:  -5.5428 | Function Loss:  -3.7091\n",
      "Total loss:  -3.6498 | PDE Loss:  -5.5428 | Function Loss:  -3.7092\n",
      "Total loss:  -3.65 | PDE Loss:  -5.5431 | Function Loss:  -3.7094\n",
      "Total loss:  -3.6502 | PDE Loss:  -5.5436 | Function Loss:  -3.7096\n",
      "Total loss:  -3.6505 | PDE Loss:  -5.5453 | Function Loss:  -3.7097\n",
      "Total loss:  -3.6507 | PDE Loss:  -5.5462 | Function Loss:  -3.7098\n",
      "Total loss:  -3.651 | PDE Loss:  -5.5481 | Function Loss:  -3.7098\n",
      "Total loss:  -3.6512 | PDE Loss:  -5.5504 | Function Loss:  -3.7097\n",
      "Total loss:  -3.6514 | PDE Loss:  -5.5517 | Function Loss:  -3.7098\n",
      "Total loss:  -3.6516 | PDE Loss:  -5.5535 | Function Loss:  -3.7097\n",
      "Total loss:  -3.6517 | PDE Loss:  -5.554 | Function Loss:  -3.7098\n",
      "Total loss:  -3.6519 | PDE Loss:  -5.5548 | Function Loss:  -3.7099\n",
      "Total loss:  -3.652 | PDE Loss:  -5.5548 | Function Loss:  -3.71\n",
      "Total loss:  -3.6521 | PDE Loss:  -5.5544 | Function Loss:  -3.7102\n",
      "Total loss:  -3.6522 | PDE Loss:  -5.5537 | Function Loss:  -3.7104\n",
      "Total loss:  -3.6523 | PDE Loss:  -5.5531 | Function Loss:  -3.7106\n",
      "Total loss:  -3.6523 | PDE Loss:  -5.5526 | Function Loss:  -3.7107\n",
      "Total loss:  -3.6524 | PDE Loss:  -5.5519 | Function Loss:  -3.7109\n",
      "Total loss:  -3.6525 | PDE Loss:  -5.5515 | Function Loss:  -3.711\n",
      "Total loss:  -3.6525 | PDE Loss:  -5.5509 | Function Loss:  -3.7112\n",
      "Total loss:  -3.6526 | PDE Loss:  -5.5502 | Function Loss:  -3.7114\n",
      "Total loss:  -3.6527 | PDE Loss:  -5.5496 | Function Loss:  -3.7115\n",
      "Total loss:  -3.6527 | PDE Loss:  -5.5489 | Function Loss:  -3.7117\n",
      "Total loss:  -3.6529 | PDE Loss:  -5.5487 | Function Loss:  -3.7119\n",
      "Total loss:  -3.653 | PDE Loss:  -5.5474 | Function Loss:  -3.7123\n",
      "Total loss:  -3.6531 | PDE Loss:  -5.547 | Function Loss:  -3.7125\n",
      "Total loss:  -3.6533 | PDE Loss:  -5.5465 | Function Loss:  -3.7128\n",
      "Total loss:  -3.6536 | PDE Loss:  -5.5462 | Function Loss:  -3.7131\n",
      "Total loss:  -3.6539 | PDE Loss:  -5.5456 | Function Loss:  -3.7135\n",
      "Total loss:  -3.654 | PDE Loss:  -5.5465 | Function Loss:  -3.7135\n",
      "Total loss:  -3.6541 | PDE Loss:  -5.5461 | Function Loss:  -3.7137\n",
      "Total loss:  -3.6542 | PDE Loss:  -5.5467 | Function Loss:  -3.7137\n",
      "Total loss:  -3.6543 | PDE Loss:  -5.5476 | Function Loss:  -3.7137\n",
      "Total loss:  -3.6545 | PDE Loss:  -5.5493 | Function Loss:  -3.7137\n",
      "Total loss:  -3.6546 | PDE Loss:  -5.5505 | Function Loss:  -3.7137\n",
      "Total loss:  -3.6548 | PDE Loss:  -5.5524 | Function Loss:  -3.7136\n",
      "Total loss:  -3.6549 | PDE Loss:  -5.5534 | Function Loss:  -3.7136\n",
      "Total loss:  -3.655 | PDE Loss:  -5.5544 | Function Loss:  -3.7136\n",
      "Total loss:  -3.6551 | PDE Loss:  -5.5545 | Function Loss:  -3.7137\n",
      "Total loss:  -3.6552 | PDE Loss:  -5.558 | Function Loss:  -3.7132\n",
      "Total loss:  -3.6553 | PDE Loss:  -5.5568 | Function Loss:  -3.7135\n",
      "Total loss:  -3.6554 | PDE Loss:  -5.5554 | Function Loss:  -3.7139\n",
      "Total loss:  -3.6555 | PDE Loss:  -5.5539 | Function Loss:  -3.7142\n",
      "Total loss:  -3.6557 | PDE Loss:  -5.5524 | Function Loss:  -3.7146\n",
      "Total loss:  -3.6558 | PDE Loss:  -5.551 | Function Loss:  -3.7149\n",
      "Total loss:  -3.6559 | PDE Loss:  -5.5498 | Function Loss:  -3.7152\n",
      "Total loss:  -3.656 | PDE Loss:  -5.5489 | Function Loss:  -3.7155\n",
      "Total loss:  -3.6562 | PDE Loss:  -5.5481 | Function Loss:  -3.7158\n",
      "Total loss:  -3.6564 | PDE Loss:  -5.548 | Function Loss:  -3.716\n",
      "Total loss:  -3.6566 | PDE Loss:  -5.5477 | Function Loss:  -3.7163\n",
      "Total loss:  -3.6568 | PDE Loss:  -5.5479 | Function Loss:  -3.7165\n",
      "Total loss:  -3.657 | PDE Loss:  -5.548 | Function Loss:  -3.7167\n",
      "Total loss:  -3.6572 | PDE Loss:  -5.5482 | Function Loss:  -3.717\n",
      "Total loss:  -3.6575 | PDE Loss:  -5.5481 | Function Loss:  -3.7173\n",
      "Total loss:  -3.6576 | PDE Loss:  -5.548 | Function Loss:  -3.7175\n",
      "Total loss:  -3.6579 | PDE Loss:  -5.5473 | Function Loss:  -3.7179\n",
      "Total loss:  -3.6582 | PDE Loss:  -5.546 | Function Loss:  -3.7184\n",
      "Total loss:  -3.6585 | PDE Loss:  -5.546 | Function Loss:  -3.7187\n",
      "Total loss:  -3.6587 | PDE Loss:  -5.5451 | Function Loss:  -3.7191\n",
      "Total loss:  -3.6589 | PDE Loss:  -5.5448 | Function Loss:  -3.7194\n",
      "Total loss:  -3.659 | PDE Loss:  -5.5443 | Function Loss:  -3.7196\n",
      "Total loss:  -3.6591 | PDE Loss:  -5.5438 | Function Loss:  -3.7198\n",
      "Total loss:  -3.6592 | PDE Loss:  -5.5443 | Function Loss:  -3.7199\n",
      "Total loss:  -3.6593 | PDE Loss:  -5.5443 | Function Loss:  -3.72\n",
      "Total loss:  -3.6594 | PDE Loss:  -5.545 | Function Loss:  -3.72\n",
      "Total loss:  -3.6595 | PDE Loss:  -5.5458 | Function Loss:  -3.72\n",
      "Total loss:  -3.6596 | PDE Loss:  -5.5457 | Function Loss:  -3.7201\n",
      "Total loss:  -3.6598 | PDE Loss:  -5.5467 | Function Loss:  -3.7201\n",
      "Total loss:  -3.6599 | PDE Loss:  -5.5477 | Function Loss:  -3.7201\n",
      "Total loss:  -3.66 | PDE Loss:  -5.5488 | Function Loss:  -3.7201\n",
      "Total loss:  -3.6602 | PDE Loss:  -5.5494 | Function Loss:  -3.7202\n",
      "Total loss:  -3.6603 | PDE Loss:  -5.5501 | Function Loss:  -3.7202\n",
      "Total loss:  -3.6604 | PDE Loss:  -5.5507 | Function Loss:  -3.7203\n",
      "Total loss:  -3.6605 | PDE Loss:  -5.5505 | Function Loss:  -3.7204\n",
      "Total loss:  -3.6606 | PDE Loss:  -5.5503 | Function Loss:  -3.7206\n",
      "Total loss:  -3.6608 | PDE Loss:  -5.5495 | Function Loss:  -3.7209\n",
      "Total loss:  -3.6609 | PDE Loss:  -5.5485 | Function Loss:  -3.7211\n",
      "Total loss:  -3.661 | PDE Loss:  -5.5479 | Function Loss:  -3.7214\n",
      "Total loss:  -3.6611 | PDE Loss:  -5.5468 | Function Loss:  -3.7217\n",
      "Total loss:  -3.6613 | PDE Loss:  -5.5468 | Function Loss:  -3.7219\n",
      "Total loss:  -3.6614 | PDE Loss:  -5.5458 | Function Loss:  -3.7222\n",
      "Total loss:  -3.6616 | PDE Loss:  -5.546 | Function Loss:  -3.7224\n",
      "Total loss:  -3.6618 | PDE Loss:  -5.5465 | Function Loss:  -3.7225\n",
      "Total loss:  -3.662 | PDE Loss:  -5.5478 | Function Loss:  -3.7225\n",
      "Total loss:  -3.6622 | PDE Loss:  -5.5491 | Function Loss:  -3.7225\n",
      "Total loss:  -3.6623 | PDE Loss:  -5.5508 | Function Loss:  -3.7225\n",
      "Total loss:  -3.6625 | PDE Loss:  -5.5525 | Function Loss:  -3.7225\n",
      "Total loss:  -3.6627 | PDE Loss:  -5.5545 | Function Loss:  -3.7224\n",
      "Total loss:  -3.663 | PDE Loss:  -5.5548 | Function Loss:  -3.7226\n",
      "Total loss:  -3.6632 | PDE Loss:  -5.5567 | Function Loss:  -3.7225\n",
      "Total loss:  -3.6633 | PDE Loss:  -5.5584 | Function Loss:  -3.7225\n",
      "Total loss:  -3.6635 | PDE Loss:  -5.5596 | Function Loss:  -3.7225\n",
      "Total loss:  -3.6637 | PDE Loss:  -5.5608 | Function Loss:  -3.7225\n",
      "Total loss:  -3.6638 | PDE Loss:  -5.562 | Function Loss:  -3.7225\n",
      "Total loss:  -3.664 | PDE Loss:  -5.5633 | Function Loss:  -3.7225\n",
      "Total loss:  -3.6642 | PDE Loss:  -5.5654 | Function Loss:  -3.7224\n",
      "Total loss:  -3.6644 | PDE Loss:  -5.5675 | Function Loss:  -3.7223\n",
      "Total loss:  -3.6646 | PDE Loss:  -5.5703 | Function Loss:  -3.7222\n",
      "Total loss:  -3.6648 | PDE Loss:  -5.5734 | Function Loss:  -3.7221\n",
      "Total loss:  -3.6651 | PDE Loss:  -5.5774 | Function Loss:  -3.7218\n",
      "Total loss:  -3.6653 | PDE Loss:  -5.5797 | Function Loss:  -3.7217\n",
      "Total loss:  -3.6654 | PDE Loss:  -5.5814 | Function Loss:  -3.7216\n",
      "Total loss:  -3.6656 | PDE Loss:  -5.5823 | Function Loss:  -3.7217\n",
      "Total loss:  -3.6657 | PDE Loss:  -5.583 | Function Loss:  -3.7218\n",
      "Total loss:  -3.6659 | PDE Loss:  -5.5824 | Function Loss:  -3.722\n",
      "Total loss:  -3.6661 | PDE Loss:  -5.582 | Function Loss:  -3.7223\n",
      "Total loss:  -3.6663 | PDE Loss:  -5.5802 | Function Loss:  -3.7227\n",
      "Total loss:  -3.6664 | PDE Loss:  -5.5785 | Function Loss:  -3.7231\n",
      "Total loss:  -3.6666 | PDE Loss:  -5.576 | Function Loss:  -3.7237\n",
      "Total loss:  -3.6668 | PDE Loss:  -5.5729 | Function Loss:  -3.7243\n",
      "Total loss:  -3.667 | PDE Loss:  -5.5706 | Function Loss:  -3.7249\n",
      "Total loss:  -3.6671 | PDE Loss:  -5.5667 | Function Loss:  -3.7256\n",
      "Total loss:  -3.6673 | PDE Loss:  -5.5664 | Function Loss:  -3.7258\n",
      "Total loss:  -3.6675 | PDE Loss:  -5.5675 | Function Loss:  -3.7259\n",
      "Total loss:  -3.6677 | PDE Loss:  -5.5692 | Function Loss:  -3.7259\n",
      "Total loss:  -3.6678 | PDE Loss:  -5.5712 | Function Loss:  -3.7257\n",
      "Total loss:  -3.6679 | PDE Loss:  -5.5734 | Function Loss:  -3.7256\n",
      "Total loss:  -3.6681 | PDE Loss:  -5.5756 | Function Loss:  -3.7254\n",
      "Total loss:  -3.6682 | PDE Loss:  -5.5786 | Function Loss:  -3.7251\n",
      "Total loss:  -3.6683 | PDE Loss:  -5.5809 | Function Loss:  -3.7249\n",
      "Total loss:  -3.6684 | PDE Loss:  -5.5832 | Function Loss:  -3.7247\n",
      "Total loss:  -3.6685 | PDE Loss:  -5.585 | Function Loss:  -3.7246\n",
      "Total loss:  -3.6686 | PDE Loss:  -5.5864 | Function Loss:  -3.7245\n",
      "Total loss:  -3.6687 | PDE Loss:  -5.588 | Function Loss:  -3.7245\n",
      "Total loss:  -3.6689 | PDE Loss:  -5.5891 | Function Loss:  -3.7245\n",
      "Total loss:  -3.669 | PDE Loss:  -5.5907 | Function Loss:  -3.7244\n",
      "Total loss:  -3.6692 | PDE Loss:  -5.5914 | Function Loss:  -3.7245\n",
      "Total loss:  -3.6693 | PDE Loss:  -5.5925 | Function Loss:  -3.7245\n",
      "Total loss:  -3.6695 | PDE Loss:  -5.5933 | Function Loss:  -3.7246\n",
      "Total loss:  -3.6698 | PDE Loss:  -5.5931 | Function Loss:  -3.725\n",
      "Total loss:  -3.6701 | PDE Loss:  -5.593 | Function Loss:  -3.7253\n",
      "Total loss:  -3.6704 | PDE Loss:  -5.5913 | Function Loss:  -3.7259\n",
      "Total loss:  -3.6707 | PDE Loss:  -5.5899 | Function Loss:  -3.7264\n",
      "Total loss:  -3.6709 | PDE Loss:  -5.5874 | Function Loss:  -3.727\n",
      "Total loss:  -3.6711 | PDE Loss:  -5.5857 | Function Loss:  -3.7274\n",
      "Total loss:  -3.6712 | PDE Loss:  -5.584 | Function Loss:  -3.7278\n",
      "Total loss:  -3.6714 | PDE Loss:  -5.5825 | Function Loss:  -3.7283\n",
      "Total loss:  -3.6716 | PDE Loss:  -5.5815 | Function Loss:  -3.7286\n",
      "Total loss:  -3.6718 | PDE Loss:  -5.5803 | Function Loss:  -3.7291\n",
      "Total loss:  -3.672 | PDE Loss:  -5.5807 | Function Loss:  -3.7292\n",
      "Total loss:  -3.6722 | PDE Loss:  -5.5813 | Function Loss:  -3.7293\n",
      "Total loss:  -3.6723 | PDE Loss:  -5.5823 | Function Loss:  -3.7293\n",
      "Total loss:  -3.6724 | PDE Loss:  -5.5839 | Function Loss:  -3.7292\n",
      "Total loss:  -3.6724 | PDE Loss:  -5.5843 | Function Loss:  -3.7292\n",
      "Total loss:  -3.6725 | PDE Loss:  -5.585 | Function Loss:  -3.7292\n",
      "Total loss:  -3.6726 | PDE Loss:  -5.5856 | Function Loss:  -3.7292\n",
      "Total loss:  -3.6727 | PDE Loss:  -5.5865 | Function Loss:  -3.7292\n",
      "Total loss:  -3.6729 | PDE Loss:  -5.5874 | Function Loss:  -3.7293\n",
      "Total loss:  -3.673 | PDE Loss:  -5.5883 | Function Loss:  -3.7293\n",
      "Total loss:  -3.6732 | PDE Loss:  -5.5894 | Function Loss:  -3.7294\n",
      "Total loss:  -3.6734 | PDE Loss:  -5.5903 | Function Loss:  -3.7294\n",
      "Total loss:  -3.6735 | PDE Loss:  -5.5911 | Function Loss:  -3.7295\n",
      "Total loss:  -3.6737 | PDE Loss:  -5.5921 | Function Loss:  -3.7295\n",
      "Total loss:  -3.6739 | PDE Loss:  -5.5929 | Function Loss:  -3.7296\n",
      "Total loss:  -3.6739 | PDE Loss:  -5.5937 | Function Loss:  -3.7296\n",
      "Total loss:  -3.674 | PDE Loss:  -5.5944 | Function Loss:  -3.7296\n",
      "Total loss:  -3.6741 | PDE Loss:  -5.5952 | Function Loss:  -3.7296\n",
      "Total loss:  -3.6743 | PDE Loss:  -5.5969 | Function Loss:  -3.7296\n",
      "Total loss:  -3.6744 | PDE Loss:  -5.599 | Function Loss:  -3.7295\n",
      "Total loss:  -3.6746 | PDE Loss:  -5.6015 | Function Loss:  -3.7293\n",
      "Total loss:  -3.6748 | PDE Loss:  -5.6037 | Function Loss:  -3.7292\n",
      "Total loss:  -3.675 | PDE Loss:  -5.6066 | Function Loss:  -3.7291\n",
      "Total loss:  -3.6753 | PDE Loss:  -5.6097 | Function Loss:  -3.729\n",
      "Total loss:  -3.6755 | PDE Loss:  -5.6116 | Function Loss:  -3.729\n",
      "Total loss:  -3.6757 | PDE Loss:  -5.6132 | Function Loss:  -3.729\n",
      "Total loss:  -3.6758 | PDE Loss:  -5.6139 | Function Loss:  -3.7291\n",
      "Total loss:  -3.676 | PDE Loss:  -5.614 | Function Loss:  -3.7292\n",
      "Total loss:  -3.6762 | PDE Loss:  -5.6144 | Function Loss:  -3.7294\n",
      "Total loss:  -3.6764 | PDE Loss:  -5.6147 | Function Loss:  -3.7296\n",
      "Total loss:  -3.6766 | PDE Loss:  -5.6145 | Function Loss:  -3.7298\n",
      "Total loss:  -3.6767 | PDE Loss:  -5.615 | Function Loss:  -3.7299\n",
      "Total loss:  -3.6769 | PDE Loss:  -5.6165 | Function Loss:  -3.7299\n",
      "Total loss:  -3.677 | PDE Loss:  -5.6172 | Function Loss:  -3.73\n",
      "Total loss:  -3.6771 | PDE Loss:  -5.6191 | Function Loss:  -3.7299\n",
      "Total loss:  -3.6773 | PDE Loss:  -5.6196 | Function Loss:  -3.7299\n",
      "Total loss:  -3.6774 | PDE Loss:  -5.6208 | Function Loss:  -3.7299\n",
      "Total loss:  -3.6776 | PDE Loss:  -5.6209 | Function Loss:  -3.7301\n",
      "Total loss:  -3.6778 | PDE Loss:  -5.6231 | Function Loss:  -3.7301\n",
      "Total loss:  -3.6779 | PDE Loss:  -5.623 | Function Loss:  -3.7303\n",
      "Total loss:  -3.6781 | PDE Loss:  -5.6224 | Function Loss:  -3.7305\n",
      "Total loss:  -3.6782 | PDE Loss:  -5.622 | Function Loss:  -3.7307\n",
      "Total loss:  -3.6783 | PDE Loss:  -5.622 | Function Loss:  -3.7308\n",
      "Total loss:  -3.6785 | PDE Loss:  -5.622 | Function Loss:  -3.731\n",
      "Total loss:  -3.6786 | PDE Loss:  -5.6228 | Function Loss:  -3.731\n",
      "Total loss:  -3.6787 | PDE Loss:  -5.6241 | Function Loss:  -3.731\n",
      "Total loss:  -3.6789 | PDE Loss:  -5.6265 | Function Loss:  -3.7309\n",
      "Total loss:  -3.679 | PDE Loss:  -5.6285 | Function Loss:  -3.7308\n",
      "Total loss:  -3.6791 | PDE Loss:  -5.6307 | Function Loss:  -3.7306\n",
      "Total loss:  -3.6793 | PDE Loss:  -5.6319 | Function Loss:  -3.7306\n",
      "Total loss:  -3.6794 | PDE Loss:  -5.6329 | Function Loss:  -3.7306\n",
      "Total loss:  -3.6794 | PDE Loss:  -5.6335 | Function Loss:  -3.7306\n",
      "Total loss:  -3.6795 | PDE Loss:  -5.6336 | Function Loss:  -3.7307\n",
      "Total loss:  -3.6796 | PDE Loss:  -5.6339 | Function Loss:  -3.7308\n",
      "Total loss:  -3.6797 | PDE Loss:  -5.6336 | Function Loss:  -3.7309\n",
      "Total loss:  -3.6798 | PDE Loss:  -5.6343 | Function Loss:  -3.731\n",
      "Total loss:  -3.6799 | PDE Loss:  -5.6342 | Function Loss:  -3.7311\n",
      "Total loss:  -3.6801 | PDE Loss:  -5.6347 | Function Loss:  -3.7312\n",
      "Total loss:  -3.6802 | PDE Loss:  -5.6351 | Function Loss:  -3.7313\n",
      "Total loss:  -3.6803 | PDE Loss:  -5.6357 | Function Loss:  -3.7313\n",
      "Total loss:  -3.6805 | PDE Loss:  -5.6372 | Function Loss:  -3.7313\n",
      "Total loss:  -3.6806 | PDE Loss:  -5.6385 | Function Loss:  -3.7313\n",
      "Total loss:  -3.6808 | PDE Loss:  -5.6395 | Function Loss:  -3.7314\n",
      "Total loss:  -3.681 | PDE Loss:  -5.6411 | Function Loss:  -3.7314\n",
      "Total loss:  -3.6812 | PDE Loss:  -5.6415 | Function Loss:  -3.7316\n",
      "Total loss:  -3.6813 | PDE Loss:  -5.6421 | Function Loss:  -3.7317\n",
      "Total loss:  -3.6814 | PDE Loss:  -5.6419 | Function Loss:  -3.7318\n",
      "Total loss:  -3.6815 | PDE Loss:  -5.6416 | Function Loss:  -3.732\n",
      "Total loss:  -3.6816 | PDE Loss:  -5.6411 | Function Loss:  -3.7321\n",
      "Total loss:  -3.6817 | PDE Loss:  -5.6403 | Function Loss:  -3.7323\n",
      "Total loss:  -3.6817 | PDE Loss:  -5.6398 | Function Loss:  -3.7324\n",
      "Total loss:  -3.6818 | PDE Loss:  -5.6395 | Function Loss:  -3.7325\n",
      "Total loss:  -3.6819 | PDE Loss:  -5.6393 | Function Loss:  -3.7327\n",
      "Total loss:  -3.6821 | PDE Loss:  -5.6398 | Function Loss:  -3.7328\n",
      "Total loss:  -3.6822 | PDE Loss:  -5.6403 | Function Loss:  -3.7328\n",
      "Total loss:  -3.6823 | PDE Loss:  -5.6416 | Function Loss:  -3.7328\n",
      "Total loss:  -3.6823 | PDE Loss:  -5.6425 | Function Loss:  -3.7328\n",
      "Total loss:  -3.6825 | PDE Loss:  -5.6441 | Function Loss:  -3.7327\n",
      "Total loss:  -3.6826 | PDE Loss:  -5.6459 | Function Loss:  -3.7326\n",
      "Total loss:  -3.6827 | PDE Loss:  -5.6474 | Function Loss:  -3.7326\n",
      "Total loss:  -3.6828 | PDE Loss:  -5.6494 | Function Loss:  -3.7325\n",
      "Total loss:  -3.6829 | PDE Loss:  -5.6501 | Function Loss:  -3.7325\n",
      "Total loss:  -3.683 | PDE Loss:  -5.6504 | Function Loss:  -3.7326\n",
      "Total loss:  -3.6831 | PDE Loss:  -5.6502 | Function Loss:  -3.7327\n",
      "Total loss:  -3.6833 | PDE Loss:  -5.6492 | Function Loss:  -3.733\n",
      "Total loss:  -3.6834 | PDE Loss:  -5.6484 | Function Loss:  -3.7332\n",
      "Total loss:  -3.6835 | PDE Loss:  -5.6468 | Function Loss:  -3.7335\n",
      "Total loss:  -3.6836 | PDE Loss:  -5.6462 | Function Loss:  -3.7337\n",
      "Total loss:  -3.6837 | PDE Loss:  -5.6448 | Function Loss:  -3.734\n",
      "Total loss:  -3.6838 | PDE Loss:  -5.6451 | Function Loss:  -3.7341\n",
      "Total loss:  -3.6839 | PDE Loss:  -5.645 | Function Loss:  -3.7342\n",
      "Total loss:  -3.6841 | PDE Loss:  -5.6456 | Function Loss:  -3.7343\n",
      "Total loss:  -3.6842 | PDE Loss:  -5.6469 | Function Loss:  -3.7343\n",
      "Total loss:  -3.6843 | PDE Loss:  -5.6482 | Function Loss:  -3.7343\n",
      "Total loss:  -3.6844 | PDE Loss:  -5.6501 | Function Loss:  -3.7342\n",
      "Total loss:  -3.6845 | PDE Loss:  -5.651 | Function Loss:  -3.7342\n",
      "Total loss:  -3.6847 | PDE Loss:  -5.6522 | Function Loss:  -3.7342\n",
      "Total loss:  -3.6848 | PDE Loss:  -5.6526 | Function Loss:  -3.7342\n",
      "Total loss:  -3.6849 | PDE Loss:  -5.6534 | Function Loss:  -3.7343\n",
      "Total loss:  -3.6851 | PDE Loss:  -5.6538 | Function Loss:  -3.7344\n",
      "Total loss:  -3.6852 | PDE Loss:  -5.6536 | Function Loss:  -3.7346\n",
      "Total loss:  -3.6853 | PDE Loss:  -5.6539 | Function Loss:  -3.7347\n",
      "Total loss:  -3.6854 | PDE Loss:  -5.6523 | Function Loss:  -3.735\n",
      "Total loss:  -3.6855 | PDE Loss:  -5.6526 | Function Loss:  -3.7351\n",
      "Total loss:  -3.6856 | PDE Loss:  -5.6529 | Function Loss:  -3.7351\n",
      "Total loss:  -3.6857 | PDE Loss:  -5.6532 | Function Loss:  -3.7352\n",
      "Total loss:  -3.6857 | PDE Loss:  -5.6536 | Function Loss:  -3.7352\n",
      "Total loss:  -3.6858 | PDE Loss:  -5.6539 | Function Loss:  -3.7353\n",
      "Total loss:  -3.6859 | PDE Loss:  -5.6545 | Function Loss:  -3.7353\n",
      "Total loss:  -3.686 | PDE Loss:  -5.6546 | Function Loss:  -3.7354\n",
      "Total loss:  -3.6861 | PDE Loss:  -5.6548 | Function Loss:  -3.7354\n",
      "Total loss:  -3.6861 | PDE Loss:  -5.6545 | Function Loss:  -3.7356\n",
      "Total loss:  -3.6862 | PDE Loss:  -5.6547 | Function Loss:  -3.7357\n",
      "Total loss:  -3.6863 | PDE Loss:  -5.6545 | Function Loss:  -3.7358\n",
      "Total loss:  -3.6864 | PDE Loss:  -5.6538 | Function Loss:  -3.736\n",
      "Total loss:  -3.6866 | PDE Loss:  -5.6543 | Function Loss:  -3.7361\n",
      "Total loss:  -3.6866 | PDE Loss:  -5.6538 | Function Loss:  -3.7362\n",
      "Total loss:  -3.6868 | PDE Loss:  -5.6536 | Function Loss:  -3.7364\n",
      "Total loss:  -3.6869 | PDE Loss:  -5.6542 | Function Loss:  -3.7364\n",
      "Total loss:  -3.687 | PDE Loss:  -5.6546 | Function Loss:  -3.7365\n",
      "Total loss:  -3.6871 | PDE Loss:  -5.6558 | Function Loss:  -3.7365\n",
      "Total loss:  -3.6872 | PDE Loss:  -5.6557 | Function Loss:  -3.7366\n",
      "Total loss:  -3.6874 | PDE Loss:  -5.6567 | Function Loss:  -3.7367\n",
      "Total loss:  -3.6875 | PDE Loss:  -5.657 | Function Loss:  -3.7367\n",
      "Total loss:  -3.6876 | PDE Loss:  -5.6584 | Function Loss:  -3.7367\n",
      "Total loss:  -3.6877 | PDE Loss:  -5.6585 | Function Loss:  -3.7368\n",
      "Total loss:  -3.6878 | PDE Loss:  -5.6588 | Function Loss:  -3.7369\n",
      "Total loss:  -3.6878 | PDE Loss:  -5.6589 | Function Loss:  -3.7369\n",
      "Total loss:  -3.688 | PDE Loss:  -5.6593 | Function Loss:  -3.737\n",
      "Total loss:  -3.6881 | PDE Loss:  -5.6605 | Function Loss:  -3.737\n",
      "Total loss:  -3.6882 | PDE Loss:  -5.6617 | Function Loss:  -3.737\n",
      "Total loss:  -3.6884 | PDE Loss:  -5.6648 | Function Loss:  -3.7369\n",
      "Total loss:  -3.6886 | PDE Loss:  -5.6669 | Function Loss:  -3.7368\n",
      "Total loss:  -3.6887 | PDE Loss:  -5.6692 | Function Loss:  -3.7367\n",
      "Total loss:  -3.6888 | PDE Loss:  -5.6716 | Function Loss:  -3.7365\n",
      "Total loss:  -3.6889 | PDE Loss:  -5.6727 | Function Loss:  -3.7365\n",
      "Total loss:  -3.6889 | PDE Loss:  -5.6744 | Function Loss:  -3.7363\n",
      "Total loss:  -3.6889 | PDE Loss:  -5.6754 | Function Loss:  -3.7362\n",
      "Total loss:  -3.689 | PDE Loss:  -5.6763 | Function Loss:  -3.7362\n",
      "Total loss:  -3.6891 | PDE Loss:  -5.6777 | Function Loss:  -3.7362\n",
      "Total loss:  -3.6892 | PDE Loss:  -5.678 | Function Loss:  -3.7363\n",
      "Total loss:  -3.6894 | PDE Loss:  -5.68 | Function Loss:  -3.7362\n",
      "Total loss:  -3.6895 | PDE Loss:  -5.6789 | Function Loss:  -3.7364\n",
      "Total loss:  -3.6896 | PDE Loss:  -5.6794 | Function Loss:  -3.7365\n",
      "Total loss:  -3.6897 | PDE Loss:  -5.6793 | Function Loss:  -3.7367\n",
      "Total loss:  -3.6899 | PDE Loss:  -5.6787 | Function Loss:  -3.7369\n",
      "Total loss:  -3.69 | PDE Loss:  -5.6775 | Function Loss:  -3.7372\n",
      "Total loss:  -3.6901 | PDE Loss:  -5.6768 | Function Loss:  -3.7374\n",
      "Total loss:  -3.6902 | PDE Loss:  -5.6757 | Function Loss:  -3.7376\n",
      "Total loss:  -3.6904 | PDE Loss:  -5.6746 | Function Loss:  -3.7379\n",
      "Total loss:  -3.6906 | PDE Loss:  -5.6737 | Function Loss:  -3.7382\n",
      "Total loss:  -3.6907 | PDE Loss:  -5.6727 | Function Loss:  -3.7385\n",
      "Total loss:  -3.6909 | PDE Loss:  -5.6721 | Function Loss:  -3.7388\n",
      "Total loss:  -3.691 | PDE Loss:  -5.6715 | Function Loss:  -3.739\n",
      "Total loss:  -3.6912 | PDE Loss:  -5.671 | Function Loss:  -3.7392\n",
      "Total loss:  -3.6913 | PDE Loss:  -5.6706 | Function Loss:  -3.7394\n",
      "Total loss:  -3.6915 | PDE Loss:  -5.67 | Function Loss:  -3.7397\n",
      "Total loss:  -3.6917 | PDE Loss:  -5.6697 | Function Loss:  -3.7399\n",
      "Total loss:  -3.6918 | PDE Loss:  -5.6688 | Function Loss:  -3.7402\n",
      "Total loss:  -3.6919 | PDE Loss:  -5.6686 | Function Loss:  -3.7404\n",
      "Total loss:  -3.6921 | PDE Loss:  -5.6677 | Function Loss:  -3.7406\n",
      "Total loss:  -3.6922 | PDE Loss:  -5.6675 | Function Loss:  -3.7408\n",
      "Total loss:  -3.6923 | PDE Loss:  -5.6668 | Function Loss:  -3.741\n",
      "Total loss:  -3.6924 | PDE Loss:  -5.6666 | Function Loss:  -3.7411\n",
      "Total loss:  -3.6925 | PDE Loss:  -5.666 | Function Loss:  -3.7413\n",
      "Total loss:  -3.6926 | PDE Loss:  -5.6654 | Function Loss:  -3.7415\n",
      "Total loss:  -3.6927 | PDE Loss:  -5.6657 | Function Loss:  -3.7416\n",
      "Total loss:  -3.6929 | PDE Loss:  -5.6659 | Function Loss:  -3.7417\n",
      "Total loss:  -3.693 | PDE Loss:  -5.6667 | Function Loss:  -3.7418\n",
      "Total loss:  -3.6931 | PDE Loss:  -5.6672 | Function Loss:  -3.7418\n",
      "Total loss:  -3.6932 | PDE Loss:  -5.6684 | Function Loss:  -3.7419\n",
      "Total loss:  -3.6934 | PDE Loss:  -5.6691 | Function Loss:  -3.7419\n",
      "Total loss:  -3.6935 | PDE Loss:  -5.6702 | Function Loss:  -3.7419\n",
      "Total loss:  -3.6937 | PDE Loss:  -5.6708 | Function Loss:  -3.742\n",
      "Total loss:  -3.6938 | PDE Loss:  -5.6713 | Function Loss:  -3.7422\n",
      "Total loss:  -3.6939 | PDE Loss:  -5.672 | Function Loss:  -3.7422\n",
      "Total loss:  -3.694 | PDE Loss:  -5.6716 | Function Loss:  -3.7423\n",
      "Total loss:  -3.6941 | PDE Loss:  -5.6718 | Function Loss:  -3.7424\n",
      "Total loss:  -3.6941 | PDE Loss:  -5.6716 | Function Loss:  -3.7425\n",
      "Total loss:  -3.6942 | PDE Loss:  -5.6716 | Function Loss:  -3.7425\n",
      "Total loss:  -3.6943 | PDE Loss:  -5.6716 | Function Loss:  -3.7426\n",
      "Total loss:  -3.6944 | PDE Loss:  -5.6725 | Function Loss:  -3.7426\n",
      "Total loss:  -3.6944 | PDE Loss:  -5.6723 | Function Loss:  -3.7427\n",
      "Total loss:  -3.6945 | PDE Loss:  -5.672 | Function Loss:  -3.7428\n",
      "Total loss:  -3.6945 | PDE Loss:  -5.672 | Function Loss:  -3.7429\n",
      "Total loss:  -3.6947 | PDE Loss:  -5.6722 | Function Loss:  -3.743\n",
      "Total loss:  -3.6948 | PDE Loss:  -5.6716 | Function Loss:  -3.7432\n",
      "Total loss:  -3.6949 | PDE Loss:  -5.672 | Function Loss:  -3.7433\n",
      "Total loss:  -3.695 | PDE Loss:  -5.671 | Function Loss:  -3.7435\n",
      "Total loss:  -3.6951 | PDE Loss:  -5.6693 | Function Loss:  -3.7438\n",
      "Total loss:  -3.6953 | PDE Loss:  -5.6682 | Function Loss:  -3.7441\n",
      "Total loss:  -3.6954 | PDE Loss:  -5.6667 | Function Loss:  -3.7445\n",
      "Total loss:  -3.6956 | PDE Loss:  -5.6659 | Function Loss:  -3.7448\n",
      "Total loss:  -3.6958 | PDE Loss:  -5.6645 | Function Loss:  -3.7452\n",
      "Total loss:  -3.696 | PDE Loss:  -5.6641 | Function Loss:  -3.7454\n",
      "Total loss:  -3.6961 | PDE Loss:  -5.6636 | Function Loss:  -3.7457\n",
      "Total loss:  -3.6963 | PDE Loss:  -5.6633 | Function Loss:  -3.7459\n",
      "Total loss:  -3.6965 | PDE Loss:  -5.6646 | Function Loss:  -3.7459\n",
      "Total loss:  -3.6966 | PDE Loss:  -5.6651 | Function Loss:  -3.746\n",
      "Total loss:  -3.6967 | PDE Loss:  -5.6659 | Function Loss:  -3.746\n",
      "Total loss:  -3.6968 | PDE Loss:  -5.6673 | Function Loss:  -3.746\n",
      "Total loss:  -3.6969 | PDE Loss:  -5.6681 | Function Loss:  -3.746\n",
      "Total loss:  -3.6971 | PDE Loss:  -5.6687 | Function Loss:  -3.7461\n",
      "Total loss:  -3.6971 | PDE Loss:  -5.6681 | Function Loss:  -3.7462\n",
      "Total loss:  -3.6972 | PDE Loss:  -5.6675 | Function Loss:  -3.7464\n",
      "Total loss:  -3.6974 | PDE Loss:  -5.6659 | Function Loss:  -3.7468\n",
      "Total loss:  -3.6975 | PDE Loss:  -5.6627 | Function Loss:  -3.7473\n",
      "Total loss:  -3.6977 | PDE Loss:  -5.6596 | Function Loss:  -3.748\n",
      "Total loss:  -3.6979 | PDE Loss:  -5.656 | Function Loss:  -3.7486\n",
      "Total loss:  -3.6981 | PDE Loss:  -5.6527 | Function Loss:  -3.7492\n",
      "Total loss:  -3.6982 | PDE Loss:  -5.6507 | Function Loss:  -3.7496\n",
      "Total loss:  -3.6983 | PDE Loss:  -5.6489 | Function Loss:  -3.75\n",
      "Total loss:  -3.6985 | PDE Loss:  -5.6483 | Function Loss:  -3.7502\n",
      "Total loss:  -3.6986 | PDE Loss:  -5.6485 | Function Loss:  -3.7503\n",
      "Total loss:  -3.6987 | PDE Loss:  -5.6488 | Function Loss:  -3.7504\n",
      "Total loss:  -3.6989 | PDE Loss:  -5.6497 | Function Loss:  -3.7504\n",
      "Total loss:  -3.699 | PDE Loss:  -5.6504 | Function Loss:  -3.7505\n",
      "Total loss:  -3.6992 | PDE Loss:  -5.6517 | Function Loss:  -3.7506\n",
      "Total loss:  -3.6994 | PDE Loss:  -5.6527 | Function Loss:  -3.7507\n",
      "Total loss:  -3.6996 | PDE Loss:  -5.6544 | Function Loss:  -3.7507\n",
      "Total loss:  -3.6998 | PDE Loss:  -5.6566 | Function Loss:  -3.7506\n",
      "Total loss:  -3.7 | PDE Loss:  -5.6579 | Function Loss:  -3.7507\n",
      "Total loss:  -3.7001 | PDE Loss:  -5.6582 | Function Loss:  -3.7507\n",
      "Total loss:  -3.7002 | PDE Loss:  -5.659 | Function Loss:  -3.7508\n",
      "Total loss:  -3.7004 | PDE Loss:  -5.6598 | Function Loss:  -3.7509\n",
      "Total loss:  -3.7005 | PDE Loss:  -5.661 | Function Loss:  -3.7509\n",
      "Total loss:  -3.7006 | PDE Loss:  -5.6622 | Function Loss:  -3.7508\n",
      "Total loss:  -3.7007 | PDE Loss:  -5.664 | Function Loss:  -3.7508\n",
      "Total loss:  -3.7009 | PDE Loss:  -5.6651 | Function Loss:  -3.7508\n",
      "Total loss:  -3.701 | PDE Loss:  -5.666 | Function Loss:  -3.7508\n",
      "Total loss:  -3.7011 | PDE Loss:  -5.667 | Function Loss:  -3.7508\n",
      "Total loss:  -3.7012 | PDE Loss:  -5.6677 | Function Loss:  -3.7509\n",
      "Total loss:  -3.7013 | PDE Loss:  -5.6678 | Function Loss:  -3.7509\n",
      "Total loss:  -3.7014 | PDE Loss:  -5.6676 | Function Loss:  -3.7511\n",
      "Total loss:  -3.7015 | PDE Loss:  -5.6674 | Function Loss:  -3.7512\n",
      "Total loss:  -3.7016 | PDE Loss:  -5.6672 | Function Loss:  -3.7513\n",
      "Total loss:  -3.7017 | PDE Loss:  -5.6673 | Function Loss:  -3.7514\n",
      "Total loss:  -3.7018 | PDE Loss:  -5.6676 | Function Loss:  -3.7516\n",
      "Total loss:  -3.702 | PDE Loss:  -5.6683 | Function Loss:  -3.7517\n",
      "Total loss:  -3.7021 | PDE Loss:  -5.671 | Function Loss:  -3.7515\n",
      "Total loss:  -3.7023 | PDE Loss:  -5.6707 | Function Loss:  -3.7517\n",
      "Total loss:  -3.7024 | PDE Loss:  -5.6711 | Function Loss:  -3.7518\n",
      "Total loss:  -3.7026 | PDE Loss:  -5.6722 | Function Loss:  -3.7519\n",
      "Total loss:  -3.7029 | PDE Loss:  -5.6734 | Function Loss:  -3.752\n",
      "Total loss:  -3.7031 | PDE Loss:  -5.6745 | Function Loss:  -3.7521\n",
      "Total loss:  -3.7034 | PDE Loss:  -5.6756 | Function Loss:  -3.7524\n",
      "Total loss:  -3.7037 | PDE Loss:  -5.6759 | Function Loss:  -3.7526\n",
      "Total loss:  -3.7039 | PDE Loss:  -5.6764 | Function Loss:  -3.7528\n",
      "Total loss:  -3.704 | PDE Loss:  -5.6758 | Function Loss:  -3.7531\n",
      "Total loss:  -3.7042 | PDE Loss:  -5.6753 | Function Loss:  -3.7533\n",
      "Total loss:  -3.7043 | PDE Loss:  -5.6745 | Function Loss:  -3.7535\n",
      "Total loss:  -3.7043 | PDE Loss:  -5.6741 | Function Loss:  -3.7536\n",
      "Total loss:  -3.7044 | PDE Loss:  -5.674 | Function Loss:  -3.7537\n",
      "Total loss:  -3.7045 | PDE Loss:  -5.6741 | Function Loss:  -3.7537\n",
      "Total loss:  -3.7046 | PDE Loss:  -5.6742 | Function Loss:  -3.7538\n",
      "Total loss:  -3.7047 | PDE Loss:  -5.6744 | Function Loss:  -3.7539\n",
      "Total loss:  -3.7047 | PDE Loss:  -5.6747 | Function Loss:  -3.754\n",
      "Total loss:  -3.7048 | PDE Loss:  -5.6753 | Function Loss:  -3.754\n",
      "Total loss:  -3.7049 | PDE Loss:  -5.6763 | Function Loss:  -3.754\n",
      "Total loss:  -3.7051 | PDE Loss:  -5.6773 | Function Loss:  -3.754\n",
      "Total loss:  -3.7052 | PDE Loss:  -5.6788 | Function Loss:  -3.754\n",
      "Total loss:  -3.7053 | PDE Loss:  -5.6797 | Function Loss:  -3.754\n",
      "Total loss:  -3.7054 | PDE Loss:  -5.6804 | Function Loss:  -3.7541\n",
      "Total loss:  -3.7056 | PDE Loss:  -5.6809 | Function Loss:  -3.7542\n",
      "Total loss:  -3.7058 | PDE Loss:  -5.6802 | Function Loss:  -3.7544\n",
      "Total loss:  -3.7059 | PDE Loss:  -5.6803 | Function Loss:  -3.7546\n",
      "Total loss:  -3.706 | PDE Loss:  -5.6792 | Function Loss:  -3.7548\n",
      "Total loss:  -3.706 | PDE Loss:  -5.6782 | Function Loss:  -3.7549\n",
      "Total loss:  -3.7061 | PDE Loss:  -5.6772 | Function Loss:  -3.7551\n",
      "Total loss:  -3.7061 | PDE Loss:  -5.6763 | Function Loss:  -3.7553\n",
      "Total loss:  -3.7062 | PDE Loss:  -5.6754 | Function Loss:  -3.7555\n",
      "Total loss:  -3.7062 | PDE Loss:  -5.6749 | Function Loss:  -3.7556\n",
      "Total loss:  -3.7063 | PDE Loss:  -5.6743 | Function Loss:  -3.7558\n",
      "Total loss:  -3.7063 | PDE Loss:  -5.6749 | Function Loss:  -3.7557\n",
      "Total loss:  -3.7064 | PDE Loss:  -5.6749 | Function Loss:  -3.7558\n",
      "Total loss:  -3.7064 | PDE Loss:  -5.675 | Function Loss:  -3.7558\n",
      "Total loss:  -3.7065 | PDE Loss:  -5.6754 | Function Loss:  -3.7558\n",
      "Total loss:  -3.7065 | PDE Loss:  -5.6759 | Function Loss:  -3.7558\n",
      "Total loss:  -3.7066 | PDE Loss:  -5.6769 | Function Loss:  -3.7558\n",
      "Total loss:  -3.7066 | PDE Loss:  -5.6778 | Function Loss:  -3.7557\n",
      "Total loss:  -3.7067 | PDE Loss:  -5.679 | Function Loss:  -3.7557\n",
      "Total loss:  -3.7068 | PDE Loss:  -5.6796 | Function Loss:  -3.7557\n",
      "Total loss:  -3.7069 | PDE Loss:  -5.6812 | Function Loss:  -3.7557\n",
      "Total loss:  -3.7071 | PDE Loss:  -5.6803 | Function Loss:  -3.7559\n",
      "Total loss:  -3.7071 | PDE Loss:  -5.6801 | Function Loss:  -3.756\n",
      "Total loss:  -3.7072 | PDE Loss:  -5.6791 | Function Loss:  -3.7562\n",
      "Total loss:  -3.7073 | PDE Loss:  -5.6785 | Function Loss:  -3.7564\n",
      "Total loss:  -3.7074 | PDE Loss:  -5.6778 | Function Loss:  -3.7565\n",
      "Total loss:  -3.7074 | PDE Loss:  -5.6772 | Function Loss:  -3.7567\n",
      "Total loss:  -3.7075 | PDE Loss:  -5.678 | Function Loss:  -3.7567\n",
      "Total loss:  -3.7076 | PDE Loss:  -5.6779 | Function Loss:  -3.7567\n",
      "Total loss:  -3.7076 | PDE Loss:  -5.6786 | Function Loss:  -3.7567\n",
      "Total loss:  -3.7078 | PDE Loss:  -5.6801 | Function Loss:  -3.7567\n",
      "Total loss:  -3.7079 | PDE Loss:  -5.6826 | Function Loss:  -3.7565\n",
      "Total loss:  -3.708 | PDE Loss:  -5.6852 | Function Loss:  -3.7564\n",
      "Total loss:  -3.7081 | PDE Loss:  -5.6879 | Function Loss:  -3.7561\n",
      "Total loss:  -3.7082 | PDE Loss:  -5.6909 | Function Loss:  -3.7559\n",
      "Total loss:  -3.7083 | PDE Loss:  -5.6939 | Function Loss:  -3.7557\n",
      "Total loss:  -3.7084 | PDE Loss:  -5.6978 | Function Loss:  -3.7554\n",
      "Total loss:  -3.7085 | PDE Loss:  -5.6986 | Function Loss:  -3.7554\n",
      "Total loss:  -3.7086 | PDE Loss:  -5.6994 | Function Loss:  -3.7554\n",
      "Total loss:  -3.7086 | PDE Loss:  -5.6991 | Function Loss:  -3.7555\n",
      "Total loss:  -3.7087 | PDE Loss:  -5.6987 | Function Loss:  -3.7556\n",
      "Total loss:  -3.7088 | PDE Loss:  -5.6979 | Function Loss:  -3.7557\n",
      "Total loss:  -3.7088 | PDE Loss:  -5.6972 | Function Loss:  -3.7559\n",
      "Total loss:  -3.7089 | PDE Loss:  -5.6964 | Function Loss:  -3.7561\n",
      "Total loss:  -3.7089 | PDE Loss:  -5.6958 | Function Loss:  -3.7562\n",
      "Total loss:  -3.709 | PDE Loss:  -5.6952 | Function Loss:  -3.7564\n",
      "Total loss:  -3.7092 | PDE Loss:  -5.6953 | Function Loss:  -3.7565\n",
      "Total loss:  -3.7093 | PDE Loss:  -5.6955 | Function Loss:  -3.7566\n",
      "Total loss:  -3.7094 | PDE Loss:  -5.6962 | Function Loss:  -3.7566\n",
      "Total loss:  -3.7095 | PDE Loss:  -5.6971 | Function Loss:  -3.7566\n",
      "Total loss:  -3.7095 | PDE Loss:  -5.6978 | Function Loss:  -3.7566\n",
      "Total loss:  -3.7096 | PDE Loss:  -5.6985 | Function Loss:  -3.7566\n",
      "Total loss:  -3.7097 | PDE Loss:  -5.699 | Function Loss:  -3.7566\n",
      "Total loss:  -3.7097 | PDE Loss:  -5.6996 | Function Loss:  -3.7566\n",
      "Total loss:  -3.7098 | PDE Loss:  -5.7001 | Function Loss:  -3.7566\n",
      "Total loss:  -3.7099 | PDE Loss:  -5.7009 | Function Loss:  -3.7567\n",
      "Total loss:  -3.71 | PDE Loss:  -5.702 | Function Loss:  -3.7567\n",
      "Total loss:  -3.7101 | PDE Loss:  -5.7026 | Function Loss:  -3.7567\n",
      "Total loss:  -3.7102 | PDE Loss:  -5.7032 | Function Loss:  -3.7568\n",
      "Total loss:  -3.7103 | PDE Loss:  -5.7034 | Function Loss:  -3.7569\n",
      "Total loss:  -3.7104 | PDE Loss:  -5.7034 | Function Loss:  -3.757\n",
      "Total loss:  -3.7105 | PDE Loss:  -5.7029 | Function Loss:  -3.7571\n",
      "Total loss:  -3.7106 | PDE Loss:  -5.7023 | Function Loss:  -3.7573\n",
      "Total loss:  -3.7107 | PDE Loss:  -5.7026 | Function Loss:  -3.7574\n",
      "Total loss:  -3.7108 | PDE Loss:  -5.7017 | Function Loss:  -3.7576\n",
      "Total loss:  -3.711 | PDE Loss:  -5.7012 | Function Loss:  -3.7578\n",
      "Total loss:  -3.7111 | PDE Loss:  -5.7009 | Function Loss:  -3.758\n",
      "Total loss:  -3.7112 | PDE Loss:  -5.701 | Function Loss:  -3.7581\n",
      "Total loss:  -3.7113 | PDE Loss:  -5.7011 | Function Loss:  -3.7582\n",
      "Total loss:  -3.7113 | PDE Loss:  -5.7013 | Function Loss:  -3.7582\n",
      "Total loss:  -3.7114 | PDE Loss:  -5.7005 | Function Loss:  -3.7584\n",
      "Total loss:  -3.7115 | PDE Loss:  -5.7007 | Function Loss:  -3.7585\n",
      "Total loss:  -3.7115 | PDE Loss:  -5.6997 | Function Loss:  -3.7586\n",
      "Total loss:  -3.7116 | PDE Loss:  -5.6999 | Function Loss:  -3.7587\n",
      "Total loss:  -3.7117 | PDE Loss:  -5.7003 | Function Loss:  -3.7588\n",
      "Total loss:  -3.7118 | PDE Loss:  -5.7001 | Function Loss:  -3.7589\n",
      "Total loss:  -3.7119 | PDE Loss:  -5.6996 | Function Loss:  -3.7591\n",
      "Total loss:  -3.712 | PDE Loss:  -5.6985 | Function Loss:  -3.7593\n",
      "Total loss:  -3.7122 | PDE Loss:  -5.6948 | Function Loss:  -3.7599\n",
      "Total loss:  -3.7123 | PDE Loss:  -5.6935 | Function Loss:  -3.7602\n",
      "Total loss:  -3.7125 | PDE Loss:  -5.6918 | Function Loss:  -3.7606\n",
      "Total loss:  -3.7127 | PDE Loss:  -5.6891 | Function Loss:  -3.7612\n",
      "Total loss:  -3.7129 | PDE Loss:  -5.6867 | Function Loss:  -3.7617\n",
      "Total loss:  -3.7132 | PDE Loss:  -5.684 | Function Loss:  -3.7623\n",
      "Total loss:  -3.7133 | PDE Loss:  -5.682 | Function Loss:  -3.7627\n",
      "Total loss:  -3.7135 | PDE Loss:  -5.6809 | Function Loss:  -3.763\n",
      "Total loss:  -3.7136 | PDE Loss:  -5.68 | Function Loss:  -3.7632\n",
      "Total loss:  -3.7138 | PDE Loss:  -5.6801 | Function Loss:  -3.7634\n",
      "Total loss:  -3.7139 | PDE Loss:  -5.6805 | Function Loss:  -3.7635\n",
      "Total loss:  -3.714 | PDE Loss:  -5.6811 | Function Loss:  -3.7636\n",
      "Total loss:  -3.7141 | PDE Loss:  -5.6822 | Function Loss:  -3.7636\n",
      "Total loss:  -3.7142 | PDE Loss:  -5.6835 | Function Loss:  -3.7635\n",
      "Total loss:  -3.7143 | PDE Loss:  -5.6846 | Function Loss:  -3.7635\n",
      "Total loss:  -3.7144 | PDE Loss:  -5.6855 | Function Loss:  -3.7634\n",
      "Total loss:  -3.7144 | PDE Loss:  -5.6862 | Function Loss:  -3.7634\n",
      "Total loss:  -3.7145 | PDE Loss:  -5.6864 | Function Loss:  -3.7634\n",
      "Total loss:  -3.7145 | PDE Loss:  -5.6865 | Function Loss:  -3.7635\n",
      "Total loss:  -3.7146 | PDE Loss:  -5.6862 | Function Loss:  -3.7637\n",
      "Total loss:  -3.7147 | PDE Loss:  -5.6859 | Function Loss:  -3.7638\n",
      "Total loss:  -3.7148 | PDE Loss:  -5.6853 | Function Loss:  -3.764\n",
      "Total loss:  -3.7149 | PDE Loss:  -5.6848 | Function Loss:  -3.7641\n",
      "Total loss:  -3.715 | PDE Loss:  -5.6848 | Function Loss:  -3.7642\n",
      "Total loss:  -3.715 | PDE Loss:  -5.685 | Function Loss:  -3.7643\n",
      "Total loss:  -3.7151 | PDE Loss:  -5.6855 | Function Loss:  -3.7643\n",
      "Total loss:  -3.7152 | PDE Loss:  -5.6857 | Function Loss:  -3.7643\n",
      "Total loss:  -3.7152 | PDE Loss:  -5.6862 | Function Loss:  -3.7643\n",
      "Total loss:  -3.7154 | PDE Loss:  -5.6869 | Function Loss:  -3.7644\n",
      "Total loss:  -3.7155 | PDE Loss:  -5.6874 | Function Loss:  -3.7645\n",
      "Total loss:  -3.7157 | PDE Loss:  -5.6869 | Function Loss:  -3.7648\n",
      "Total loss:  -3.7159 | PDE Loss:  -5.6862 | Function Loss:  -3.7651\n",
      "Total loss:  -3.7161 | PDE Loss:  -5.6841 | Function Loss:  -3.7656\n",
      "Total loss:  -3.7163 | PDE Loss:  -5.6824 | Function Loss:  -3.766\n",
      "Total loss:  -3.7165 | PDE Loss:  -5.6796 | Function Loss:  -3.7665\n",
      "Total loss:  -3.7166 | PDE Loss:  -5.6775 | Function Loss:  -3.7669\n",
      "Total loss:  -3.7168 | PDE Loss:  -5.6751 | Function Loss:  -3.7674\n",
      "Total loss:  -3.7168 | PDE Loss:  -5.6734 | Function Loss:  -3.7677\n",
      "Total loss:  -3.7169 | PDE Loss:  -5.672 | Function Loss:  -3.768\n",
      "Total loss:  -3.717 | PDE Loss:  -5.6709 | Function Loss:  -3.7682\n",
      "Total loss:  -3.7171 | PDE Loss:  -5.6702 | Function Loss:  -3.7684\n",
      "Total loss:  -3.7172 | PDE Loss:  -5.67 | Function Loss:  -3.7685\n",
      "Total loss:  -3.7173 | PDE Loss:  -5.6701 | Function Loss:  -3.7686\n",
      "Total loss:  -3.7174 | PDE Loss:  -5.6697 | Function Loss:  -3.7688\n",
      "Total loss:  -3.7175 | PDE Loss:  -5.6705 | Function Loss:  -3.7689\n",
      "Total loss:  -3.7176 | PDE Loss:  -5.6704 | Function Loss:  -3.7689\n",
      "Total loss:  -3.7177 | PDE Loss:  -5.671 | Function Loss:  -3.769\n",
      "Total loss:  -3.7178 | PDE Loss:  -5.6709 | Function Loss:  -3.7691\n",
      "Total loss:  -3.7178 | PDE Loss:  -5.6709 | Function Loss:  -3.7692\n",
      "Total loss:  -3.7179 | PDE Loss:  -5.6707 | Function Loss:  -3.7693\n",
      "Total loss:  -3.718 | PDE Loss:  -5.6708 | Function Loss:  -3.7694\n",
      "Total loss:  -3.7182 | PDE Loss:  -5.6722 | Function Loss:  -3.7693\n",
      "Total loss:  -3.7183 | PDE Loss:  -5.6726 | Function Loss:  -3.7694\n",
      "Total loss:  -3.7184 | PDE Loss:  -5.6733 | Function Loss:  -3.7695\n",
      "Total loss:  -3.7185 | PDE Loss:  -5.6743 | Function Loss:  -3.7694\n",
      "Total loss:  -3.7186 | PDE Loss:  -5.6752 | Function Loss:  -3.7694\n",
      "Total loss:  -3.7186 | PDE Loss:  -5.6759 | Function Loss:  -3.7694\n",
      "Total loss:  -3.7187 | PDE Loss:  -5.6765 | Function Loss:  -3.7694\n",
      "Total loss:  -3.7188 | PDE Loss:  -5.6769 | Function Loss:  -3.7694\n",
      "Total loss:  -3.7188 | PDE Loss:  -5.6771 | Function Loss:  -3.7695\n",
      "Total loss:  -3.7189 | PDE Loss:  -5.6769 | Function Loss:  -3.7696\n",
      "Total loss:  -3.719 | PDE Loss:  -5.6759 | Function Loss:  -3.7698\n",
      "Total loss:  -3.7191 | PDE Loss:  -5.6747 | Function Loss:  -3.7701\n",
      "Total loss:  -3.7193 | PDE Loss:  -5.6734 | Function Loss:  -3.7704\n",
      "Total loss:  -3.7194 | PDE Loss:  -5.6716 | Function Loss:  -3.7708\n",
      "Total loss:  -3.7195 | PDE Loss:  -5.6704 | Function Loss:  -3.7711\n",
      "Total loss:  -3.7196 | PDE Loss:  -5.6685 | Function Loss:  -3.7715\n",
      "Total loss:  -3.7197 | PDE Loss:  -5.6677 | Function Loss:  -3.7717\n",
      "Total loss:  -3.7198 | PDE Loss:  -5.6672 | Function Loss:  -3.7718\n",
      "Total loss:  -3.7199 | PDE Loss:  -5.6669 | Function Loss:  -3.772\n",
      "Total loss:  -3.72 | PDE Loss:  -5.6666 | Function Loss:  -3.7721\n",
      "Total loss:  -3.7202 | PDE Loss:  -5.6665 | Function Loss:  -3.7723\n",
      "Total loss:  -3.7203 | PDE Loss:  -5.6664 | Function Loss:  -3.7725\n",
      "Total loss:  -3.7204 | PDE Loss:  -5.6666 | Function Loss:  -3.7725\n",
      "Total loss:  -3.7205 | PDE Loss:  -5.6667 | Function Loss:  -3.7727\n",
      "Total loss:  -3.7207 | PDE Loss:  -5.6667 | Function Loss:  -3.7728\n",
      "Total loss:  -3.7208 | PDE Loss:  -5.6667 | Function Loss:  -3.773\n",
      "Total loss:  -3.721 | PDE Loss:  -5.667 | Function Loss:  -3.7732\n",
      "Total loss:  -3.7212 | PDE Loss:  -5.667 | Function Loss:  -3.7734\n",
      "Total loss:  -3.7213 | PDE Loss:  -5.6678 | Function Loss:  -3.7735\n",
      "Total loss:  -3.7215 | PDE Loss:  -5.6688 | Function Loss:  -3.7736\n",
      "Total loss:  -3.7217 | PDE Loss:  -5.6702 | Function Loss:  -3.7736\n",
      "Total loss:  -3.7219 | PDE Loss:  -5.6713 | Function Loss:  -3.7737\n",
      "Total loss:  -3.7221 | PDE Loss:  -5.6734 | Function Loss:  -3.7737\n",
      "Total loss:  -3.7223 | PDE Loss:  -5.6757 | Function Loss:  -3.7735\n",
      "Total loss:  -3.7224 | PDE Loss:  -5.6777 | Function Loss:  -3.7734\n",
      "Total loss:  -3.7225 | PDE Loss:  -5.6793 | Function Loss:  -3.7733\n",
      "Total loss:  -3.7226 | PDE Loss:  -5.6809 | Function Loss:  -3.7732\n",
      "Total loss:  -3.7227 | PDE Loss:  -5.6818 | Function Loss:  -3.7732\n",
      "Total loss:  -3.7228 | PDE Loss:  -5.6825 | Function Loss:  -3.7732\n",
      "Total loss:  -3.7229 | PDE Loss:  -5.6828 | Function Loss:  -3.7733\n",
      "Total loss:  -3.7229 | PDE Loss:  -5.6827 | Function Loss:  -3.7734\n",
      "Total loss:  -3.723 | PDE Loss:  -5.6823 | Function Loss:  -3.7735\n",
      "Total loss:  -3.7231 | PDE Loss:  -5.6819 | Function Loss:  -3.7737\n",
      "Total loss:  -3.7232 | PDE Loss:  -5.6812 | Function Loss:  -3.7739\n",
      "Total loss:  -3.7234 | PDE Loss:  -5.6805 | Function Loss:  -3.7742\n",
      "Total loss:  -3.7235 | PDE Loss:  -5.6798 | Function Loss:  -3.7744\n",
      "Total loss:  -3.7237 | PDE Loss:  -5.6779 | Function Loss:  -3.7748\n",
      "Total loss:  -3.7239 | PDE Loss:  -5.6781 | Function Loss:  -3.775\n",
      "Total loss:  -3.7241 | PDE Loss:  -5.6782 | Function Loss:  -3.7752\n",
      "Total loss:  -3.7243 | PDE Loss:  -5.6762 | Function Loss:  -3.7757\n",
      "Total loss:  -3.7246 | PDE Loss:  -5.6797 | Function Loss:  -3.7756\n",
      "Total loss:  -3.7248 | PDE Loss:  -5.6814 | Function Loss:  -3.7756\n",
      "Total loss:  -3.725 | PDE Loss:  -5.684 | Function Loss:  -3.7755\n",
      "Total loss:  -3.7252 | PDE Loss:  -5.6865 | Function Loss:  -3.7755\n",
      "Total loss:  -3.7253 | PDE Loss:  -5.6885 | Function Loss:  -3.7754\n",
      "Total loss:  -3.7255 | PDE Loss:  -5.6906 | Function Loss:  -3.7753\n",
      "Total loss:  -3.7256 | PDE Loss:  -5.6915 | Function Loss:  -3.7753\n",
      "Total loss:  -3.7257 | PDE Loss:  -5.6923 | Function Loss:  -3.7754\n",
      "Total loss:  -3.7259 | PDE Loss:  -5.6918 | Function Loss:  -3.7756\n",
      "Total loss:  -3.726 | PDE Loss:  -5.6907 | Function Loss:  -3.7759\n",
      "Total loss:  -3.7262 | PDE Loss:  -5.6903 | Function Loss:  -3.7761\n",
      "Total loss:  -3.7264 | PDE Loss:  -5.689 | Function Loss:  -3.7765\n",
      "Total loss:  -3.7265 | PDE Loss:  -5.6869 | Function Loss:  -3.7769\n",
      "Total loss:  -3.7267 | PDE Loss:  -5.6871 | Function Loss:  -3.7771\n",
      "Total loss:  -3.7268 | PDE Loss:  -5.6864 | Function Loss:  -3.7773\n",
      "Total loss:  -3.727 | PDE Loss:  -5.6846 | Function Loss:  -3.7777\n",
      "Total loss:  -3.7271 | PDE Loss:  -5.6841 | Function Loss:  -3.7779\n",
      "Total loss:  -3.7272 | PDE Loss:  -5.6831 | Function Loss:  -3.7782\n",
      "Total loss:  -3.7273 | PDE Loss:  -5.6826 | Function Loss:  -3.7783\n",
      "Total loss:  -3.7274 | PDE Loss:  -5.6816 | Function Loss:  -3.7786\n",
      "Total loss:  -3.7276 | PDE Loss:  -5.6815 | Function Loss:  -3.7788\n",
      "Total loss:  -3.7278 | PDE Loss:  -5.68 | Function Loss:  -3.7792\n",
      "Total loss:  -3.728 | PDE Loss:  -5.6803 | Function Loss:  -3.7794\n",
      "Total loss:  -3.7283 | PDE Loss:  -5.6764 | Function Loss:  -3.7802\n",
      "Total loss:  -3.7284 | PDE Loss:  -5.6777 | Function Loss:  -3.7802\n",
      "Total loss:  -3.7288 | PDE Loss:  -5.6802 | Function Loss:  -3.7803\n",
      "Total loss:  -3.7291 | PDE Loss:  -5.6827 | Function Loss:  -3.7804\n",
      "Total loss:  -3.7295 | PDE Loss:  -5.6841 | Function Loss:  -3.7806\n",
      "Total loss:  -3.7297 | PDE Loss:  -5.6838 | Function Loss:  -3.7809\n",
      "Total loss:  -3.73 | PDE Loss:  -5.6817 | Function Loss:  -3.7815\n",
      "Total loss:  -3.7302 | PDE Loss:  -5.6794 | Function Loss:  -3.782\n",
      "Total loss:  -3.7304 | PDE Loss:  -5.6768 | Function Loss:  -3.7826\n",
      "Total loss:  -3.7306 | PDE Loss:  -5.6737 | Function Loss:  -3.7832\n",
      "Total loss:  -3.7308 | PDE Loss:  -5.6715 | Function Loss:  -3.7837\n",
      "Total loss:  -3.7309 | PDE Loss:  -5.6685 | Function Loss:  -3.7842\n",
      "Total loss:  -3.7311 | PDE Loss:  -5.6676 | Function Loss:  -3.7845\n",
      "Total loss:  -3.7312 | PDE Loss:  -5.6671 | Function Loss:  -3.7847\n",
      "Total loss:  -3.7314 | PDE Loss:  -5.6679 | Function Loss:  -3.7848\n",
      "Total loss:  -3.7316 | PDE Loss:  -5.6687 | Function Loss:  -3.7849\n",
      "Total loss:  -3.7317 | PDE Loss:  -5.6701 | Function Loss:  -3.7849\n",
      "Total loss:  -3.7318 | PDE Loss:  -5.671 | Function Loss:  -3.7849\n",
      "Total loss:  -3.7319 | PDE Loss:  -5.6719 | Function Loss:  -3.7849\n",
      "Total loss:  -3.7321 | PDE Loss:  -5.6736 | Function Loss:  -3.7848\n",
      "Total loss:  -3.7322 | PDE Loss:  -5.6741 | Function Loss:  -3.7849\n",
      "Total loss:  -3.7322 | PDE Loss:  -5.6752 | Function Loss:  -3.7848\n",
      "Total loss:  -3.7324 | PDE Loss:  -5.6764 | Function Loss:  -3.7848\n",
      "Total loss:  -3.732 | PDE Loss:  -5.6769 | Function Loss:  -3.7844\n",
      "Total loss:  -3.7324 | PDE Loss:  -5.6774 | Function Loss:  -3.7848\n",
      "Total loss:  -3.7326 | PDE Loss:  -5.6786 | Function Loss:  -3.7848\n",
      "Total loss:  -3.7327 | PDE Loss:  -5.6799 | Function Loss:  -3.7848\n",
      "Total loss:  -3.7329 | PDE Loss:  -5.6815 | Function Loss:  -3.7847\n",
      "Total loss:  -3.733 | PDE Loss:  -5.6835 | Function Loss:  -3.7846\n",
      "Total loss:  -3.7331 | PDE Loss:  -5.6857 | Function Loss:  -3.7845\n",
      "Total loss:  -3.7332 | PDE Loss:  -5.685 | Function Loss:  -3.7847\n",
      "Total loss:  -3.7334 | PDE Loss:  -5.6839 | Function Loss:  -3.785\n",
      "Total loss:  -3.7335 | PDE Loss:  -5.6829 | Function Loss:  -3.7852\n",
      "Total loss:  -3.7336 | PDE Loss:  -5.682 | Function Loss:  -3.7855\n",
      "Total loss:  -3.7336 | PDE Loss:  -5.6811 | Function Loss:  -3.7856\n",
      "Total loss:  -3.7337 | PDE Loss:  -5.6803 | Function Loss:  -3.7859\n",
      "Total loss:  -3.7338 | PDE Loss:  -5.6795 | Function Loss:  -3.7861\n",
      "Total loss:  -3.734 | PDE Loss:  -5.6789 | Function Loss:  -3.7863\n",
      "Total loss:  -3.7341 | PDE Loss:  -5.6785 | Function Loss:  -3.7865\n",
      "Total loss:  -3.7343 | PDE Loss:  -5.6783 | Function Loss:  -3.7867\n",
      "Total loss:  -3.7345 | PDE Loss:  -5.678 | Function Loss:  -3.787\n",
      "Total loss:  -3.7346 | PDE Loss:  -5.6774 | Function Loss:  -3.7872\n",
      "Total loss:  -3.7347 | PDE Loss:  -5.678 | Function Loss:  -3.7873\n",
      "Total loss:  -3.735 | PDE Loss:  -5.6788 | Function Loss:  -3.7875\n",
      "Total loss:  -3.7352 | PDE Loss:  -5.6794 | Function Loss:  -3.7876\n",
      "Total loss:  -3.7354 | PDE Loss:  -5.6798 | Function Loss:  -3.7878\n",
      "Total loss:  -3.7356 | PDE Loss:  -5.6801 | Function Loss:  -3.788\n",
      "Total loss:  -3.7359 | PDE Loss:  -5.6806 | Function Loss:  -3.7882\n",
      "Total loss:  -3.7361 | PDE Loss:  -5.6815 | Function Loss:  -3.7884\n",
      "Total loss:  -3.7363 | PDE Loss:  -5.683 | Function Loss:  -3.7885\n",
      "Total loss:  -3.7365 | PDE Loss:  -5.6831 | Function Loss:  -3.7887\n",
      "Total loss:  -3.7367 | PDE Loss:  -5.6848 | Function Loss:  -3.7886\n",
      "Total loss:  -3.7368 | PDE Loss:  -5.6849 | Function Loss:  -3.7888\n",
      "Total loss:  -3.737 | PDE Loss:  -5.6859 | Function Loss:  -3.7888\n",
      "Total loss:  -3.7372 | PDE Loss:  -5.6868 | Function Loss:  -3.7889\n",
      "Total loss:  -3.7374 | PDE Loss:  -5.688 | Function Loss:  -3.789\n",
      "Total loss:  -3.7376 | PDE Loss:  -5.6898 | Function Loss:  -3.789\n",
      "Total loss:  -3.7379 | PDE Loss:  -5.6912 | Function Loss:  -3.7892\n",
      "Total loss:  -3.7381 | PDE Loss:  -5.6918 | Function Loss:  -3.7893\n",
      "Total loss:  -3.7383 | PDE Loss:  -5.6929 | Function Loss:  -3.7894\n",
      "Total loss:  -3.7385 | PDE Loss:  -5.6939 | Function Loss:  -3.7895\n",
      "Total loss:  -3.7387 | PDE Loss:  -5.6945 | Function Loss:  -3.7896\n",
      "Total loss:  -3.7386 | PDE Loss:  -5.6967 | Function Loss:  -3.7893\n",
      "Total loss:  -3.7388 | PDE Loss:  -5.6959 | Function Loss:  -3.7895\n",
      "Total loss:  -3.7389 | PDE Loss:  -5.6966 | Function Loss:  -3.7897\n",
      "Total loss:  -3.7391 | PDE Loss:  -5.6975 | Function Loss:  -3.7898\n",
      "Total loss:  -3.7394 | PDE Loss:  -5.6989 | Function Loss:  -3.7899\n",
      "Total loss:  -3.7396 | PDE Loss:  -5.7003 | Function Loss:  -3.79\n",
      "Total loss:  -3.7399 | PDE Loss:  -5.7022 | Function Loss:  -3.79\n",
      "Total loss:  -3.74 | PDE Loss:  -5.7034 | Function Loss:  -3.7901\n",
      "Total loss:  -3.7403 | PDE Loss:  -5.7045 | Function Loss:  -3.7902\n",
      "Total loss:  -3.7405 | PDE Loss:  -5.7049 | Function Loss:  -3.7903\n",
      "Total loss:  -3.7406 | PDE Loss:  -5.7056 | Function Loss:  -3.7905\n",
      "Total loss:  -3.7408 | PDE Loss:  -5.7049 | Function Loss:  -3.7907\n",
      "Total loss:  -3.7408 | PDE Loss:  -5.7045 | Function Loss:  -3.7908\n",
      "Total loss:  -3.7409 | PDE Loss:  -5.704 | Function Loss:  -3.791\n",
      "Total loss:  -3.741 | PDE Loss:  -5.7032 | Function Loss:  -3.7911\n",
      "Total loss:  -3.741 | PDE Loss:  -5.7026 | Function Loss:  -3.7913\n",
      "Total loss:  -3.741 | PDE Loss:  -5.7021 | Function Loss:  -3.7913\n",
      "Total loss:  -3.7411 | PDE Loss:  -5.7018 | Function Loss:  -3.7915\n",
      "Total loss:  -3.7411 | PDE Loss:  -5.7014 | Function Loss:  -3.7915\n",
      "Total loss:  -3.7412 | PDE Loss:  -5.7014 | Function Loss:  -3.7916\n",
      "Total loss:  -3.7413 | PDE Loss:  -5.7018 | Function Loss:  -3.7917\n",
      "Total loss:  -3.7414 | PDE Loss:  -5.7026 | Function Loss:  -3.7917\n",
      "Total loss:  -3.7415 | PDE Loss:  -5.704 | Function Loss:  -3.7916\n",
      "Total loss:  -3.7416 | PDE Loss:  -5.7055 | Function Loss:  -3.7915\n",
      "Total loss:  -3.7416 | PDE Loss:  -5.7064 | Function Loss:  -3.7915\n",
      "Total loss:  -3.7417 | PDE Loss:  -5.7077 | Function Loss:  -3.7914\n",
      "Total loss:  -3.7418 | PDE Loss:  -5.7088 | Function Loss:  -3.7914\n",
      "Total loss:  -3.7419 | PDE Loss:  -5.7095 | Function Loss:  -3.7914\n",
      "Total loss:  -3.7419 | PDE Loss:  -5.7108 | Function Loss:  -3.7913\n",
      "Total loss:  -3.742 | PDE Loss:  -5.7101 | Function Loss:  -3.7915\n",
      "Total loss:  -3.7422 | PDE Loss:  -5.7092 | Function Loss:  -3.7917\n",
      "Total loss:  -3.7423 | PDE Loss:  -5.7078 | Function Loss:  -3.7921\n",
      "Total loss:  -3.7424 | PDE Loss:  -5.7064 | Function Loss:  -3.7924\n",
      "Total loss:  -3.7425 | PDE Loss:  -5.7053 | Function Loss:  -3.7926\n",
      "Total loss:  -3.7427 | PDE Loss:  -5.7041 | Function Loss:  -3.7929\n",
      "Total loss:  -3.7428 | PDE Loss:  -5.7035 | Function Loss:  -3.7931\n",
      "Total loss:  -3.7428 | PDE Loss:  -5.7032 | Function Loss:  -3.7932\n",
      "Total loss:  -3.7429 | PDE Loss:  -5.7032 | Function Loss:  -3.7933\n",
      "Total loss:  -3.7431 | PDE Loss:  -5.7036 | Function Loss:  -3.7935\n",
      "Total loss:  -3.7433 | PDE Loss:  -5.7043 | Function Loss:  -3.7936\n",
      "Total loss:  -3.7435 | PDE Loss:  -5.7039 | Function Loss:  -3.7939\n",
      "Total loss:  -3.7436 | PDE Loss:  -5.7039 | Function Loss:  -3.794\n",
      "Total loss:  -3.7437 | PDE Loss:  -5.7035 | Function Loss:  -3.7942\n",
      "Total loss:  -3.7438 | PDE Loss:  -5.7032 | Function Loss:  -3.7944\n",
      "Total loss:  -3.7439 | PDE Loss:  -5.7024 | Function Loss:  -3.7945\n",
      "Total loss:  -3.744 | PDE Loss:  -5.7023 | Function Loss:  -3.7946\n",
      "Total loss:  -3.744 | PDE Loss:  -5.7018 | Function Loss:  -3.7948\n",
      "Total loss:  -3.7441 | PDE Loss:  -5.7019 | Function Loss:  -3.7948\n",
      "Total loss:  -3.7442 | PDE Loss:  -5.7019 | Function Loss:  -3.7949\n",
      "Total loss:  -3.7443 | PDE Loss:  -5.7023 | Function Loss:  -3.7949\n",
      "Total loss:  -3.7443 | PDE Loss:  -5.7024 | Function Loss:  -3.795\n",
      "Total loss:  -3.7444 | PDE Loss:  -5.7027 | Function Loss:  -3.7951\n",
      "Total loss:  -3.7446 | PDE Loss:  -5.7029 | Function Loss:  -3.7952\n",
      "Total loss:  -3.7447 | PDE Loss:  -5.703 | Function Loss:  -3.7953\n",
      "Total loss:  -3.7448 | PDE Loss:  -5.7028 | Function Loss:  -3.7955\n",
      "Total loss:  -3.745 | PDE Loss:  -5.7025 | Function Loss:  -3.7957\n",
      "Total loss:  -3.7452 | PDE Loss:  -5.7029 | Function Loss:  -3.7959\n",
      "Total loss:  -3.7454 | PDE Loss:  -5.6996 | Function Loss:  -3.7966\n",
      "Total loss:  -3.7457 | PDE Loss:  -5.7016 | Function Loss:  -3.7966\n",
      "Total loss:  -3.746 | PDE Loss:  -5.7036 | Function Loss:  -3.7967\n",
      "Total loss:  -3.7464 | PDE Loss:  -5.7068 | Function Loss:  -3.7968\n",
      "Total loss:  -3.7467 | PDE Loss:  -5.7085 | Function Loss:  -3.7969\n",
      "Total loss:  -3.747 | PDE Loss:  -5.7106 | Function Loss:  -3.797\n",
      "Total loss:  -3.7472 | PDE Loss:  -5.7117 | Function Loss:  -3.7971\n",
      "Total loss:  -3.7474 | PDE Loss:  -5.7129 | Function Loss:  -3.7971\n",
      "Total loss:  -3.7475 | PDE Loss:  -5.7136 | Function Loss:  -3.7972\n",
      "Total loss:  -3.7477 | PDE Loss:  -5.7133 | Function Loss:  -3.7975\n",
      "Total loss:  -3.7479 | PDE Loss:  -5.7146 | Function Loss:  -3.7976\n",
      "Total loss:  -3.7481 | PDE Loss:  -5.7124 | Function Loss:  -3.798\n",
      "Total loss:  -3.7483 | PDE Loss:  -5.7113 | Function Loss:  -3.7984\n",
      "Total loss:  -3.7486 | PDE Loss:  -5.7093 | Function Loss:  -3.7989\n",
      "Total loss:  -3.7488 | PDE Loss:  -5.7077 | Function Loss:  -3.7993\n",
      "Total loss:  -3.749 | PDE Loss:  -5.7062 | Function Loss:  -3.7997\n",
      "Total loss:  -3.7491 | PDE Loss:  -5.7049 | Function Loss:  -3.8001\n",
      "Total loss:  -3.7494 | PDE Loss:  -5.7033 | Function Loss:  -3.8005\n",
      "Total loss:  -3.7496 | PDE Loss:  -5.7016 | Function Loss:  -3.801\n",
      "Total loss:  -3.7498 | PDE Loss:  -5.6999 | Function Loss:  -3.8015\n",
      "Total loss:  -3.7501 | PDE Loss:  -5.6986 | Function Loss:  -3.802\n",
      "Total loss:  -3.7504 | PDE Loss:  -5.6968 | Function Loss:  -3.8025\n",
      "Total loss:  -3.7506 | PDE Loss:  -5.6961 | Function Loss:  -3.8029\n",
      "Total loss:  -3.7508 | PDE Loss:  -5.696 | Function Loss:  -3.8031\n",
      "Total loss:  -3.7509 | PDE Loss:  -5.6966 | Function Loss:  -3.8031\n",
      "Total loss:  -3.7511 | PDE Loss:  -5.6974 | Function Loss:  -3.8032\n",
      "Total loss:  -3.7512 | PDE Loss:  -5.6981 | Function Loss:  -3.8033\n",
      "Total loss:  -3.7513 | PDE Loss:  -5.6987 | Function Loss:  -3.8033\n",
      "Total loss:  -3.7514 | PDE Loss:  -5.6994 | Function Loss:  -3.8034\n",
      "Total loss:  -3.7516 | PDE Loss:  -5.6995 | Function Loss:  -3.8035\n",
      "Total loss:  -3.7517 | PDE Loss:  -5.6998 | Function Loss:  -3.8036\n",
      "Total loss:  -3.7518 | PDE Loss:  -5.6993 | Function Loss:  -3.8038\n",
      "Total loss:  -3.7519 | PDE Loss:  -5.6992 | Function Loss:  -3.8039\n",
      "Total loss:  -3.7519 | PDE Loss:  -5.6982 | Function Loss:  -3.8041\n",
      "Total loss:  -3.752 | PDE Loss:  -5.6975 | Function Loss:  -3.8043\n",
      "Total loss:  -3.7521 | PDE Loss:  -5.6977 | Function Loss:  -3.8044\n",
      "Total loss:  -3.7522 | PDE Loss:  -5.6969 | Function Loss:  -3.8046\n",
      "Total loss:  -3.7523 | PDE Loss:  -5.6967 | Function Loss:  -3.8048\n",
      "Total loss:  -3.7525 | PDE Loss:  -5.6955 | Function Loss:  -3.8051\n",
      "Total loss:  -3.7527 | PDE Loss:  -5.6949 | Function Loss:  -3.8054\n",
      "Total loss:  -3.753 | PDE Loss:  -5.6944 | Function Loss:  -3.8058\n",
      "Total loss:  -3.7532 | PDE Loss:  -5.6939 | Function Loss:  -3.8061\n",
      "Total loss:  -3.7535 | PDE Loss:  -5.6931 | Function Loss:  -3.8065\n",
      "Total loss:  -3.7539 | PDE Loss:  -5.6934 | Function Loss:  -3.8069\n",
      "Total loss:  -3.7542 | PDE Loss:  -5.6925 | Function Loss:  -3.8074\n",
      "Total loss:  -3.7546 | PDE Loss:  -5.6933 | Function Loss:  -3.8077\n",
      "Total loss:  -3.7548 | PDE Loss:  -5.6958 | Function Loss:  -3.8077\n",
      "Total loss:  -3.755 | PDE Loss:  -5.6968 | Function Loss:  -3.8078\n",
      "Total loss:  -3.7552 | PDE Loss:  -5.6987 | Function Loss:  -3.8077\n",
      "Total loss:  -3.7553 | PDE Loss:  -5.7004 | Function Loss:  -3.8076\n",
      "Total loss:  -3.7555 | PDE Loss:  -5.7031 | Function Loss:  -3.8075\n",
      "Total loss:  -3.7556 | PDE Loss:  -5.7049 | Function Loss:  -3.8074\n",
      "Total loss:  -3.7558 | PDE Loss:  -5.7064 | Function Loss:  -3.8074\n",
      "Total loss:  -3.7558 | PDE Loss:  -5.7079 | Function Loss:  -3.8073\n",
      "Total loss:  -3.7559 | PDE Loss:  -5.7084 | Function Loss:  -3.8073\n",
      "Total loss:  -3.756 | PDE Loss:  -5.7088 | Function Loss:  -3.8073\n",
      "Total loss:  -3.7561 | PDE Loss:  -5.7091 | Function Loss:  -3.8074\n",
      "Total loss:  -3.7561 | PDE Loss:  -5.7093 | Function Loss:  -3.8074\n",
      "Total loss:  -3.7562 | PDE Loss:  -5.7095 | Function Loss:  -3.8075\n",
      "Total loss:  -3.7563 | PDE Loss:  -5.7094 | Function Loss:  -3.8076\n",
      "Total loss:  -3.7564 | PDE Loss:  -5.709 | Function Loss:  -3.8078\n",
      "Total loss:  -3.7565 | PDE Loss:  -5.7097 | Function Loss:  -3.8078\n",
      "Total loss:  -3.7566 | PDE Loss:  -5.7103 | Function Loss:  -3.8079\n",
      "Total loss:  -3.7568 | PDE Loss:  -5.711 | Function Loss:  -3.8079\n",
      "Total loss:  -3.7569 | PDE Loss:  -5.7118 | Function Loss:  -3.808\n",
      "Total loss:  -3.757 | PDE Loss:  -5.7126 | Function Loss:  -3.8079\n",
      "Total loss:  -3.7571 | PDE Loss:  -5.7135 | Function Loss:  -3.808\n",
      "Total loss:  -3.7572 | PDE Loss:  -5.7148 | Function Loss:  -3.8079\n",
      "Total loss:  -3.7573 | PDE Loss:  -5.7159 | Function Loss:  -3.8079\n",
      "Total loss:  -3.7574 | PDE Loss:  -5.7179 | Function Loss:  -3.8078\n",
      "Total loss:  -3.7575 | PDE Loss:  -5.7188 | Function Loss:  -3.8078\n",
      "Total loss:  -3.7577 | PDE Loss:  -5.7198 | Function Loss:  -3.8079\n",
      "Total loss:  -3.7579 | PDE Loss:  -5.7212 | Function Loss:  -3.8079\n",
      "Total loss:  -3.758 | PDE Loss:  -5.7221 | Function Loss:  -3.808\n",
      "Total loss:  -3.7582 | PDE Loss:  -5.7234 | Function Loss:  -3.808\n",
      "Total loss:  -3.7584 | PDE Loss:  -5.724 | Function Loss:  -3.8081\n",
      "Total loss:  -3.7586 | PDE Loss:  -5.7246 | Function Loss:  -3.8083\n",
      "Total loss:  -3.7588 | PDE Loss:  -5.7244 | Function Loss:  -3.8086\n",
      "Total loss:  -3.759 | PDE Loss:  -5.7242 | Function Loss:  -3.8088\n",
      "Total loss:  -3.7591 | PDE Loss:  -5.7249 | Function Loss:  -3.8089\n",
      "Total loss:  -3.7593 | PDE Loss:  -5.7246 | Function Loss:  -3.8091\n",
      "Total loss:  -3.7595 | PDE Loss:  -5.7242 | Function Loss:  -3.8094\n",
      "Total loss:  -3.7597 | PDE Loss:  -5.7236 | Function Loss:  -3.8096\n",
      "Total loss:  -3.7599 | PDE Loss:  -5.7226 | Function Loss:  -3.81\n",
      "Total loss:  -3.7601 | PDE Loss:  -5.7219 | Function Loss:  -3.8104\n",
      "Total loss:  -3.7604 | PDE Loss:  -5.7197 | Function Loss:  -3.8109\n",
      "Total loss:  -3.7606 | PDE Loss:  -5.7194 | Function Loss:  -3.8112\n",
      "Total loss:  -3.7608 | PDE Loss:  -5.717 | Function Loss:  -3.8117\n",
      "Total loss:  -3.761 | PDE Loss:  -5.7164 | Function Loss:  -3.812\n",
      "Total loss:  -3.7612 | PDE Loss:  -5.7154 | Function Loss:  -3.8123\n",
      "Total loss:  -3.7613 | PDE Loss:  -5.7146 | Function Loss:  -3.8126\n",
      "Total loss:  -3.7614 | PDE Loss:  -5.7145 | Function Loss:  -3.8127\n",
      "Total loss:  -3.7615 | PDE Loss:  -5.7146 | Function Loss:  -3.8128\n",
      "Total loss:  -3.7617 | PDE Loss:  -5.7147 | Function Loss:  -3.813\n",
      "Total loss:  -3.7618 | PDE Loss:  -5.7157 | Function Loss:  -3.813\n",
      "Total loss:  -3.7619 | PDE Loss:  -5.7163 | Function Loss:  -3.813\n",
      "Total loss:  -3.762 | PDE Loss:  -5.7171 | Function Loss:  -3.813\n",
      "Total loss:  -3.7621 | PDE Loss:  -5.7187 | Function Loss:  -3.813\n",
      "Total loss:  -3.7623 | PDE Loss:  -5.7206 | Function Loss:  -3.813\n",
      "Total loss:  -3.7624 | PDE Loss:  -5.722 | Function Loss:  -3.8129\n",
      "Total loss:  -3.7626 | PDE Loss:  -5.724 | Function Loss:  -3.8128\n",
      "Total loss:  -3.7627 | PDE Loss:  -5.7255 | Function Loss:  -3.8128\n",
      "Total loss:  -3.7628 | PDE Loss:  -5.7273 | Function Loss:  -3.8127\n",
      "Total loss:  -3.7629 | PDE Loss:  -5.7291 | Function Loss:  -3.8126\n",
      "Total loss:  -3.7631 | PDE Loss:  -5.7317 | Function Loss:  -3.8125\n",
      "Total loss:  -3.7633 | PDE Loss:  -5.7334 | Function Loss:  -3.8125\n",
      "Total loss:  -3.7636 | PDE Loss:  -5.7365 | Function Loss:  -3.8124\n",
      "Total loss:  -3.7638 | PDE Loss:  -5.7386 | Function Loss:  -3.8125\n",
      "Total loss:  -3.7641 | PDE Loss:  -5.7403 | Function Loss:  -3.8126\n",
      "Total loss:  -3.7643 | PDE Loss:  -5.7423 | Function Loss:  -3.8126\n",
      "Total loss:  -3.7645 | PDE Loss:  -5.7419 | Function Loss:  -3.8128\n",
      "Total loss:  -3.7646 | PDE Loss:  -5.7431 | Function Loss:  -3.8128\n",
      "Total loss:  -3.7647 | PDE Loss:  -5.7439 | Function Loss:  -3.8129\n",
      "Total loss:  -3.7648 | PDE Loss:  -5.7443 | Function Loss:  -3.8129\n",
      "Total loss:  -3.7649 | PDE Loss:  -5.7449 | Function Loss:  -3.8129\n",
      "Total loss:  -3.7649 | PDE Loss:  -5.7457 | Function Loss:  -3.8129\n",
      "Total loss:  -3.7651 | PDE Loss:  -5.7467 | Function Loss:  -3.8129\n",
      "Total loss:  -3.7652 | PDE Loss:  -5.7481 | Function Loss:  -3.8128\n",
      "Total loss:  -3.7652 | PDE Loss:  -5.75 | Function Loss:  -3.8127\n",
      "Total loss:  -3.7653 | PDE Loss:  -5.7515 | Function Loss:  -3.8126\n",
      "Total loss:  -3.7654 | PDE Loss:  -5.7526 | Function Loss:  -3.8126\n",
      "Total loss:  -3.7654 | PDE Loss:  -5.7537 | Function Loss:  -3.8125\n",
      "Total loss:  -3.7655 | PDE Loss:  -5.7548 | Function Loss:  -3.8125\n",
      "Total loss:  -3.7655 | PDE Loss:  -5.7555 | Function Loss:  -3.8124\n",
      "Total loss:  -3.7656 | PDE Loss:  -5.7558 | Function Loss:  -3.8125\n",
      "Total loss:  -3.7657 | PDE Loss:  -5.7563 | Function Loss:  -3.8125\n",
      "Total loss:  -3.7658 | PDE Loss:  -5.7564 | Function Loss:  -3.8126\n",
      "Total loss:  -3.7658 | PDE Loss:  -5.7565 | Function Loss:  -3.8126\n",
      "Total loss:  -3.7659 | PDE Loss:  -5.7568 | Function Loss:  -3.8127\n",
      "Total loss:  -3.766 | PDE Loss:  -5.7577 | Function Loss:  -3.8127\n",
      "Total loss:  -3.7661 | PDE Loss:  -5.7581 | Function Loss:  -3.8128\n",
      "Total loss:  -3.7662 | PDE Loss:  -5.7595 | Function Loss:  -3.8127\n",
      "Total loss:  -3.7663 | PDE Loss:  -5.76 | Function Loss:  -3.8128\n",
      "Total loss:  -3.7664 | PDE Loss:  -5.761 | Function Loss:  -3.8127\n",
      "Total loss:  -3.7664 | PDE Loss:  -5.7616 | Function Loss:  -3.8127\n",
      "Total loss:  -3.7665 | PDE Loss:  -5.7623 | Function Loss:  -3.8127\n",
      "Total loss:  -3.7666 | PDE Loss:  -5.763 | Function Loss:  -3.8127\n",
      "Total loss:  -3.7667 | PDE Loss:  -5.7636 | Function Loss:  -3.8128\n",
      "Total loss:  -3.7668 | PDE Loss:  -5.7638 | Function Loss:  -3.8128\n",
      "Total loss:  -3.7668 | PDE Loss:  -5.764 | Function Loss:  -3.8129\n",
      "Total loss:  -3.7669 | PDE Loss:  -5.7634 | Function Loss:  -3.8131\n",
      "Total loss:  -3.767 | PDE Loss:  -5.7631 | Function Loss:  -3.8132\n",
      "Total loss:  -3.7671 | PDE Loss:  -5.7623 | Function Loss:  -3.8134\n",
      "Total loss:  -3.7672 | PDE Loss:  -5.7611 | Function Loss:  -3.8137\n",
      "Total loss:  -3.7673 | PDE Loss:  -5.7601 | Function Loss:  -3.8139\n",
      "Total loss:  -3.7674 | PDE Loss:  -5.7589 | Function Loss:  -3.8141\n",
      "Total loss:  -3.7675 | PDE Loss:  -5.7581 | Function Loss:  -3.8144\n",
      "Total loss:  -3.7677 | PDE Loss:  -5.7568 | Function Loss:  -3.8146\n",
      "Total loss:  -3.7678 | PDE Loss:  -5.7565 | Function Loss:  -3.8148\n",
      "Total loss:  -3.7678 | PDE Loss:  -5.7553 | Function Loss:  -3.815\n",
      "Total loss:  -3.7679 | PDE Loss:  -5.7548 | Function Loss:  -3.8152\n",
      "Total loss:  -3.768 | PDE Loss:  -5.7546 | Function Loss:  -3.8153\n",
      "Total loss:  -3.7681 | PDE Loss:  -5.7548 | Function Loss:  -3.8153\n",
      "Total loss:  -3.7681 | PDE Loss:  -5.7546 | Function Loss:  -3.8154\n",
      "Total loss:  -3.7682 | PDE Loss:  -5.755 | Function Loss:  -3.8154\n",
      "Total loss:  -3.7682 | PDE Loss:  -5.7551 | Function Loss:  -3.8154\n",
      "Total loss:  -3.7682 | PDE Loss:  -5.7553 | Function Loss:  -3.8155\n",
      "Total loss:  -3.7683 | PDE Loss:  -5.7554 | Function Loss:  -3.8155\n",
      "Total loss:  -3.7684 | PDE Loss:  -5.7558 | Function Loss:  -3.8156\n",
      "Total loss:  -3.7684 | PDE Loss:  -5.7556 | Function Loss:  -3.8157\n",
      "Total loss:  -3.7685 | PDE Loss:  -5.7552 | Function Loss:  -3.8158\n",
      "Total loss:  -3.7686 | PDE Loss:  -5.7547 | Function Loss:  -3.8159\n",
      "Total loss:  -3.7686 | PDE Loss:  -5.7541 | Function Loss:  -3.816\n",
      "Total loss:  -3.7687 | PDE Loss:  -5.7534 | Function Loss:  -3.8162\n",
      "Total loss:  -3.7688 | PDE Loss:  -5.7522 | Function Loss:  -3.8164\n",
      "Total loss:  -3.7688 | PDE Loss:  -5.7512 | Function Loss:  -3.8166\n",
      "Total loss:  -3.7689 | PDE Loss:  -5.7499 | Function Loss:  -3.8168\n",
      "Total loss:  -3.7689 | PDE Loss:  -5.7495 | Function Loss:  -3.8169\n",
      "Total loss:  -3.769 | PDE Loss:  -5.7487 | Function Loss:  -3.817\n",
      "Total loss:  -3.769 | PDE Loss:  -5.7483 | Function Loss:  -3.8171\n",
      "Total loss:  -3.769 | PDE Loss:  -5.7478 | Function Loss:  -3.8172\n",
      "Total loss:  -3.7691 | PDE Loss:  -5.7471 | Function Loss:  -3.8174\n",
      "Total loss:  -3.7692 | PDE Loss:  -5.7465 | Function Loss:  -3.8175\n",
      "Total loss:  -3.7692 | PDE Loss:  -5.7457 | Function Loss:  -3.8177\n",
      "Total loss:  -3.7693 | PDE Loss:  -5.7442 | Function Loss:  -3.8179\n",
      "Total loss:  -3.7694 | PDE Loss:  -5.7437 | Function Loss:  -3.8181\n",
      "Total loss:  -3.7694 | PDE Loss:  -5.7404 | Function Loss:  -3.8185\n",
      "Total loss:  -3.7695 | PDE Loss:  -5.7408 | Function Loss:  -3.8186\n",
      "Total loss:  -3.7696 | PDE Loss:  -5.7416 | Function Loss:  -3.8186\n",
      "Total loss:  -3.7697 | PDE Loss:  -5.7429 | Function Loss:  -3.8186\n",
      "Total loss:  -3.7698 | PDE Loss:  -5.7434 | Function Loss:  -3.8186\n",
      "Total loss:  -3.77 | PDE Loss:  -5.7456 | Function Loss:  -3.8185\n",
      "Total loss:  -3.7701 | PDE Loss:  -5.7459 | Function Loss:  -3.8186\n",
      "Total loss:  -3.7702 | PDE Loss:  -5.7459 | Function Loss:  -3.8188\n",
      "Total loss:  -3.7704 | PDE Loss:  -5.7457 | Function Loss:  -3.819\n",
      "Total loss:  -3.7706 | PDE Loss:  -5.7451 | Function Loss:  -3.8193\n",
      "Total loss:  -3.7708 | PDE Loss:  -5.7451 | Function Loss:  -3.8195\n",
      "Total loss:  -3.771 | PDE Loss:  -5.7439 | Function Loss:  -3.8199\n",
      "Total loss:  -3.7711 | PDE Loss:  -5.7435 | Function Loss:  -3.8201\n",
      "Total loss:  -3.7713 | PDE Loss:  -5.7426 | Function Loss:  -3.8203\n",
      "Total loss:  -3.7714 | PDE Loss:  -5.7428 | Function Loss:  -3.8205\n",
      "Total loss:  -3.7715 | PDE Loss:  -5.742 | Function Loss:  -3.8207\n",
      "Total loss:  -3.7717 | PDE Loss:  -5.7421 | Function Loss:  -3.8209\n",
      "Total loss:  -3.7718 | PDE Loss:  -5.7419 | Function Loss:  -3.821\n",
      "Total loss:  -3.772 | PDE Loss:  -5.7423 | Function Loss:  -3.8211\n",
      "Total loss:  -3.7721 | PDE Loss:  -5.7429 | Function Loss:  -3.8212\n",
      "Total loss:  -3.7723 | PDE Loss:  -5.743 | Function Loss:  -3.8215\n",
      "Total loss:  -3.7726 | PDE Loss:  -5.7459 | Function Loss:  -3.8214\n",
      "Total loss:  -3.7728 | PDE Loss:  -5.7466 | Function Loss:  -3.8215\n",
      "Total loss:  -3.773 | PDE Loss:  -5.7486 | Function Loss:  -3.8216\n",
      "Total loss:  -3.7733 | PDE Loss:  -5.7513 | Function Loss:  -3.8215\n",
      "Total loss:  -3.7735 | PDE Loss:  -5.7517 | Function Loss:  -3.8217\n",
      "Total loss:  -3.7736 | PDE Loss:  -5.7529 | Function Loss:  -3.8217\n",
      "Total loss:  -3.7738 | PDE Loss:  -5.7545 | Function Loss:  -3.8217\n",
      "Total loss:  -3.7739 | PDE Loss:  -5.7552 | Function Loss:  -3.8218\n",
      "Total loss:  -3.7741 | PDE Loss:  -5.7576 | Function Loss:  -3.8217\n",
      "Total loss:  -3.7742 | PDE Loss:  -5.7581 | Function Loss:  -3.8218\n",
      "Total loss:  -3.7743 | PDE Loss:  -5.7592 | Function Loss:  -3.8218\n",
      "Total loss:  -3.7745 | PDE Loss:  -5.76 | Function Loss:  -3.8219\n",
      "Total loss:  -3.7747 | PDE Loss:  -5.7602 | Function Loss:  -3.8221\n",
      "Total loss:  -3.775 | PDE Loss:  -5.7606 | Function Loss:  -3.8223\n",
      "Total loss:  -3.7751 | PDE Loss:  -5.7602 | Function Loss:  -3.8226\n",
      "Total loss:  -3.7753 | PDE Loss:  -5.7608 | Function Loss:  -3.8227\n",
      "Total loss:  -3.7754 | PDE Loss:  -5.7607 | Function Loss:  -3.8228\n",
      "Total loss:  -3.7755 | PDE Loss:  -5.7609 | Function Loss:  -3.8229\n",
      "Total loss:  -3.7756 | PDE Loss:  -5.7607 | Function Loss:  -3.823\n",
      "Total loss:  -3.7757 | PDE Loss:  -5.7606 | Function Loss:  -3.8232\n",
      "Total loss:  -3.7757 | PDE Loss:  -5.7607 | Function Loss:  -3.8232\n",
      "Total loss:  -3.7758 | PDE Loss:  -5.7606 | Function Loss:  -3.8233\n",
      "Total loss:  -3.7759 | PDE Loss:  -5.7606 | Function Loss:  -3.8234\n",
      "Total loss:  -3.7761 | PDE Loss:  -5.7604 | Function Loss:  -3.8236\n",
      "Total loss:  -3.7762 | PDE Loss:  -5.7606 | Function Loss:  -3.8238\n",
      "Total loss:  -3.7753 | PDE Loss:  -5.7504 | Function Loss:  -3.8239\n",
      "Total loss:  -3.7763 | PDE Loss:  -5.7595 | Function Loss:  -3.8239\n",
      "Total loss:  -3.7764 | PDE Loss:  -5.76 | Function Loss:  -3.824\n",
      "Total loss:  -3.7765 | PDE Loss:  -5.7597 | Function Loss:  -3.8242\n",
      "Total loss:  -3.7766 | PDE Loss:  -5.7599 | Function Loss:  -3.8242\n",
      "Total loss:  -3.7767 | PDE Loss:  -5.7602 | Function Loss:  -3.8243\n",
      "Total loss:  -3.7767 | PDE Loss:  -5.7604 | Function Loss:  -3.8243\n",
      "Total loss:  -3.7768 | PDE Loss:  -5.7608 | Function Loss:  -3.8243\n",
      "Total loss:  -3.7768 | PDE Loss:  -5.7611 | Function Loss:  -3.8243\n",
      "Total loss:  -3.7769 | PDE Loss:  -5.7608 | Function Loss:  -3.8244\n",
      "Total loss:  -3.7769 | PDE Loss:  -5.7612 | Function Loss:  -3.8244\n",
      "Total loss:  -3.777 | PDE Loss:  -5.762 | Function Loss:  -3.8244\n",
      "Total loss:  -3.7771 | PDE Loss:  -5.7613 | Function Loss:  -3.8246\n",
      "Total loss:  -3.7772 | PDE Loss:  -5.7633 | Function Loss:  -3.8245\n",
      "Total loss:  -3.7772 | PDE Loss:  -5.7631 | Function Loss:  -3.8246\n",
      "Total loss:  -3.7773 | PDE Loss:  -5.7629 | Function Loss:  -3.8247\n",
      "Total loss:  -3.7775 | PDE Loss:  -5.7625 | Function Loss:  -3.8249\n",
      "Total loss:  -3.7776 | PDE Loss:  -5.7626 | Function Loss:  -3.825\n",
      "Total loss:  -3.7777 | PDE Loss:  -5.7622 | Function Loss:  -3.8252\n",
      "Total loss:  -3.7777 | PDE Loss:  -5.7622 | Function Loss:  -3.8252\n",
      "Total loss:  -3.7778 | PDE Loss:  -5.7622 | Function Loss:  -3.8253\n",
      "Total loss:  -3.7779 | PDE Loss:  -5.7626 | Function Loss:  -3.8253\n",
      "Total loss:  -3.7779 | PDE Loss:  -5.7635 | Function Loss:  -3.8253\n",
      "Total loss:  -3.778 | PDE Loss:  -5.7642 | Function Loss:  -3.8253\n",
      "Total loss:  -3.7781 | PDE Loss:  -5.7647 | Function Loss:  -3.8254\n",
      "Total loss:  -3.7782 | PDE Loss:  -5.7655 | Function Loss:  -3.8254\n",
      "Total loss:  -3.7783 | PDE Loss:  -5.7656 | Function Loss:  -3.8254\n",
      "Total loss:  -3.7784 | PDE Loss:  -5.7658 | Function Loss:  -3.8255\n",
      "Total loss:  -3.7785 | PDE Loss:  -5.7651 | Function Loss:  -3.8257\n",
      "Total loss:  -3.7786 | PDE Loss:  -5.7646 | Function Loss:  -3.8259\n",
      "Total loss:  -3.7786 | PDE Loss:  -5.7638 | Function Loss:  -3.8261\n",
      "Total loss:  -3.7787 | PDE Loss:  -5.7631 | Function Loss:  -3.8262\n",
      "Total loss:  -3.7787 | PDE Loss:  -5.7624 | Function Loss:  -3.8264\n",
      "Total loss:  -3.7789 | PDE Loss:  -5.7617 | Function Loss:  -3.8266\n",
      "Total loss:  -3.779 | PDE Loss:  -5.7609 | Function Loss:  -3.8268\n",
      "Total loss:  -3.7791 | PDE Loss:  -5.7603 | Function Loss:  -3.827\n",
      "Total loss:  -3.7793 | PDE Loss:  -5.7595 | Function Loss:  -3.8274\n",
      "Total loss:  -3.7795 | PDE Loss:  -5.7568 | Function Loss:  -3.8278\n",
      "Total loss:  -3.7797 | PDE Loss:  -5.757 | Function Loss:  -3.828\n",
      "Total loss:  -3.7799 | PDE Loss:  -5.7573 | Function Loss:  -3.8283\n",
      "Total loss:  -3.7801 | PDE Loss:  -5.7578 | Function Loss:  -3.8284\n",
      "Total loss:  -3.7803 | PDE Loss:  -5.7577 | Function Loss:  -3.8287\n",
      "Total loss:  -3.7804 | PDE Loss:  -5.7577 | Function Loss:  -3.8288\n",
      "Total loss:  -3.7806 | PDE Loss:  -5.7574 | Function Loss:  -3.829\n",
      "Total loss:  -3.7806 | PDE Loss:  -5.7557 | Function Loss:  -3.8292\n",
      "Total loss:  -3.7807 | PDE Loss:  -5.7563 | Function Loss:  -3.8293\n",
      "Total loss:  -3.7808 | PDE Loss:  -5.7532 | Function Loss:  -3.8298\n",
      "Total loss:  -3.7809 | PDE Loss:  -5.7525 | Function Loss:  -3.8299\n",
      "Total loss:  -3.781 | PDE Loss:  -5.7516 | Function Loss:  -3.8301\n",
      "Total loss:  -3.781 | PDE Loss:  -5.751 | Function Loss:  -3.8303\n",
      "Total loss:  -3.7811 | PDE Loss:  -5.7506 | Function Loss:  -3.8304\n",
      "Total loss:  -3.7812 | PDE Loss:  -5.7503 | Function Loss:  -3.8305\n",
      "Total loss:  -3.7813 | PDE Loss:  -5.7501 | Function Loss:  -3.8306\n",
      "Total loss:  -3.7813 | PDE Loss:  -5.7501 | Function Loss:  -3.8307\n",
      "Total loss:  -3.7813 | PDE Loss:  -5.7501 | Function Loss:  -3.8307\n",
      "Total loss:  -3.7814 | PDE Loss:  -5.7503 | Function Loss:  -3.8308\n",
      "Total loss:  -3.7814 | PDE Loss:  -5.7503 | Function Loss:  -3.8308\n",
      "Total loss:  -3.7815 | PDE Loss:  -5.7506 | Function Loss:  -3.8308\n",
      "Total loss:  -3.7815 | PDE Loss:  -5.7509 | Function Loss:  -3.8308\n",
      "Total loss:  -3.7816 | PDE Loss:  -5.7513 | Function Loss:  -3.8308\n",
      "Total loss:  -3.7816 | PDE Loss:  -5.7521 | Function Loss:  -3.8308\n",
      "Total loss:  -3.7817 | PDE Loss:  -5.7531 | Function Loss:  -3.8307\n",
      "Total loss:  -3.7818 | PDE Loss:  -5.7536 | Function Loss:  -3.8308\n",
      "Total loss:  -3.7817 | PDE Loss:  -5.7557 | Function Loss:  -3.8305\n",
      "Total loss:  -3.7818 | PDE Loss:  -5.7547 | Function Loss:  -3.8307\n",
      "Total loss:  -3.7818 | PDE Loss:  -5.755 | Function Loss:  -3.8307\n",
      "Total loss:  -3.7819 | PDE Loss:  -5.7551 | Function Loss:  -3.8307\n",
      "Total loss:  -3.7819 | PDE Loss:  -5.7556 | Function Loss:  -3.8307\n",
      "Total loss:  -3.782 | PDE Loss:  -5.7559 | Function Loss:  -3.8308\n",
      "Total loss:  -3.7821 | PDE Loss:  -5.7564 | Function Loss:  -3.8308\n",
      "Total loss:  -3.7821 | PDE Loss:  -5.7568 | Function Loss:  -3.8308\n",
      "Total loss:  -3.7822 | PDE Loss:  -5.7576 | Function Loss:  -3.8308\n",
      "Total loss:  -3.7823 | PDE Loss:  -5.7579 | Function Loss:  -3.8308\n",
      "Total loss:  -3.7824 | PDE Loss:  -5.7588 | Function Loss:  -3.8309\n",
      "Total loss:  -3.7826 | PDE Loss:  -5.7592 | Function Loss:  -3.831\n",
      "Total loss:  -3.7827 | PDE Loss:  -5.7601 | Function Loss:  -3.8311\n",
      "Total loss:  -3.7829 | PDE Loss:  -5.7603 | Function Loss:  -3.8312\n",
      "Total loss:  -3.783 | PDE Loss:  -5.7607 | Function Loss:  -3.8313\n",
      "Total loss:  -3.7831 | PDE Loss:  -5.7607 | Function Loss:  -3.8314\n",
      "Total loss:  -3.7832 | PDE Loss:  -5.7607 | Function Loss:  -3.8315\n",
      "Total loss:  -3.7832 | PDE Loss:  -5.7607 | Function Loss:  -3.8315\n",
      "Total loss:  -3.7833 | PDE Loss:  -5.7608 | Function Loss:  -3.8317\n",
      "Total loss:  -3.7834 | PDE Loss:  -5.7622 | Function Loss:  -3.8316\n",
      "Total loss:  -3.7835 | PDE Loss:  -5.7626 | Function Loss:  -3.8316\n",
      "Total loss:  -3.7835 | PDE Loss:  -5.7632 | Function Loss:  -3.8316\n",
      "Total loss:  -3.7836 | PDE Loss:  -5.7638 | Function Loss:  -3.8316\n",
      "Total loss:  -3.7837 | PDE Loss:  -5.7648 | Function Loss:  -3.8316\n",
      "Total loss:  -3.7838 | PDE Loss:  -5.7651 | Function Loss:  -3.8316\n",
      "Total loss:  -3.7838 | PDE Loss:  -5.7655 | Function Loss:  -3.8317\n",
      "Total loss:  -3.7839 | PDE Loss:  -5.766 | Function Loss:  -3.8317\n",
      "Total loss:  -3.784 | PDE Loss:  -5.7658 | Function Loss:  -3.8318\n",
      "Total loss:  -3.7841 | PDE Loss:  -5.7653 | Function Loss:  -3.832\n",
      "Total loss:  -3.7842 | PDE Loss:  -5.7636 | Function Loss:  -3.8323\n",
      "Total loss:  -3.7843 | PDE Loss:  -5.7632 | Function Loss:  -3.8325\n",
      "Total loss:  -3.7844 | PDE Loss:  -5.7616 | Function Loss:  -3.8328\n",
      "Total loss:  -3.7845 | PDE Loss:  -5.7599 | Function Loss:  -3.8331\n",
      "Total loss:  -3.7846 | PDE Loss:  -5.7581 | Function Loss:  -3.8334\n",
      "Total loss:  -3.7848 | PDE Loss:  -5.7568 | Function Loss:  -3.8338\n",
      "Total loss:  -3.7849 | PDE Loss:  -5.7552 | Function Loss:  -3.8341\n",
      "Total loss:  -3.7851 | PDE Loss:  -5.7554 | Function Loss:  -3.8343\n",
      "Total loss:  -3.7852 | PDE Loss:  -5.7559 | Function Loss:  -3.8344\n",
      "Total loss:  -3.7854 | PDE Loss:  -5.7564 | Function Loss:  -3.8345\n",
      "Total loss:  -3.7855 | PDE Loss:  -5.7563 | Function Loss:  -3.8347\n",
      "Total loss:  -3.7857 | PDE Loss:  -5.7557 | Function Loss:  -3.8349\n",
      "Total loss:  -3.7858 | PDE Loss:  -5.7553 | Function Loss:  -3.8351\n",
      "Total loss:  -3.786 | PDE Loss:  -5.7549 | Function Loss:  -3.8354\n",
      "Total loss:  -3.7862 | PDE Loss:  -5.7544 | Function Loss:  -3.8357\n",
      "Total loss:  -3.7864 | PDE Loss:  -5.7532 | Function Loss:  -3.836\n",
      "Total loss:  -3.7866 | PDE Loss:  -5.7517 | Function Loss:  -3.8364\n",
      "Total loss:  -3.7867 | PDE Loss:  -5.7505 | Function Loss:  -3.8367\n",
      "Total loss:  -3.7869 | PDE Loss:  -5.7491 | Function Loss:  -3.8371\n",
      "Total loss:  -3.7871 | PDE Loss:  -5.7471 | Function Loss:  -3.8375\n",
      "Total loss:  -3.7873 | PDE Loss:  -5.7453 | Function Loss:  -3.838\n",
      "Total loss:  -3.7875 | PDE Loss:  -5.7429 | Function Loss:  -3.8385\n",
      "Total loss:  -3.7876 | PDE Loss:  -5.7412 | Function Loss:  -3.8389\n",
      "Total loss:  -3.7878 | PDE Loss:  -5.7403 | Function Loss:  -3.8391\n",
      "Total loss:  -3.7879 | PDE Loss:  -5.7389 | Function Loss:  -3.8395\n",
      "Total loss:  -3.788 | PDE Loss:  -5.7388 | Function Loss:  -3.8395\n",
      "Total loss:  -3.788 | PDE Loss:  -5.7389 | Function Loss:  -3.8396\n",
      "Total loss:  -3.7881 | PDE Loss:  -5.739 | Function Loss:  -3.8397\n",
      "Total loss:  -3.7883 | PDE Loss:  -5.7392 | Function Loss:  -3.8399\n",
      "Total loss:  -3.7884 | PDE Loss:  -5.7387 | Function Loss:  -3.84\n",
      "Total loss:  -3.7885 | PDE Loss:  -5.7379 | Function Loss:  -3.8402\n",
      "Total loss:  -3.7886 | PDE Loss:  -5.737 | Function Loss:  -3.8405\n",
      "Total loss:  -3.7887 | PDE Loss:  -5.7357 | Function Loss:  -3.8407\n",
      "Total loss:  -3.7888 | PDE Loss:  -5.7344 | Function Loss:  -3.841\n",
      "Total loss:  -3.7888 | PDE Loss:  -5.7331 | Function Loss:  -3.8413\n",
      "Total loss:  -3.7889 | PDE Loss:  -5.732 | Function Loss:  -3.8415\n",
      "Total loss:  -3.7891 | PDE Loss:  -5.7309 | Function Loss:  -3.8418\n",
      "Total loss:  -3.7892 | PDE Loss:  -5.7294 | Function Loss:  -3.8421\n",
      "Total loss:  -3.7893 | PDE Loss:  -5.7288 | Function Loss:  -3.8424\n",
      "Total loss:  -3.7894 | PDE Loss:  -5.7285 | Function Loss:  -3.8425\n",
      "Total loss:  -3.7895 | PDE Loss:  -5.7282 | Function Loss:  -3.8426\n",
      "Total loss:  -3.7897 | PDE Loss:  -5.7282 | Function Loss:  -3.8428\n",
      "Total loss:  -3.7899 | PDE Loss:  -5.7274 | Function Loss:  -3.8431\n",
      "Total loss:  -3.79 | PDE Loss:  -5.7263 | Function Loss:  -3.8435\n",
      "Total loss:  -3.7902 | PDE Loss:  -5.7261 | Function Loss:  -3.8436\n",
      "Total loss:  -3.7903 | PDE Loss:  -5.7244 | Function Loss:  -3.844\n",
      "Total loss:  -3.7904 | PDE Loss:  -5.7243 | Function Loss:  -3.8441\n",
      "Total loss:  -3.7905 | PDE Loss:  -5.7239 | Function Loss:  -3.8443\n",
      "Total loss:  -3.7905 | PDE Loss:  -5.724 | Function Loss:  -3.8444\n",
      "Total loss:  -3.7906 | PDE Loss:  -5.7241 | Function Loss:  -3.8444\n",
      "Total loss:  -3.7906 | PDE Loss:  -5.7243 | Function Loss:  -3.8444\n",
      "Total loss:  -3.7908 | PDE Loss:  -5.7249 | Function Loss:  -3.8445\n",
      "Total loss:  -3.7909 | PDE Loss:  -5.7255 | Function Loss:  -3.8445\n",
      "Total loss:  -3.7909 | PDE Loss:  -5.7258 | Function Loss:  -3.8446\n",
      "Total loss:  -3.791 | PDE Loss:  -5.7261 | Function Loss:  -3.8446\n",
      "Total loss:  -3.7911 | PDE Loss:  -5.7262 | Function Loss:  -3.8447\n",
      "Total loss:  -3.7912 | PDE Loss:  -5.7246 | Function Loss:  -3.845\n",
      "Total loss:  -3.7913 | PDE Loss:  -5.7254 | Function Loss:  -3.845\n",
      "Total loss:  -3.7914 | PDE Loss:  -5.7253 | Function Loss:  -3.8452\n",
      "Total loss:  -3.7915 | PDE Loss:  -5.7249 | Function Loss:  -3.8453\n",
      "Total loss:  -3.7916 | PDE Loss:  -5.7238 | Function Loss:  -3.8456\n",
      "Total loss:  -3.7916 | PDE Loss:  -5.723 | Function Loss:  -3.8457\n",
      "Total loss:  -3.7917 | PDE Loss:  -5.7217 | Function Loss:  -3.846\n",
      "Total loss:  -3.7918 | PDE Loss:  -5.7207 | Function Loss:  -3.8462\n",
      "Total loss:  -3.7919 | PDE Loss:  -5.7196 | Function Loss:  -3.8464\n",
      "Total loss:  -3.7919 | PDE Loss:  -5.7183 | Function Loss:  -3.8467\n",
      "Total loss:  -3.792 | PDE Loss:  -5.7177 | Function Loss:  -3.8468\n",
      "Total loss:  -3.792 | PDE Loss:  -5.7172 | Function Loss:  -3.847\n",
      "Total loss:  -3.7921 | PDE Loss:  -5.7172 | Function Loss:  -3.8471\n",
      "Total loss:  -3.7922 | PDE Loss:  -5.7169 | Function Loss:  -3.8472\n",
      "Total loss:  -3.7923 | PDE Loss:  -5.7173 | Function Loss:  -3.8472\n",
      "Total loss:  -3.7923 | PDE Loss:  -5.7166 | Function Loss:  -3.8474\n",
      "Total loss:  -3.7924 | PDE Loss:  -5.717 | Function Loss:  -3.8474\n",
      "Total loss:  -3.7925 | PDE Loss:  -5.7178 | Function Loss:  -3.8474\n",
      "Total loss:  -3.7925 | PDE Loss:  -5.7181 | Function Loss:  -3.8474\n",
      "Total loss:  -3.7926 | PDE Loss:  -5.7188 | Function Loss:  -3.8474\n",
      "Total loss:  -3.7926 | PDE Loss:  -5.7189 | Function Loss:  -3.8474\n",
      "Total loss:  -3.7927 | PDE Loss:  -5.7193 | Function Loss:  -3.8474\n",
      "Total loss:  -3.7927 | PDE Loss:  -5.7194 | Function Loss:  -3.8474\n",
      "Total loss:  -3.7928 | PDE Loss:  -5.7194 | Function Loss:  -3.8475\n",
      "Total loss:  -3.7928 | PDE Loss:  -5.7192 | Function Loss:  -3.8476\n",
      "Total loss:  -3.7928 | PDE Loss:  -5.7191 | Function Loss:  -3.8476\n",
      "Total loss:  -3.7929 | PDE Loss:  -5.7189 | Function Loss:  -3.8477\n",
      "Total loss:  -3.7929 | PDE Loss:  -5.7184 | Function Loss:  -3.8478\n",
      "Total loss:  -3.793 | PDE Loss:  -5.7183 | Function Loss:  -3.8479\n",
      "Total loss:  -3.7931 | PDE Loss:  -5.7176 | Function Loss:  -3.8481\n",
      "Total loss:  -3.7932 | PDE Loss:  -5.7171 | Function Loss:  -3.8483\n",
      "Total loss:  -3.7933 | PDE Loss:  -5.7171 | Function Loss:  -3.8484\n",
      "Total loss:  -3.7933 | PDE Loss:  -5.7151 | Function Loss:  -3.8486\n",
      "Total loss:  -3.7934 | PDE Loss:  -5.7167 | Function Loss:  -3.8486\n",
      "Total loss:  -3.7935 | PDE Loss:  -5.717 | Function Loss:  -3.8487\n",
      "Total loss:  -3.7936 | PDE Loss:  -5.7173 | Function Loss:  -3.8487\n",
      "Total loss:  -3.7937 | PDE Loss:  -5.718 | Function Loss:  -3.8487\n",
      "Total loss:  -3.7938 | PDE Loss:  -5.7189 | Function Loss:  -3.8487\n",
      "Total loss:  -3.7939 | PDE Loss:  -5.7197 | Function Loss:  -3.8487\n",
      "Total loss:  -3.794 | PDE Loss:  -5.7206 | Function Loss:  -3.8487\n",
      "Total loss:  -3.7941 | PDE Loss:  -5.7211 | Function Loss:  -3.8488\n",
      "Total loss:  -3.7942 | PDE Loss:  -5.7215 | Function Loss:  -3.8488\n",
      "Total loss:  -3.7942 | PDE Loss:  -5.7217 | Function Loss:  -3.8488\n",
      "Total loss:  -3.7944 | PDE Loss:  -5.7222 | Function Loss:  -3.8489\n",
      "Total loss:  -3.7945 | PDE Loss:  -5.7223 | Function Loss:  -3.8491\n",
      "Total loss:  -3.7946 | PDE Loss:  -5.723 | Function Loss:  -3.8491\n",
      "Total loss:  -3.7947 | PDE Loss:  -5.723 | Function Loss:  -3.8492\n",
      "Total loss:  -3.7948 | PDE Loss:  -5.7231 | Function Loss:  -3.8493\n",
      "Total loss:  -3.7949 | PDE Loss:  -5.7232 | Function Loss:  -3.8494\n",
      "Total loss:  -3.795 | PDE Loss:  -5.7233 | Function Loss:  -3.8495\n",
      "Total loss:  -3.7951 | PDE Loss:  -5.7238 | Function Loss:  -3.8496\n",
      "Total loss:  -3.7952 | PDE Loss:  -5.7241 | Function Loss:  -3.8496\n",
      "Total loss:  -3.7953 | PDE Loss:  -5.7248 | Function Loss:  -3.8496\n",
      "Total loss:  -3.7954 | PDE Loss:  -5.7259 | Function Loss:  -3.8496\n",
      "Total loss:  -3.7955 | PDE Loss:  -5.7271 | Function Loss:  -3.8496\n",
      "Total loss:  -3.7957 | PDE Loss:  -5.7281 | Function Loss:  -3.8496\n",
      "Total loss:  -3.7957 | PDE Loss:  -5.729 | Function Loss:  -3.8496\n",
      "Total loss:  -3.7958 | PDE Loss:  -5.7294 | Function Loss:  -3.8496\n",
      "Total loss:  -3.7959 | PDE Loss:  -5.7294 | Function Loss:  -3.8497\n",
      "Total loss:  -3.796 | PDE Loss:  -5.7293 | Function Loss:  -3.8498\n",
      "Total loss:  -3.796 | PDE Loss:  -5.7291 | Function Loss:  -3.8499\n",
      "Total loss:  -3.7961 | PDE Loss:  -5.729 | Function Loss:  -3.85\n",
      "Total loss:  -3.7962 | PDE Loss:  -5.7287 | Function Loss:  -3.8501\n",
      "Total loss:  -3.7962 | PDE Loss:  -5.7292 | Function Loss:  -3.8501\n",
      "Total loss:  -3.7963 | PDE Loss:  -5.7292 | Function Loss:  -3.8502\n",
      "Total loss:  -3.7964 | PDE Loss:  -5.7302 | Function Loss:  -3.8501\n",
      "Total loss:  -3.7964 | PDE Loss:  -5.7296 | Function Loss:  -3.8503\n",
      "Total loss:  -3.7965 | PDE Loss:  -5.731 | Function Loss:  -3.8502\n",
      "Total loss:  -3.7966 | PDE Loss:  -5.7315 | Function Loss:  -3.8502\n",
      "Total loss:  -3.7968 | PDE Loss:  -5.7337 | Function Loss:  -3.8501\n",
      "Total loss:  -3.7969 | PDE Loss:  -5.7337 | Function Loss:  -3.8503\n",
      "Total loss:  -3.797 | PDE Loss:  -5.7337 | Function Loss:  -3.8504\n",
      "Total loss:  -3.7972 | PDE Loss:  -5.7333 | Function Loss:  -3.8507\n",
      "Total loss:  -3.7973 | PDE Loss:  -5.733 | Function Loss:  -3.8509\n",
      "Total loss:  -3.7974 | PDE Loss:  -5.732 | Function Loss:  -3.8511\n",
      "Total loss:  -3.7975 | PDE Loss:  -5.7317 | Function Loss:  -3.8513\n",
      "Total loss:  -3.7976 | PDE Loss:  -5.7309 | Function Loss:  -3.8514\n",
      "Total loss:  -3.7977 | PDE Loss:  -5.7309 | Function Loss:  -3.8515\n",
      "Total loss:  -3.7978 | PDE Loss:  -5.7312 | Function Loss:  -3.8516\n",
      "Total loss:  -3.7979 | PDE Loss:  -5.7316 | Function Loss:  -3.8517\n",
      "Total loss:  -3.7981 | PDE Loss:  -5.7326 | Function Loss:  -3.8517\n",
      "Total loss:  -3.7981 | PDE Loss:  -5.7329 | Function Loss:  -3.8518\n",
      "Total loss:  -3.7982 | PDE Loss:  -5.7337 | Function Loss:  -3.8518\n",
      "Total loss:  -3.7983 | PDE Loss:  -5.7339 | Function Loss:  -3.8519\n",
      "Total loss:  -3.7985 | PDE Loss:  -5.7339 | Function Loss:  -3.8521\n",
      "Total loss:  -3.7987 | PDE Loss:  -5.7334 | Function Loss:  -3.8523\n",
      "Total loss:  -3.7988 | PDE Loss:  -5.7332 | Function Loss:  -3.8525\n",
      "Total loss:  -3.799 | PDE Loss:  -5.7319 | Function Loss:  -3.8529\n",
      "Total loss:  -3.7991 | PDE Loss:  -5.7299 | Function Loss:  -3.8533\n",
      "Total loss:  -3.7992 | PDE Loss:  -5.728 | Function Loss:  -3.8537\n",
      "Total loss:  -3.7993 | PDE Loss:  -5.7266 | Function Loss:  -3.8539\n",
      "Total loss:  -3.7993 | PDE Loss:  -5.7259 | Function Loss:  -3.8541\n",
      "Total loss:  -3.7994 | PDE Loss:  -5.7252 | Function Loss:  -3.8543\n",
      "Total loss:  -3.7995 | PDE Loss:  -5.7253 | Function Loss:  -3.8543\n",
      "Total loss:  -3.7995 | PDE Loss:  -5.7255 | Function Loss:  -3.8544\n",
      "Total loss:  -3.7996 | PDE Loss:  -5.7271 | Function Loss:  -3.8543\n",
      "Total loss:  -3.7998 | PDE Loss:  -5.7288 | Function Loss:  -3.8542\n",
      "Total loss:  -3.7999 | PDE Loss:  -5.7316 | Function Loss:  -3.8539\n",
      "Total loss:  -3.8 | PDE Loss:  -5.7331 | Function Loss:  -3.8539\n",
      "Total loss:  -3.8001 | PDE Loss:  -5.7356 | Function Loss:  -3.8537\n",
      "Total loss:  -3.8002 | PDE Loss:  -5.7365 | Function Loss:  -3.8536\n",
      "Total loss:  -3.8003 | PDE Loss:  -5.7381 | Function Loss:  -3.8536\n",
      "Total loss:  -3.8004 | PDE Loss:  -5.7391 | Function Loss:  -3.8536\n",
      "Total loss:  -3.8006 | PDE Loss:  -5.7398 | Function Loss:  -3.8537\n",
      "Total loss:  -3.8007 | PDE Loss:  -5.7368 | Function Loss:  -3.8542\n",
      "Total loss:  -3.8009 | PDE Loss:  -5.738 | Function Loss:  -3.8543\n",
      "Total loss:  -3.8011 | PDE Loss:  -5.7374 | Function Loss:  -3.8545\n",
      "Total loss:  -3.8013 | PDE Loss:  -5.7366 | Function Loss:  -3.8549\n",
      "Total loss:  -3.8015 | PDE Loss:  -5.7346 | Function Loss:  -3.8553\n",
      "Total loss:  -3.8016 | PDE Loss:  -5.7338 | Function Loss:  -3.8556\n",
      "Total loss:  -3.8017 | PDE Loss:  -5.7321 | Function Loss:  -3.8559\n",
      "Total loss:  -3.8018 | PDE Loss:  -5.7316 | Function Loss:  -3.8561\n",
      "Total loss:  -3.8019 | PDE Loss:  -5.7314 | Function Loss:  -3.8563\n",
      "Total loss:  -3.8021 | PDE Loss:  -5.7315 | Function Loss:  -3.8564\n",
      "Total loss:  -3.8022 | PDE Loss:  -5.7328 | Function Loss:  -3.8564\n",
      "Total loss:  -3.8024 | PDE Loss:  -5.7338 | Function Loss:  -3.8565\n",
      "Total loss:  -3.8025 | PDE Loss:  -5.7354 | Function Loss:  -3.8563\n",
      "Total loss:  -3.8025 | PDE Loss:  -5.7371 | Function Loss:  -3.8562\n",
      "Total loss:  -3.8026 | PDE Loss:  -5.7377 | Function Loss:  -3.8562\n",
      "Total loss:  -3.8027 | PDE Loss:  -5.7394 | Function Loss:  -3.8561\n",
      "Total loss:  -3.8028 | PDE Loss:  -5.7404 | Function Loss:  -3.8561\n",
      "Total loss:  -3.8029 | PDE Loss:  -5.7408 | Function Loss:  -3.8561\n",
      "Total loss:  -3.803 | PDE Loss:  -5.7414 | Function Loss:  -3.8561\n",
      "Total loss:  -3.8031 | PDE Loss:  -5.7408 | Function Loss:  -3.8563\n",
      "Total loss:  -3.8031 | PDE Loss:  -5.7409 | Function Loss:  -3.8564\n",
      "Total loss:  -3.8032 | PDE Loss:  -5.7407 | Function Loss:  -3.8565\n",
      "Total loss:  -3.8033 | PDE Loss:  -5.7402 | Function Loss:  -3.8567\n",
      "Total loss:  -3.8034 | PDE Loss:  -5.7399 | Function Loss:  -3.8568\n",
      "Total loss:  -3.8035 | PDE Loss:  -5.7401 | Function Loss:  -3.8569\n",
      "Total loss:  -3.8036 | PDE Loss:  -5.7399 | Function Loss:  -3.857\n",
      "Total loss:  -3.8036 | PDE Loss:  -5.7406 | Function Loss:  -3.857\n",
      "Total loss:  -3.8037 | PDE Loss:  -5.7415 | Function Loss:  -3.857\n",
      "Total loss:  -3.8038 | PDE Loss:  -5.7424 | Function Loss:  -3.857\n",
      "Total loss:  -3.8039 | PDE Loss:  -5.7433 | Function Loss:  -3.8569\n",
      "Total loss:  -3.804 | PDE Loss:  -5.7439 | Function Loss:  -3.8569\n",
      "Total loss:  -3.8041 | PDE Loss:  -5.7444 | Function Loss:  -3.857\n",
      "Total loss:  -3.8041 | PDE Loss:  -5.7443 | Function Loss:  -3.8571\n",
      "Total loss:  -3.8042 | PDE Loss:  -5.7441 | Function Loss:  -3.8572\n",
      "Total loss:  -3.8042 | PDE Loss:  -5.7435 | Function Loss:  -3.8573\n",
      "Total loss:  -3.8043 | PDE Loss:  -5.7428 | Function Loss:  -3.8575\n",
      "Total loss:  -3.8043 | PDE Loss:  -5.7417 | Function Loss:  -3.8576\n",
      "Total loss:  -3.8044 | PDE Loss:  -5.741 | Function Loss:  -3.8578\n",
      "Total loss:  -3.8044 | PDE Loss:  -5.7405 | Function Loss:  -3.8579\n",
      "Total loss:  -3.8045 | PDE Loss:  -5.7401 | Function Loss:  -3.858\n",
      "Total loss:  -3.8045 | PDE Loss:  -5.7402 | Function Loss:  -3.858\n",
      "Total loss:  -3.8045 | PDE Loss:  -5.74 | Function Loss:  -3.8581\n",
      "Total loss:  -3.8046 | PDE Loss:  -5.7403 | Function Loss:  -3.8581\n",
      "Total loss:  -3.8046 | PDE Loss:  -5.7404 | Function Loss:  -3.8581\n",
      "Total loss:  -3.8047 | PDE Loss:  -5.7405 | Function Loss:  -3.8582\n",
      "Total loss:  -3.8047 | PDE Loss:  -5.7407 | Function Loss:  -3.8582\n",
      "Total loss:  -3.8047 | PDE Loss:  -5.7404 | Function Loss:  -3.8583\n",
      "Total loss:  -3.8048 | PDE Loss:  -5.7401 | Function Loss:  -3.8584\n",
      "Total loss:  -3.8049 | PDE Loss:  -5.7397 | Function Loss:  -3.8585\n",
      "Total loss:  -3.805 | PDE Loss:  -5.7395 | Function Loss:  -3.8586\n",
      "Total loss:  -3.8051 | PDE Loss:  -5.7395 | Function Loss:  -3.8587\n",
      "Total loss:  -3.8052 | PDE Loss:  -5.7395 | Function Loss:  -3.8589\n",
      "Total loss:  -3.8052 | PDE Loss:  -5.7401 | Function Loss:  -3.8589\n",
      "Total loss:  -3.8053 | PDE Loss:  -5.7405 | Function Loss:  -3.8589\n",
      "Total loss:  -3.8053 | PDE Loss:  -5.7416 | Function Loss:  -3.8588\n",
      "Total loss:  -3.8054 | PDE Loss:  -5.7423 | Function Loss:  -3.8587\n",
      "Total loss:  -3.8054 | PDE Loss:  -5.7433 | Function Loss:  -3.8587\n",
      "Total loss:  -3.8055 | PDE Loss:  -5.7444 | Function Loss:  -3.8586\n",
      "Total loss:  -3.8055 | PDE Loss:  -5.7455 | Function Loss:  -3.8585\n",
      "Total loss:  -3.8056 | PDE Loss:  -5.7464 | Function Loss:  -3.8584\n",
      "Total loss:  -3.8056 | PDE Loss:  -5.7471 | Function Loss:  -3.8584\n",
      "Total loss:  -3.8057 | PDE Loss:  -5.7479 | Function Loss:  -3.8583\n",
      "Total loss:  -3.8058 | PDE Loss:  -5.7482 | Function Loss:  -3.8584\n",
      "Total loss:  -3.8058 | PDE Loss:  -5.7482 | Function Loss:  -3.8585\n",
      "Total loss:  -3.8059 | PDE Loss:  -5.7481 | Function Loss:  -3.8585\n",
      "Total loss:  -3.8059 | PDE Loss:  -5.7476 | Function Loss:  -3.8587\n",
      "Total loss:  -3.806 | PDE Loss:  -5.748 | Function Loss:  -3.8587\n",
      "Total loss:  -3.806 | PDE Loss:  -5.7478 | Function Loss:  -3.8587\n",
      "Total loss:  -3.8061 | PDE Loss:  -5.7474 | Function Loss:  -3.8589\n",
      "Total loss:  -3.8061 | PDE Loss:  -5.7475 | Function Loss:  -3.8589\n",
      "Total loss:  -3.8061 | PDE Loss:  -5.7475 | Function Loss:  -3.8589\n",
      "Total loss:  -3.8062 | PDE Loss:  -5.7481 | Function Loss:  -3.8589\n",
      "Total loss:  -3.8063 | PDE Loss:  -5.7492 | Function Loss:  -3.8589\n",
      "Total loss:  -3.8064 | PDE Loss:  -5.7513 | Function Loss:  -3.8587\n",
      "Total loss:  -3.8065 | PDE Loss:  -5.7527 | Function Loss:  -3.8587\n",
      "Total loss:  -3.8066 | PDE Loss:  -5.7546 | Function Loss:  -3.8585\n",
      "Total loss:  -3.8066 | PDE Loss:  -5.7556 | Function Loss:  -3.8584\n",
      "Total loss:  -3.8067 | PDE Loss:  -5.7563 | Function Loss:  -3.8584\n",
      "Total loss:  -3.8067 | PDE Loss:  -5.7567 | Function Loss:  -3.8584\n",
      "Total loss:  -3.8067 | PDE Loss:  -5.7567 | Function Loss:  -3.8584\n",
      "Total loss:  -3.8068 | PDE Loss:  -5.7565 | Function Loss:  -3.8585\n",
      "Total loss:  -3.8068 | PDE Loss:  -5.7563 | Function Loss:  -3.8586\n",
      "Total loss:  -3.8069 | PDE Loss:  -5.756 | Function Loss:  -3.8587\n",
      "Total loss:  -3.8069 | PDE Loss:  -5.7559 | Function Loss:  -3.8588\n",
      "Total loss:  -3.807 | PDE Loss:  -5.7552 | Function Loss:  -3.859\n",
      "Total loss:  -3.8071 | PDE Loss:  -5.7551 | Function Loss:  -3.859\n",
      "Total loss:  -3.8071 | PDE Loss:  -5.7552 | Function Loss:  -3.8591\n",
      "Total loss:  -3.8072 | PDE Loss:  -5.7552 | Function Loss:  -3.8591\n",
      "Total loss:  -3.8072 | PDE Loss:  -5.7554 | Function Loss:  -3.8591\n",
      "Total loss:  -3.8072 | PDE Loss:  -5.7558 | Function Loss:  -3.8591\n",
      "Total loss:  -3.8073 | PDE Loss:  -5.7558 | Function Loss:  -3.8592\n",
      "Total loss:  -3.8073 | PDE Loss:  -5.7561 | Function Loss:  -3.8592\n",
      "Total loss:  -3.8074 | PDE Loss:  -5.756 | Function Loss:  -3.8593\n",
      "Total loss:  -3.8075 | PDE Loss:  -5.7559 | Function Loss:  -3.8593\n",
      "Total loss:  -3.8075 | PDE Loss:  -5.7555 | Function Loss:  -3.8595\n",
      "Total loss:  -3.8076 | PDE Loss:  -5.7548 | Function Loss:  -3.8596\n",
      "Total loss:  -3.8076 | PDE Loss:  -5.7541 | Function Loss:  -3.8598\n",
      "Total loss:  -3.8077 | PDE Loss:  -5.7529 | Function Loss:  -3.86\n",
      "Total loss:  -3.8078 | PDE Loss:  -5.7522 | Function Loss:  -3.8602\n",
      "Total loss:  -3.8078 | PDE Loss:  -5.7506 | Function Loss:  -3.8604\n",
      "Total loss:  -3.8079 | PDE Loss:  -5.7492 | Function Loss:  -3.8607\n",
      "Total loss:  -3.808 | PDE Loss:  -5.7482 | Function Loss:  -3.8609\n",
      "Total loss:  -3.8081 | PDE Loss:  -5.7479 | Function Loss:  -3.8611\n",
      "Total loss:  -3.8081 | PDE Loss:  -5.747 | Function Loss:  -3.8612\n",
      "Total loss:  -3.8082 | PDE Loss:  -5.7472 | Function Loss:  -3.8613\n",
      "Total loss:  -3.8083 | PDE Loss:  -5.7473 | Function Loss:  -3.8613\n",
      "Total loss:  -3.8083 | PDE Loss:  -5.7479 | Function Loss:  -3.8613\n",
      "Total loss:  -3.8084 | PDE Loss:  -5.7487 | Function Loss:  -3.8613\n",
      "Total loss:  -3.8085 | PDE Loss:  -5.7493 | Function Loss:  -3.8613\n",
      "Total loss:  -3.8086 | PDE Loss:  -5.7502 | Function Loss:  -3.8614\n",
      "Total loss:  -3.8087 | PDE Loss:  -5.751 | Function Loss:  -3.8614\n",
      "Total loss:  -3.8089 | PDE Loss:  -5.7515 | Function Loss:  -3.8616\n",
      "Total loss:  -3.8091 | PDE Loss:  -5.7517 | Function Loss:  -3.8617\n",
      "Total loss:  -3.8092 | PDE Loss:  -5.7515 | Function Loss:  -3.8619\n",
      "Total loss:  -3.8094 | PDE Loss:  -5.7511 | Function Loss:  -3.8621\n",
      "Total loss:  -3.8094 | PDE Loss:  -5.7507 | Function Loss:  -3.8622\n",
      "Total loss:  -3.8095 | PDE Loss:  -5.7502 | Function Loss:  -3.8623\n",
      "Total loss:  -3.8095 | PDE Loss:  -5.7501 | Function Loss:  -3.8624\n",
      "Total loss:  -3.8096 | PDE Loss:  -5.75 | Function Loss:  -3.8625\n",
      "Total loss:  -3.8097 | PDE Loss:  -5.7499 | Function Loss:  -3.8626\n",
      "Total loss:  -3.8098 | PDE Loss:  -5.7504 | Function Loss:  -3.8627\n",
      "Total loss:  -3.8099 | PDE Loss:  -5.7505 | Function Loss:  -3.8628\n",
      "Total loss:  -3.81 | PDE Loss:  -5.752 | Function Loss:  -3.8627\n",
      "Total loss:  -3.81 | PDE Loss:  -5.7518 | Function Loss:  -3.8628\n",
      "Total loss:  -3.8101 | PDE Loss:  -5.7519 | Function Loss:  -3.8628\n",
      "Total loss:  -3.8101 | PDE Loss:  -5.752 | Function Loss:  -3.8628\n",
      "Total loss:  -3.8101 | PDE Loss:  -5.752 | Function Loss:  -3.8629\n",
      "Total loss:  -3.8102 | PDE Loss:  -5.7521 | Function Loss:  -3.8629\n",
      "Total loss:  -3.8102 | PDE Loss:  -5.7521 | Function Loss:  -3.8629\n",
      "Total loss:  -3.8103 | PDE Loss:  -5.7519 | Function Loss:  -3.863\n",
      "Total loss:  -3.8103 | PDE Loss:  -5.7518 | Function Loss:  -3.8631\n",
      "Total loss:  -3.8104 | PDE Loss:  -5.7513 | Function Loss:  -3.8633\n",
      "Total loss:  -3.8105 | PDE Loss:  -5.7511 | Function Loss:  -3.8634\n",
      "Total loss:  -3.8106 | PDE Loss:  -5.7504 | Function Loss:  -3.8636\n",
      "Total loss:  -3.8107 | PDE Loss:  -5.7498 | Function Loss:  -3.8638\n",
      "Total loss:  -3.8108 | PDE Loss:  -5.7488 | Function Loss:  -3.8641\n",
      "Total loss:  -3.8109 | PDE Loss:  -5.748 | Function Loss:  -3.8643\n",
      "Total loss:  -3.811 | PDE Loss:  -5.7477 | Function Loss:  -3.8645\n",
      "Total loss:  -3.8111 | PDE Loss:  -5.7474 | Function Loss:  -3.8646\n",
      "Total loss:  -3.8112 | PDE Loss:  -5.7469 | Function Loss:  -3.8648\n",
      "Total loss:  -3.8113 | PDE Loss:  -5.7477 | Function Loss:  -3.8647\n",
      "Total loss:  -3.8114 | PDE Loss:  -5.7486 | Function Loss:  -3.8647\n",
      "Total loss:  -3.8115 | PDE Loss:  -5.7499 | Function Loss:  -3.8647\n",
      "Total loss:  -3.8117 | PDE Loss:  -5.751 | Function Loss:  -3.8647\n",
      "Total loss:  -3.8118 | PDE Loss:  -5.7525 | Function Loss:  -3.8647\n",
      "Total loss:  -3.812 | PDE Loss:  -5.7538 | Function Loss:  -3.8648\n",
      "Total loss:  -3.8122 | PDE Loss:  -5.7553 | Function Loss:  -3.8648\n",
      "Total loss:  -3.8123 | PDE Loss:  -5.7558 | Function Loss:  -3.8649\n",
      "Total loss:  -3.8125 | PDE Loss:  -5.7563 | Function Loss:  -3.865\n",
      "Total loss:  -3.8126 | PDE Loss:  -5.7562 | Function Loss:  -3.8651\n",
      "Total loss:  -3.8127 | PDE Loss:  -5.7556 | Function Loss:  -3.8653\n",
      "Total loss:  -3.8128 | PDE Loss:  -5.7552 | Function Loss:  -3.8654\n",
      "Total loss:  -3.8128 | PDE Loss:  -5.7543 | Function Loss:  -3.8656\n",
      "Total loss:  -3.8129 | PDE Loss:  -5.7539 | Function Loss:  -3.8658\n",
      "Total loss:  -3.813 | PDE Loss:  -5.7529 | Function Loss:  -3.866\n",
      "Total loss:  -3.8132 | PDE Loss:  -5.7524 | Function Loss:  -3.8663\n",
      "Total loss:  -3.8133 | PDE Loss:  -5.7491 | Function Loss:  -3.8668\n",
      "Total loss:  -3.8134 | PDE Loss:  -5.7498 | Function Loss:  -3.8669\n",
      "Total loss:  -3.8135 | PDE Loss:  -5.7503 | Function Loss:  -3.8669\n",
      "Total loss:  -3.8136 | PDE Loss:  -5.7511 | Function Loss:  -3.8669\n",
      "Total loss:  -3.8137 | PDE Loss:  -5.7515 | Function Loss:  -3.867\n",
      "Total loss:  -3.8138 | PDE Loss:  -5.7523 | Function Loss:  -3.867\n",
      "Total loss:  -3.8138 | PDE Loss:  -5.7528 | Function Loss:  -3.8669\n",
      "Total loss:  -3.814 | PDE Loss:  -5.7537 | Function Loss:  -3.867\n",
      "Total loss:  -3.8141 | PDE Loss:  -5.7549 | Function Loss:  -3.8669\n",
      "Total loss:  -3.8142 | PDE Loss:  -5.7558 | Function Loss:  -3.8669\n",
      "Total loss:  -3.8143 | PDE Loss:  -5.7566 | Function Loss:  -3.8669\n",
      "Total loss:  -3.8144 | PDE Loss:  -5.7569 | Function Loss:  -3.867\n",
      "Total loss:  -3.8144 | PDE Loss:  -5.757 | Function Loss:  -3.8671\n",
      "Total loss:  -3.8145 | PDE Loss:  -5.7568 | Function Loss:  -3.8672\n",
      "Total loss:  -3.8146 | PDE Loss:  -5.7571 | Function Loss:  -3.8672\n",
      "Total loss:  -3.8147 | PDE Loss:  -5.7566 | Function Loss:  -3.8675\n",
      "Total loss:  -3.8148 | PDE Loss:  -5.756 | Function Loss:  -3.8676\n",
      "Total loss:  -3.8149 | PDE Loss:  -5.7558 | Function Loss:  -3.8678\n",
      "Total loss:  -3.815 | PDE Loss:  -5.756 | Function Loss:  -3.8679\n",
      "Total loss:  -3.8152 | PDE Loss:  -5.7558 | Function Loss:  -3.8681\n",
      "Total loss:  -3.8153 | PDE Loss:  -5.7558 | Function Loss:  -3.8682\n",
      "Total loss:  -3.8154 | PDE Loss:  -5.7561 | Function Loss:  -3.8683\n",
      "Total loss:  -3.8156 | PDE Loss:  -5.7565 | Function Loss:  -3.8684\n",
      "Total loss:  -3.8157 | PDE Loss:  -5.7566 | Function Loss:  -3.8685\n",
      "Total loss:  -3.8157 | PDE Loss:  -5.7561 | Function Loss:  -3.8687\n",
      "Total loss:  -3.8158 | PDE Loss:  -5.7556 | Function Loss:  -3.8688\n",
      "Total loss:  -3.8159 | PDE Loss:  -5.7541 | Function Loss:  -3.8691\n",
      "Total loss:  -3.816 | PDE Loss:  -5.7533 | Function Loss:  -3.8693\n",
      "Total loss:  -3.8161 | PDE Loss:  -5.7521 | Function Loss:  -3.8696\n",
      "Total loss:  -3.8161 | PDE Loss:  -5.7513 | Function Loss:  -3.8697\n",
      "Total loss:  -3.8162 | PDE Loss:  -5.7507 | Function Loss:  -3.8699\n",
      "Total loss:  -3.8163 | PDE Loss:  -5.7503 | Function Loss:  -3.87\n",
      "Total loss:  -3.8164 | PDE Loss:  -5.7511 | Function Loss:  -3.87\n",
      "Total loss:  -3.8164 | PDE Loss:  -5.7511 | Function Loss:  -3.8701\n",
      "Total loss:  -3.8165 | PDE Loss:  -5.7521 | Function Loss:  -3.87\n",
      "Total loss:  -3.8166 | PDE Loss:  -5.7538 | Function Loss:  -3.8699\n",
      "Total loss:  -3.8167 | PDE Loss:  -5.7555 | Function Loss:  -3.8698\n",
      "Total loss:  -3.8168 | PDE Loss:  -5.7572 | Function Loss:  -3.8697\n",
      "Total loss:  -3.8168 | PDE Loss:  -5.7583 | Function Loss:  -3.8696\n",
      "Total loss:  -3.8169 | PDE Loss:  -5.7592 | Function Loss:  -3.8696\n",
      "Total loss:  -3.817 | PDE Loss:  -5.76 | Function Loss:  -3.8696\n",
      "Total loss:  -3.8171 | PDE Loss:  -5.7599 | Function Loss:  -3.8697\n",
      "Total loss:  -3.8172 | PDE Loss:  -5.7594 | Function Loss:  -3.8699\n",
      "Total loss:  -3.8173 | PDE Loss:  -5.7588 | Function Loss:  -3.8701\n",
      "Total loss:  -3.8174 | PDE Loss:  -5.7569 | Function Loss:  -3.8704\n",
      "Total loss:  -3.8175 | PDE Loss:  -5.7562 | Function Loss:  -3.8706\n",
      "Total loss:  -3.8176 | PDE Loss:  -5.7555 | Function Loss:  -3.8708\n",
      "Total loss:  -3.8177 | PDE Loss:  -5.755 | Function Loss:  -3.871\n",
      "Total loss:  -3.8179 | PDE Loss:  -5.7547 | Function Loss:  -3.8713\n",
      "Total loss:  -3.818 | PDE Loss:  -5.7548 | Function Loss:  -3.8714\n",
      "Total loss:  -3.8182 | PDE Loss:  -5.7551 | Function Loss:  -3.8715\n",
      "Total loss:  -3.8184 | PDE Loss:  -5.7563 | Function Loss:  -3.8716\n",
      "Total loss:  -3.8186 | PDE Loss:  -5.7574 | Function Loss:  -3.8717\n",
      "Total loss:  -3.8187 | PDE Loss:  -5.7593 | Function Loss:  -3.8716\n",
      "Total loss:  -3.8188 | PDE Loss:  -5.7603 | Function Loss:  -3.8716\n",
      "Total loss:  -3.819 | PDE Loss:  -5.7613 | Function Loss:  -3.8716\n",
      "Total loss:  -3.8191 | PDE Loss:  -5.7623 | Function Loss:  -3.8717\n",
      "Total loss:  -3.8192 | PDE Loss:  -5.7632 | Function Loss:  -3.8717\n",
      "Total loss:  -3.8193 | PDE Loss:  -5.7641 | Function Loss:  -3.8717\n",
      "Total loss:  -3.8195 | PDE Loss:  -5.7656 | Function Loss:  -3.8717\n",
      "Total loss:  -3.8196 | PDE Loss:  -5.7666 | Function Loss:  -3.8717\n",
      "Total loss:  -3.8197 | PDE Loss:  -5.7678 | Function Loss:  -3.8716\n",
      "Total loss:  -3.8198 | PDE Loss:  -5.7692 | Function Loss:  -3.8716\n",
      "Total loss:  -3.8199 | PDE Loss:  -5.7697 | Function Loss:  -3.8716\n",
      "Total loss:  -3.82 | PDE Loss:  -5.771 | Function Loss:  -3.8716\n",
      "Total loss:  -3.8201 | PDE Loss:  -5.7714 | Function Loss:  -3.8716\n",
      "Total loss:  -3.8202 | PDE Loss:  -5.7722 | Function Loss:  -3.8716\n",
      "Total loss:  -3.8203 | PDE Loss:  -5.7728 | Function Loss:  -3.8716\n",
      "Total loss:  -3.8204 | PDE Loss:  -5.7726 | Function Loss:  -3.8718\n",
      "Total loss:  -3.8205 | PDE Loss:  -5.7732 | Function Loss:  -3.8718\n",
      "Total loss:  -3.8205 | PDE Loss:  -5.7731 | Function Loss:  -3.8719\n",
      "Total loss:  -3.8206 | PDE Loss:  -5.7729 | Function Loss:  -3.872\n",
      "Total loss:  -3.8207 | PDE Loss:  -5.7727 | Function Loss:  -3.8722\n",
      "Total loss:  -3.8208 | PDE Loss:  -5.7726 | Function Loss:  -3.8723\n",
      "Total loss:  -3.8209 | PDE Loss:  -5.7727 | Function Loss:  -3.8724\n",
      "Total loss:  -3.821 | PDE Loss:  -5.7729 | Function Loss:  -3.8724\n",
      "Total loss:  -3.8211 | PDE Loss:  -5.7733 | Function Loss:  -3.8725\n",
      "Total loss:  -3.8212 | PDE Loss:  -5.7737 | Function Loss:  -3.8725\n",
      "Total loss:  -3.8212 | PDE Loss:  -5.7744 | Function Loss:  -3.8725\n",
      "Total loss:  -3.8214 | PDE Loss:  -5.7751 | Function Loss:  -3.8726\n",
      "Total loss:  -3.8215 | PDE Loss:  -5.776 | Function Loss:  -3.8726\n",
      "Total loss:  -3.8216 | PDE Loss:  -5.7767 | Function Loss:  -3.8726\n",
      "Total loss:  -3.8217 | PDE Loss:  -5.7778 | Function Loss:  -3.8726\n",
      "Total loss:  -3.8218 | PDE Loss:  -5.7787 | Function Loss:  -3.8726\n",
      "Total loss:  -3.8219 | PDE Loss:  -5.7797 | Function Loss:  -3.8726\n",
      "Total loss:  -3.822 | PDE Loss:  -5.7805 | Function Loss:  -3.8726\n",
      "Total loss:  -3.8221 | PDE Loss:  -5.781 | Function Loss:  -3.8727\n",
      "Total loss:  -3.8222 | PDE Loss:  -5.7815 | Function Loss:  -3.8727\n",
      "Total loss:  -3.8223 | PDE Loss:  -5.782 | Function Loss:  -3.8727\n",
      "Total loss:  -3.8224 | PDE Loss:  -5.7822 | Function Loss:  -3.8728\n",
      "Total loss:  -3.8225 | PDE Loss:  -5.7822 | Function Loss:  -3.8729\n",
      "Total loss:  -3.8226 | PDE Loss:  -5.782 | Function Loss:  -3.8731\n",
      "Total loss:  -3.8227 | PDE Loss:  -5.7819 | Function Loss:  -3.8732\n",
      "Total loss:  -3.8228 | PDE Loss:  -5.7812 | Function Loss:  -3.8734\n",
      "Total loss:  -3.8229 | PDE Loss:  -5.7812 | Function Loss:  -3.8735\n",
      "Total loss:  -3.823 | PDE Loss:  -5.7808 | Function Loss:  -3.8737\n",
      "Total loss:  -3.8231 | PDE Loss:  -5.7809 | Function Loss:  -3.8738\n",
      "Total loss:  -3.8231 | PDE Loss:  -5.7805 | Function Loss:  -3.8739\n",
      "Total loss:  -3.8232 | PDE Loss:  -5.7803 | Function Loss:  -3.874\n",
      "Total loss:  -3.8233 | PDE Loss:  -5.7798 | Function Loss:  -3.8742\n",
      "Total loss:  -3.8234 | PDE Loss:  -5.7797 | Function Loss:  -3.8743\n",
      "Total loss:  -3.8235 | PDE Loss:  -5.7791 | Function Loss:  -3.8744\n",
      "Total loss:  -3.8236 | PDE Loss:  -5.779 | Function Loss:  -3.8746\n",
      "Total loss:  -3.8237 | PDE Loss:  -5.778 | Function Loss:  -3.8748\n",
      "Total loss:  -3.8237 | PDE Loss:  -5.7779 | Function Loss:  -3.8749\n",
      "Total loss:  -3.8238 | PDE Loss:  -5.7765 | Function Loss:  -3.8752\n",
      "Total loss:  -3.8239 | PDE Loss:  -5.7767 | Function Loss:  -3.8752\n",
      "Total loss:  -3.8239 | PDE Loss:  -5.7774 | Function Loss:  -3.8752\n",
      "Total loss:  -3.824 | PDE Loss:  -5.7776 | Function Loss:  -3.8752\n",
      "Total loss:  -3.8241 | PDE Loss:  -5.7784 | Function Loss:  -3.8752\n",
      "Total loss:  -3.8241 | PDE Loss:  -5.7786 | Function Loss:  -3.8753\n",
      "Total loss:  -3.8242 | PDE Loss:  -5.7798 | Function Loss:  -3.8752\n",
      "Total loss:  -3.8243 | PDE Loss:  -5.7799 | Function Loss:  -3.8753\n",
      "Total loss:  -3.8244 | PDE Loss:  -5.7796 | Function Loss:  -3.8754\n",
      "Total loss:  -3.8245 | PDE Loss:  -5.7791 | Function Loss:  -3.8756\n",
      "Total loss:  -3.8246 | PDE Loss:  -5.7785 | Function Loss:  -3.8758\n",
      "Total loss:  -3.8246 | PDE Loss:  -5.778 | Function Loss:  -3.8759\n",
      "Total loss:  -3.8247 | PDE Loss:  -5.7775 | Function Loss:  -3.876\n",
      "Total loss:  -3.8248 | PDE Loss:  -5.7772 | Function Loss:  -3.8762\n",
      "Total loss:  -3.8249 | PDE Loss:  -5.7769 | Function Loss:  -3.8763\n",
      "Total loss:  -3.825 | PDE Loss:  -5.7768 | Function Loss:  -3.8764\n",
      "Total loss:  -3.8251 | PDE Loss:  -5.7773 | Function Loss:  -3.8765\n",
      "Total loss:  -3.8252 | PDE Loss:  -5.7743 | Function Loss:  -3.877\n",
      "Total loss:  -3.8253 | PDE Loss:  -5.7767 | Function Loss:  -3.8768\n",
      "Total loss:  -3.8254 | PDE Loss:  -5.7784 | Function Loss:  -3.8767\n",
      "Total loss:  -3.8255 | PDE Loss:  -5.7803 | Function Loss:  -3.8766\n",
      "Total loss:  -3.8256 | PDE Loss:  -5.7815 | Function Loss:  -3.8766\n",
      "Total loss:  -3.8257 | PDE Loss:  -5.782 | Function Loss:  -3.8766\n",
      "Total loss:  -3.8258 | PDE Loss:  -5.7819 | Function Loss:  -3.8767\n",
      "Total loss:  -3.8259 | PDE Loss:  -5.7814 | Function Loss:  -3.8769\n",
      "Total loss:  -3.826 | PDE Loss:  -5.7806 | Function Loss:  -3.8771\n",
      "Total loss:  -3.8261 | PDE Loss:  -5.7798 | Function Loss:  -3.8773\n",
      "Total loss:  -3.8262 | PDE Loss:  -5.7788 | Function Loss:  -3.8776\n",
      "Total loss:  -3.8263 | PDE Loss:  -5.7784 | Function Loss:  -3.8777\n",
      "Total loss:  -3.8263 | PDE Loss:  -5.7784 | Function Loss:  -3.8777\n",
      "Total loss:  -3.8264 | PDE Loss:  -5.7785 | Function Loss:  -3.8778\n",
      "Total loss:  -3.8264 | PDE Loss:  -5.779 | Function Loss:  -3.8778\n",
      "Total loss:  -3.8265 | PDE Loss:  -5.7797 | Function Loss:  -3.8778\n",
      "Total loss:  -3.8265 | PDE Loss:  -5.7798 | Function Loss:  -3.8778\n",
      "Total loss:  -3.8266 | PDE Loss:  -5.781 | Function Loss:  -3.8778\n",
      "Total loss:  -3.8267 | PDE Loss:  -5.7814 | Function Loss:  -3.8778\n",
      "Total loss:  -3.8268 | PDE Loss:  -5.7815 | Function Loss:  -3.8779\n",
      "Total loss:  -3.827 | PDE Loss:  -5.7817 | Function Loss:  -3.8781\n",
      "Total loss:  -3.8271 | PDE Loss:  -5.7814 | Function Loss:  -3.8782\n",
      "Total loss:  -3.8271 | PDE Loss:  -5.7815 | Function Loss:  -3.8783\n",
      "Total loss:  -3.8272 | PDE Loss:  -5.781 | Function Loss:  -3.8784\n",
      "Total loss:  -3.8272 | PDE Loss:  -5.7814 | Function Loss:  -3.8784\n",
      "Total loss:  -3.8273 | PDE Loss:  -5.7816 | Function Loss:  -3.8784\n",
      "Total loss:  -3.8273 | PDE Loss:  -5.782 | Function Loss:  -3.8784\n",
      "Total loss:  -3.8273 | PDE Loss:  -5.7822 | Function Loss:  -3.8784\n",
      "Total loss:  -3.8274 | PDE Loss:  -5.7823 | Function Loss:  -3.8785\n",
      "Total loss:  -3.8274 | PDE Loss:  -5.7822 | Function Loss:  -3.8785\n",
      "Total loss:  -3.8275 | PDE Loss:  -5.7816 | Function Loss:  -3.8787\n",
      "Total loss:  -3.8275 | PDE Loss:  -5.7802 | Function Loss:  -3.8789\n",
      "Total loss:  -3.8276 | PDE Loss:  -5.7801 | Function Loss:  -3.879\n",
      "Total loss:  -3.8277 | PDE Loss:  -5.7797 | Function Loss:  -3.8791\n",
      "Total loss:  -3.8277 | PDE Loss:  -5.7793 | Function Loss:  -3.8792\n",
      "Total loss:  -3.8278 | PDE Loss:  -5.7787 | Function Loss:  -3.8793\n",
      "Total loss:  -3.8279 | PDE Loss:  -5.7785 | Function Loss:  -3.8795\n",
      "Total loss:  -3.8279 | PDE Loss:  -5.7783 | Function Loss:  -3.8796\n",
      "Total loss:  -3.828 | PDE Loss:  -5.7781 | Function Loss:  -3.8797\n",
      "Total loss:  -3.8281 | PDE Loss:  -5.7784 | Function Loss:  -3.8797\n",
      "Total loss:  -3.8281 | PDE Loss:  -5.7785 | Function Loss:  -3.8798\n",
      "Total loss:  -3.8282 | PDE Loss:  -5.779 | Function Loss:  -3.8798\n",
      "Total loss:  -3.8283 | PDE Loss:  -5.7794 | Function Loss:  -3.8799\n",
      "Total loss:  -3.8284 | PDE Loss:  -5.7799 | Function Loss:  -3.8799\n",
      "Total loss:  -3.8285 | PDE Loss:  -5.7796 | Function Loss:  -3.8801\n",
      "Total loss:  -3.8286 | PDE Loss:  -5.7802 | Function Loss:  -3.8801\n",
      "Total loss:  -3.8288 | PDE Loss:  -5.7806 | Function Loss:  -3.8803\n",
      "Total loss:  -3.8289 | PDE Loss:  -5.7804 | Function Loss:  -3.8804\n",
      "Total loss:  -3.829 | PDE Loss:  -5.78 | Function Loss:  -3.8806\n",
      "Total loss:  -3.8291 | PDE Loss:  -5.7794 | Function Loss:  -3.8808\n",
      "Total loss:  -3.8292 | PDE Loss:  -5.7786 | Function Loss:  -3.881\n",
      "Total loss:  -3.8293 | PDE Loss:  -5.7783 | Function Loss:  -3.8811\n",
      "Total loss:  -3.8293 | PDE Loss:  -5.7778 | Function Loss:  -3.8812\n",
      "Total loss:  -3.8294 | PDE Loss:  -5.7778 | Function Loss:  -3.8813\n",
      "Total loss:  -3.8294 | PDE Loss:  -5.7779 | Function Loss:  -3.8813\n",
      "Total loss:  -3.8295 | PDE Loss:  -5.778 | Function Loss:  -3.8814\n",
      "Total loss:  -3.8295 | PDE Loss:  -5.7785 | Function Loss:  -3.8814\n",
      "Total loss:  -3.8296 | PDE Loss:  -5.7793 | Function Loss:  -3.8813\n",
      "Total loss:  -3.8297 | PDE Loss:  -5.7804 | Function Loss:  -3.8813\n",
      "Total loss:  -3.8298 | PDE Loss:  -5.7816 | Function Loss:  -3.8812\n",
      "Total loss:  -3.8298 | PDE Loss:  -5.7823 | Function Loss:  -3.8812\n",
      "Total loss:  -3.8299 | PDE Loss:  -5.7826 | Function Loss:  -3.8812\n",
      "Total loss:  -3.83 | PDE Loss:  -5.7828 | Function Loss:  -3.8813\n",
      "Total loss:  -3.83 | PDE Loss:  -5.7824 | Function Loss:  -3.8814\n",
      "Total loss:  -3.8301 | PDE Loss:  -5.7822 | Function Loss:  -3.8815\n",
      "Total loss:  -3.8301 | PDE Loss:  -5.7816 | Function Loss:  -3.8816\n",
      "Total loss:  -3.8301 | PDE Loss:  -5.7809 | Function Loss:  -3.8817\n",
      "Total loss:  -3.8302 | PDE Loss:  -5.7803 | Function Loss:  -3.8818\n",
      "Total loss:  -3.8302 | PDE Loss:  -5.7801 | Function Loss:  -3.882\n",
      "Total loss:  -3.8303 | PDE Loss:  -5.7795 | Function Loss:  -3.8821\n",
      "Total loss:  -3.8303 | PDE Loss:  -5.7794 | Function Loss:  -3.8822\n",
      "Total loss:  -3.8304 | PDE Loss:  -5.7794 | Function Loss:  -3.8822\n",
      "Total loss:  -3.8304 | PDE Loss:  -5.7796 | Function Loss:  -3.8822\n",
      "Total loss:  -3.8304 | PDE Loss:  -5.7797 | Function Loss:  -3.8822\n",
      "Total loss:  -3.8305 | PDE Loss:  -5.7797 | Function Loss:  -3.8823\n",
      "Total loss:  -3.8306 | PDE Loss:  -5.78 | Function Loss:  -3.8823\n",
      "Total loss:  -3.8306 | PDE Loss:  -5.7798 | Function Loss:  -3.8824\n",
      "Total loss:  -3.8307 | PDE Loss:  -5.78 | Function Loss:  -3.8825\n",
      "Total loss:  -3.8308 | PDE Loss:  -5.7802 | Function Loss:  -3.8826\n",
      "Total loss:  -3.8309 | PDE Loss:  -5.7802 | Function Loss:  -3.8826\n",
      "Total loss:  -3.831 | PDE Loss:  -5.7806 | Function Loss:  -3.8827\n",
      "Total loss:  -3.831 | PDE Loss:  -5.781 | Function Loss:  -3.8827\n",
      "Total loss:  -3.8311 | PDE Loss:  -5.7815 | Function Loss:  -3.8828\n",
      "Total loss:  -3.8312 | PDE Loss:  -5.7826 | Function Loss:  -3.8827\n",
      "Total loss:  -3.8313 | PDE Loss:  -5.7839 | Function Loss:  -3.8827\n",
      "Total loss:  -3.8314 | PDE Loss:  -5.7859 | Function Loss:  -3.8825\n",
      "Total loss:  -3.8315 | PDE Loss:  -5.7875 | Function Loss:  -3.8824\n",
      "Total loss:  -3.8316 | PDE Loss:  -5.7886 | Function Loss:  -3.8824\n",
      "Total loss:  -3.8317 | PDE Loss:  -5.7898 | Function Loss:  -3.8824\n",
      "Total loss:  -3.8318 | PDE Loss:  -5.7907 | Function Loss:  -3.8824\n",
      "Total loss:  -3.832 | PDE Loss:  -5.7915 | Function Loss:  -3.8825\n",
      "Total loss:  -3.8321 | PDE Loss:  -5.7919 | Function Loss:  -3.8826\n",
      "Total loss:  -3.8323 | PDE Loss:  -5.7927 | Function Loss:  -3.8827\n",
      "Total loss:  -3.8324 | PDE Loss:  -5.7913 | Function Loss:  -3.883\n",
      "Total loss:  -3.8326 | PDE Loss:  -5.7934 | Function Loss:  -3.8829\n",
      "Total loss:  -3.8327 | PDE Loss:  -5.7933 | Function Loss:  -3.883\n",
      "Total loss:  -3.8328 | PDE Loss:  -5.7932 | Function Loss:  -3.8831\n",
      "Total loss:  -3.8328 | PDE Loss:  -5.7931 | Function Loss:  -3.8833\n",
      "Total loss:  -3.8329 | PDE Loss:  -5.7932 | Function Loss:  -3.8833\n",
      "Total loss:  -3.8329 | PDE Loss:  -5.7932 | Function Loss:  -3.8834\n",
      "Total loss:  -3.833 | PDE Loss:  -5.7935 | Function Loss:  -3.8834\n",
      "Total loss:  -3.8332 | PDE Loss:  -5.7934 | Function Loss:  -3.8836\n",
      "Total loss:  -3.8333 | PDE Loss:  -5.7936 | Function Loss:  -3.8837\n",
      "Total loss:  -3.8335 | PDE Loss:  -5.794 | Function Loss:  -3.8838\n",
      "Total loss:  -3.8337 | PDE Loss:  -5.7942 | Function Loss:  -3.8841\n",
      "Total loss:  -3.8339 | PDE Loss:  -5.7939 | Function Loss:  -3.8844\n",
      "Total loss:  -3.8341 | PDE Loss:  -5.7927 | Function Loss:  -3.8848\n",
      "Total loss:  -3.8343 | PDE Loss:  -5.7923 | Function Loss:  -3.8849\n",
      "Total loss:  -3.8344 | PDE Loss:  -5.7914 | Function Loss:  -3.8852\n",
      "Total loss:  -3.8345 | PDE Loss:  -5.7911 | Function Loss:  -3.8853\n",
      "Total loss:  -3.8346 | PDE Loss:  -5.791 | Function Loss:  -3.8854\n",
      "Total loss:  -3.8347 | PDE Loss:  -5.7916 | Function Loss:  -3.8855\n",
      "Total loss:  -3.8348 | PDE Loss:  -5.7925 | Function Loss:  -3.8855\n",
      "Total loss:  -3.8348 | PDE Loss:  -5.7942 | Function Loss:  -3.8854\n",
      "Total loss:  -3.835 | PDE Loss:  -5.7969 | Function Loss:  -3.8852\n",
      "Total loss:  -3.835 | PDE Loss:  -5.7984 | Function Loss:  -3.8851\n",
      "Total loss:  -3.8351 | PDE Loss:  -5.8005 | Function Loss:  -3.8849\n",
      "Total loss:  -3.8352 | PDE Loss:  -5.802 | Function Loss:  -3.8848\n",
      "Total loss:  -3.8353 | PDE Loss:  -5.8033 | Function Loss:  -3.8847\n",
      "Total loss:  -3.8353 | PDE Loss:  -5.8042 | Function Loss:  -3.8847\n",
      "Total loss:  -3.8354 | PDE Loss:  -5.8054 | Function Loss:  -3.8847\n",
      "Total loss:  -3.8355 | PDE Loss:  -5.8055 | Function Loss:  -3.8848\n",
      "Total loss:  -3.8357 | PDE Loss:  -5.8059 | Function Loss:  -3.8849\n",
      "Total loss:  -3.8358 | PDE Loss:  -5.8061 | Function Loss:  -3.885\n",
      "Total loss:  -3.8359 | PDE Loss:  -5.8055 | Function Loss:  -3.8851\n",
      "Total loss:  -3.836 | PDE Loss:  -5.8051 | Function Loss:  -3.8853\n",
      "Total loss:  -3.836 | PDE Loss:  -5.8047 | Function Loss:  -3.8854\n",
      "Total loss:  -3.8361 | PDE Loss:  -5.8044 | Function Loss:  -3.8856\n",
      "Total loss:  -3.8363 | PDE Loss:  -5.8042 | Function Loss:  -3.8857\n",
      "Total loss:  -3.8364 | PDE Loss:  -5.8043 | Function Loss:  -3.8859\n",
      "Total loss:  -3.8365 | PDE Loss:  -5.8049 | Function Loss:  -3.886\n",
      "Total loss:  -3.8367 | PDE Loss:  -5.806 | Function Loss:  -3.886\n",
      "Total loss:  -3.8368 | PDE Loss:  -5.8074 | Function Loss:  -3.886\n",
      "Total loss:  -3.837 | PDE Loss:  -5.8098 | Function Loss:  -3.8859\n",
      "Total loss:  -3.8371 | PDE Loss:  -5.8115 | Function Loss:  -3.8858\n",
      "Total loss:  -3.8372 | PDE Loss:  -5.813 | Function Loss:  -3.8858\n",
      "Total loss:  -3.8374 | PDE Loss:  -5.8154 | Function Loss:  -3.8857\n",
      "Total loss:  -3.8376 | PDE Loss:  -5.8176 | Function Loss:  -3.8856\n",
      "Total loss:  -3.8377 | PDE Loss:  -5.8196 | Function Loss:  -3.8855\n",
      "Total loss:  -3.8379 | PDE Loss:  -5.8209 | Function Loss:  -3.8855\n",
      "Total loss:  -3.8381 | PDE Loss:  -5.8222 | Function Loss:  -3.8856\n",
      "Total loss:  -3.8383 | PDE Loss:  -5.8252 | Function Loss:  -3.8855\n",
      "Total loss:  -3.8385 | PDE Loss:  -5.8249 | Function Loss:  -3.8858\n",
      "Total loss:  -3.8386 | PDE Loss:  -5.8244 | Function Loss:  -3.886\n",
      "Total loss:  -3.8387 | PDE Loss:  -5.8232 | Function Loss:  -3.8863\n",
      "Total loss:  -3.8388 | PDE Loss:  -5.8223 | Function Loss:  -3.8865\n",
      "Total loss:  -3.839 | PDE Loss:  -5.8211 | Function Loss:  -3.8868\n",
      "Total loss:  -3.8391 | PDE Loss:  -5.8199 | Function Loss:  -3.8871\n",
      "Total loss:  -3.8393 | PDE Loss:  -5.819 | Function Loss:  -3.8874\n",
      "Total loss:  -3.8396 | PDE Loss:  -5.8177 | Function Loss:  -3.8878\n",
      "Total loss:  -3.8398 | PDE Loss:  -5.8184 | Function Loss:  -3.888\n",
      "Total loss:  -3.84 | PDE Loss:  -5.8177 | Function Loss:  -3.8883\n",
      "Total loss:  -3.8402 | PDE Loss:  -5.8177 | Function Loss:  -3.8885\n",
      "Total loss:  -3.8404 | PDE Loss:  -5.8179 | Function Loss:  -3.8887\n",
      "Total loss:  -3.8405 | PDE Loss:  -5.8175 | Function Loss:  -3.8889\n",
      "Total loss:  -3.8407 | PDE Loss:  -5.8177 | Function Loss:  -3.8891\n",
      "Total loss:  -3.8409 | PDE Loss:  -5.8167 | Function Loss:  -3.8894\n",
      "Total loss:  -3.8411 | PDE Loss:  -5.8156 | Function Loss:  -3.8898\n",
      "Total loss:  -3.8414 | PDE Loss:  -5.8136 | Function Loss:  -3.8904\n",
      "Total loss:  -3.8417 | PDE Loss:  -5.8111 | Function Loss:  -3.8909\n",
      "Total loss:  -3.8418 | PDE Loss:  -5.8094 | Function Loss:  -3.8913\n",
      "Total loss:  -3.842 | PDE Loss:  -5.808 | Function Loss:  -3.8917\n",
      "Total loss:  -3.8421 | PDE Loss:  -5.8078 | Function Loss:  -3.8919\n",
      "Total loss:  -3.8422 | PDE Loss:  -5.8071 | Function Loss:  -3.8921\n",
      "Total loss:  -3.8423 | PDE Loss:  -5.8075 | Function Loss:  -3.8921\n",
      "Total loss:  -3.8424 | PDE Loss:  -5.8078 | Function Loss:  -3.8921\n",
      "Total loss:  -3.8425 | PDE Loss:  -5.8084 | Function Loss:  -3.8922\n",
      "Total loss:  -3.8425 | PDE Loss:  -5.8093 | Function Loss:  -3.8922\n",
      "Total loss:  -3.8424 | PDE Loss:  -5.8048 | Function Loss:  -3.8925\n",
      "Total loss:  -3.8427 | PDE Loss:  -5.8082 | Function Loss:  -3.8924\n",
      "Total loss:  -3.8428 | PDE Loss:  -5.8088 | Function Loss:  -3.8925\n",
      "Total loss:  -3.8429 | PDE Loss:  -5.809 | Function Loss:  -3.8926\n",
      "Total loss:  -3.843 | PDE Loss:  -5.8088 | Function Loss:  -3.8928\n",
      "Total loss:  -3.8432 | PDE Loss:  -5.8084 | Function Loss:  -3.893\n",
      "Total loss:  -3.8433 | PDE Loss:  -5.8073 | Function Loss:  -3.8933\n",
      "Total loss:  -3.8435 | PDE Loss:  -5.8066 | Function Loss:  -3.8935\n",
      "Total loss:  -3.8436 | PDE Loss:  -5.8051 | Function Loss:  -3.8938\n",
      "Total loss:  -3.8437 | PDE Loss:  -5.8044 | Function Loss:  -3.8941\n",
      "Total loss:  -3.8439 | PDE Loss:  -5.8026 | Function Loss:  -3.8945\n",
      "Total loss:  -3.844 | PDE Loss:  -5.7994 | Function Loss:  -3.895\n",
      "Total loss:  -3.8441 | PDE Loss:  -5.8006 | Function Loss:  -3.8949\n",
      "Total loss:  -3.8442 | PDE Loss:  -5.8009 | Function Loss:  -3.895\n",
      "Total loss:  -3.8443 | PDE Loss:  -5.8013 | Function Loss:  -3.8951\n",
      "Total loss:  -3.8443 | PDE Loss:  -5.8012 | Function Loss:  -3.8951\n",
      "Total loss:  -3.8444 | PDE Loss:  -5.8007 | Function Loss:  -3.8953\n",
      "Total loss:  -3.8444 | PDE Loss:  -5.8003 | Function Loss:  -3.8954\n",
      "Total loss:  -3.8445 | PDE Loss:  -5.7996 | Function Loss:  -3.8955\n",
      "Total loss:  -3.8445 | PDE Loss:  -5.8004 | Function Loss:  -3.8955\n",
      "Total loss:  -3.8446 | PDE Loss:  -5.7994 | Function Loss:  -3.8957\n",
      "Total loss:  -3.8447 | PDE Loss:  -5.799 | Function Loss:  -3.8958\n",
      "Total loss:  -3.8447 | PDE Loss:  -5.799 | Function Loss:  -3.8959\n",
      "Total loss:  -3.8448 | PDE Loss:  -5.7989 | Function Loss:  -3.896\n",
      "Total loss:  -3.8449 | PDE Loss:  -5.7994 | Function Loss:  -3.896\n",
      "Total loss:  -3.845 | PDE Loss:  -5.7995 | Function Loss:  -3.8961\n",
      "Total loss:  -3.8451 | PDE Loss:  -5.8 | Function Loss:  -3.8962\n",
      "Total loss:  -3.8453 | PDE Loss:  -5.8004 | Function Loss:  -3.8964\n",
      "Total loss:  -3.8455 | PDE Loss:  -5.8012 | Function Loss:  -3.8965\n",
      "Total loss:  -3.8456 | PDE Loss:  -5.8019 | Function Loss:  -3.8965\n",
      "Total loss:  -3.8458 | PDE Loss:  -5.8009 | Function Loss:  -3.8968\n",
      "Total loss:  -3.8459 | PDE Loss:  -5.8008 | Function Loss:  -3.897\n",
      "Total loss:  -3.8461 | PDE Loss:  -5.7995 | Function Loss:  -3.8973\n",
      "Total loss:  -3.8462 | PDE Loss:  -5.7991 | Function Loss:  -3.8975\n",
      "Total loss:  -3.8463 | PDE Loss:  -5.798 | Function Loss:  -3.8978\n",
      "Total loss:  -3.8465 | PDE Loss:  -5.7978 | Function Loss:  -3.898\n",
      "Total loss:  -3.8466 | PDE Loss:  -5.7969 | Function Loss:  -3.8983\n",
      "Total loss:  -3.8467 | PDE Loss:  -5.7959 | Function Loss:  -3.8985\n",
      "Total loss:  -3.8469 | PDE Loss:  -5.7955 | Function Loss:  -3.8987\n",
      "Total loss:  -3.8469 | PDE Loss:  -5.795 | Function Loss:  -3.8988\n",
      "Total loss:  -3.847 | PDE Loss:  -5.7949 | Function Loss:  -3.8989\n",
      "Total loss:  -3.847 | PDE Loss:  -5.7946 | Function Loss:  -3.899\n",
      "Total loss:  -3.8471 | PDE Loss:  -5.7943 | Function Loss:  -3.8992\n",
      "Total loss:  -3.8472 | PDE Loss:  -5.7934 | Function Loss:  -3.8994\n",
      "Total loss:  -3.8473 | PDE Loss:  -5.7929 | Function Loss:  -3.8996\n",
      "Total loss:  -3.8473 | PDE Loss:  -5.7924 | Function Loss:  -3.8997\n",
      "Total loss:  -3.8474 | PDE Loss:  -5.7915 | Function Loss:  -3.8999\n",
      "Total loss:  -3.8476 | PDE Loss:  -5.7909 | Function Loss:  -3.9001\n",
      "Total loss:  -3.8476 | PDE Loss:  -5.7893 | Function Loss:  -3.9004\n",
      "Total loss:  -3.8477 | PDE Loss:  -5.788 | Function Loss:  -3.9007\n",
      "Total loss:  -3.8479 | PDE Loss:  -5.7863 | Function Loss:  -3.901\n",
      "Total loss:  -3.848 | PDE Loss:  -5.7844 | Function Loss:  -3.9014\n",
      "Total loss:  -3.8481 | PDE Loss:  -5.783 | Function Loss:  -3.9017\n",
      "Total loss:  -3.8482 | PDE Loss:  -5.7821 | Function Loss:  -3.9019\n",
      "Total loss:  -3.8484 | PDE Loss:  -5.7835 | Function Loss:  -3.902\n",
      "Total loss:  -3.8486 | PDE Loss:  -5.7844 | Function Loss:  -3.9021\n",
      "Total loss:  -3.8487 | PDE Loss:  -5.7864 | Function Loss:  -3.902\n",
      "Total loss:  -3.8489 | PDE Loss:  -5.7883 | Function Loss:  -3.9019\n",
      "Total loss:  -3.8491 | PDE Loss:  -5.7907 | Function Loss:  -3.9019\n",
      "Total loss:  -3.8494 | PDE Loss:  -5.7934 | Function Loss:  -3.9018\n",
      "Total loss:  -3.8496 | PDE Loss:  -5.7961 | Function Loss:  -3.9017\n",
      "Total loss:  -3.8497 | PDE Loss:  -5.7968 | Function Loss:  -3.9018\n",
      "Total loss:  -3.8499 | PDE Loss:  -5.7987 | Function Loss:  -3.9017\n",
      "Total loss:  -3.85 | PDE Loss:  -5.7988 | Function Loss:  -3.9019\n",
      "Total loss:  -3.8502 | PDE Loss:  -5.7987 | Function Loss:  -3.902\n",
      "Total loss:  -3.8504 | PDE Loss:  -5.7983 | Function Loss:  -3.9023\n",
      "Total loss:  -3.8505 | PDE Loss:  -5.7983 | Function Loss:  -3.9025\n",
      "Total loss:  -3.8507 | PDE Loss:  -5.7984 | Function Loss:  -3.9027\n",
      "Total loss:  -3.8509 | PDE Loss:  -5.7993 | Function Loss:  -3.9028\n",
      "Total loss:  -3.8511 | PDE Loss:  -5.8 | Function Loss:  -3.9029\n",
      "Total loss:  -3.8513 | PDE Loss:  -5.8012 | Function Loss:  -3.903\n",
      "Total loss:  -3.8515 | PDE Loss:  -5.8018 | Function Loss:  -3.9031\n",
      "Total loss:  -3.8516 | PDE Loss:  -5.8026 | Function Loss:  -3.9031\n",
      "Total loss:  -3.8518 | PDE Loss:  -5.803 | Function Loss:  -3.9033\n",
      "Total loss:  -3.852 | PDE Loss:  -5.8032 | Function Loss:  -3.9036\n",
      "Total loss:  -3.8523 | PDE Loss:  -5.8029 | Function Loss:  -3.9039\n",
      "Total loss:  -3.8525 | PDE Loss:  -5.8036 | Function Loss:  -3.9041\n",
      "Total loss:  -3.8528 | PDE Loss:  -5.8025 | Function Loss:  -3.9045\n",
      "Total loss:  -3.8529 | PDE Loss:  -5.8031 | Function Loss:  -3.9045\n",
      "Total loss:  -3.853 | PDE Loss:  -5.8034 | Function Loss:  -3.9046\n",
      "Total loss:  -3.8531 | PDE Loss:  -5.8041 | Function Loss:  -3.9046\n",
      "Total loss:  -3.8532 | PDE Loss:  -5.8054 | Function Loss:  -3.9046\n",
      "Total loss:  -3.8533 | PDE Loss:  -5.8071 | Function Loss:  -3.9045\n",
      "Total loss:  -3.8534 | PDE Loss:  -5.8085 | Function Loss:  -3.9044\n",
      "Total loss:  -3.8535 | PDE Loss:  -5.8101 | Function Loss:  -3.9043\n",
      "Total loss:  -3.8535 | PDE Loss:  -5.8112 | Function Loss:  -3.9043\n",
      "Total loss:  -3.8537 | PDE Loss:  -5.8124 | Function Loss:  -3.9043\n",
      "Total loss:  -3.8537 | PDE Loss:  -5.8133 | Function Loss:  -3.9042\n",
      "Total loss:  -3.8539 | PDE Loss:  -5.8145 | Function Loss:  -3.9042\n",
      "Total loss:  -3.854 | PDE Loss:  -5.8149 | Function Loss:  -3.9043\n",
      "Total loss:  -3.8541 | PDE Loss:  -5.8151 | Function Loss:  -3.9044\n",
      "Total loss:  -3.8542 | PDE Loss:  -5.8137 | Function Loss:  -3.9047\n",
      "Total loss:  -3.8543 | PDE Loss:  -5.8142 | Function Loss:  -3.9048\n",
      "Total loss:  -3.8544 | PDE Loss:  -5.8131 | Function Loss:  -3.905\n",
      "Total loss:  -3.8544 | PDE Loss:  -5.8125 | Function Loss:  -3.9051\n",
      "Total loss:  -3.8544 | PDE Loss:  -5.8118 | Function Loss:  -3.9052\n",
      "Total loss:  -3.8546 | PDE Loss:  -5.8114 | Function Loss:  -3.9054\n",
      "Total loss:  -3.8546 | PDE Loss:  -5.8104 | Function Loss:  -3.9056\n",
      "Total loss:  -3.8547 | PDE Loss:  -5.8094 | Function Loss:  -3.9058\n",
      "Total loss:  -3.8548 | PDE Loss:  -5.8096 | Function Loss:  -3.9058\n",
      "Total loss:  -3.8549 | PDE Loss:  -5.8101 | Function Loss:  -3.9059\n",
      "Total loss:  -3.855 | PDE Loss:  -5.81 | Function Loss:  -3.906\n",
      "Total loss:  -3.8551 | PDE Loss:  -5.8097 | Function Loss:  -3.9062\n",
      "Total loss:  -3.8552 | PDE Loss:  -5.8088 | Function Loss:  -3.9065\n",
      "Total loss:  -3.8554 | PDE Loss:  -5.8076 | Function Loss:  -3.9068\n",
      "Total loss:  -3.8556 | PDE Loss:  -5.8061 | Function Loss:  -3.9072\n",
      "Total loss:  -3.8557 | PDE Loss:  -5.804 | Function Loss:  -3.9076\n",
      "Total loss:  -3.8558 | PDE Loss:  -5.802 | Function Loss:  -3.908\n",
      "Total loss:  -3.856 | PDE Loss:  -5.8008 | Function Loss:  -3.9083\n",
      "Total loss:  -3.8562 | PDE Loss:  -5.797 | Function Loss:  -3.909\n",
      "Total loss:  -3.8563 | PDE Loss:  -5.7956 | Function Loss:  -3.9094\n",
      "Total loss:  -3.8564 | PDE Loss:  -5.7956 | Function Loss:  -3.9095\n",
      "Total loss:  -3.8566 | PDE Loss:  -5.7957 | Function Loss:  -3.9097\n",
      "Total loss:  -3.8568 | PDE Loss:  -5.7966 | Function Loss:  -3.9098\n",
      "Total loss:  -3.8569 | PDE Loss:  -5.7973 | Function Loss:  -3.9098\n",
      "Total loss:  -3.857 | PDE Loss:  -5.7981 | Function Loss:  -3.9098\n",
      "Total loss:  -3.8572 | PDE Loss:  -5.799 | Function Loss:  -3.9099\n",
      "Total loss:  -3.8573 | PDE Loss:  -5.7994 | Function Loss:  -3.91\n",
      "Total loss:  -3.8574 | PDE Loss:  -5.8 | Function Loss:  -3.9101\n",
      "Total loss:  -3.8575 | PDE Loss:  -5.7989 | Function Loss:  -3.9103\n",
      "Total loss:  -3.8576 | PDE Loss:  -5.7999 | Function Loss:  -3.9103\n",
      "Total loss:  -3.8577 | PDE Loss:  -5.7998 | Function Loss:  -3.9104\n",
      "Total loss:  -3.8579 | PDE Loss:  -5.7998 | Function Loss:  -3.9106\n",
      "Total loss:  -3.858 | PDE Loss:  -5.7993 | Function Loss:  -3.9108\n",
      "Total loss:  -3.8581 | PDE Loss:  -5.7991 | Function Loss:  -3.9109\n",
      "Total loss:  -3.8582 | PDE Loss:  -5.7983 | Function Loss:  -3.9111\n",
      "Total loss:  -3.8583 | PDE Loss:  -5.797 | Function Loss:  -3.9114\n",
      "Total loss:  -3.8584 | PDE Loss:  -5.7966 | Function Loss:  -3.9116\n",
      "Total loss:  -3.8585 | PDE Loss:  -5.796 | Function Loss:  -3.9118\n",
      "Total loss:  -3.8586 | PDE Loss:  -5.7954 | Function Loss:  -3.912\n",
      "Total loss:  -3.8587 | PDE Loss:  -5.7944 | Function Loss:  -3.9122\n",
      "Total loss:  -3.8588 | PDE Loss:  -5.7937 | Function Loss:  -3.9125\n",
      "Total loss:  -3.8589 | PDE Loss:  -5.7925 | Function Loss:  -3.9127\n",
      "Total loss:  -3.8591 | PDE Loss:  -5.7916 | Function Loss:  -3.913\n",
      "Total loss:  -3.8591 | PDE Loss:  -5.7906 | Function Loss:  -3.9132\n",
      "Total loss:  -3.8593 | PDE Loss:  -5.7895 | Function Loss:  -3.9135\n",
      "Total loss:  -3.8594 | PDE Loss:  -5.7884 | Function Loss:  -3.9138\n",
      "Total loss:  -3.8595 | PDE Loss:  -5.7861 | Function Loss:  -3.9142\n",
      "Total loss:  -3.8595 | PDE Loss:  -5.786 | Function Loss:  -3.9143\n",
      "Total loss:  -3.8596 | PDE Loss:  -5.786 | Function Loss:  -3.9144\n",
      "Total loss:  -3.8597 | PDE Loss:  -5.7861 | Function Loss:  -3.9145\n",
      "Total loss:  -3.8598 | PDE Loss:  -5.7863 | Function Loss:  -3.9145\n",
      "Total loss:  -3.8598 | PDE Loss:  -5.7873 | Function Loss:  -3.9145\n",
      "Total loss:  -3.8599 | PDE Loss:  -5.7874 | Function Loss:  -3.9145\n",
      "Total loss:  -3.86 | PDE Loss:  -5.7875 | Function Loss:  -3.9146\n",
      "Total loss:  -3.8601 | PDE Loss:  -5.7876 | Function Loss:  -3.9147\n",
      "Total loss:  -3.8601 | PDE Loss:  -5.7877 | Function Loss:  -3.9147\n",
      "Total loss:  -3.8602 | PDE Loss:  -5.7876 | Function Loss:  -3.9148\n",
      "Total loss:  -3.8602 | PDE Loss:  -5.7874 | Function Loss:  -3.9149\n",
      "Total loss:  -3.8603 | PDE Loss:  -5.787 | Function Loss:  -3.9151\n",
      "Total loss:  -3.8604 | PDE Loss:  -5.7866 | Function Loss:  -3.9152\n",
      "Total loss:  -3.8605 | PDE Loss:  -5.7862 | Function Loss:  -3.9153\n",
      "Total loss:  -3.8605 | PDE Loss:  -5.786 | Function Loss:  -3.9154\n",
      "Total loss:  -3.8606 | PDE Loss:  -5.7856 | Function Loss:  -3.9155\n",
      "Total loss:  -3.8606 | PDE Loss:  -5.7855 | Function Loss:  -3.9156\n",
      "Total loss:  -3.8607 | PDE Loss:  -5.7852 | Function Loss:  -3.9157\n",
      "Total loss:  -3.8608 | PDE Loss:  -5.7855 | Function Loss:  -3.9158\n",
      "Total loss:  -3.8608 | PDE Loss:  -5.7852 | Function Loss:  -3.9159\n",
      "Total loss:  -3.8609 | PDE Loss:  -5.7848 | Function Loss:  -3.916\n",
      "Total loss:  -3.861 | PDE Loss:  -5.7841 | Function Loss:  -3.9162\n",
      "Total loss:  -3.8611 | PDE Loss:  -5.7836 | Function Loss:  -3.9163\n",
      "Total loss:  -3.8611 | PDE Loss:  -5.7831 | Function Loss:  -3.9165\n",
      "Total loss:  -3.8612 | PDE Loss:  -5.7826 | Function Loss:  -3.9167\n",
      "Total loss:  -3.8613 | PDE Loss:  -5.7818 | Function Loss:  -3.9168\n",
      "Total loss:  -3.8613 | PDE Loss:  -5.7817 | Function Loss:  -3.9169\n",
      "Total loss:  -3.8613 | PDE Loss:  -5.7816 | Function Loss:  -3.9169\n",
      "Total loss:  -3.8614 | PDE Loss:  -5.7817 | Function Loss:  -3.917\n",
      "Total loss:  -3.8614 | PDE Loss:  -5.782 | Function Loss:  -3.917\n",
      "Total loss:  -3.8615 | PDE Loss:  -5.7825 | Function Loss:  -3.917\n",
      "Total loss:  -3.8615 | PDE Loss:  -5.783 | Function Loss:  -3.917\n",
      "Total loss:  -3.8616 | PDE Loss:  -5.7835 | Function Loss:  -3.917\n",
      "Total loss:  -3.8616 | PDE Loss:  -5.7841 | Function Loss:  -3.9169\n",
      "Total loss:  -3.8617 | PDE Loss:  -5.7847 | Function Loss:  -3.9169\n",
      "Total loss:  -3.8617 | PDE Loss:  -5.7849 | Function Loss:  -3.9169\n",
      "Total loss:  -3.8618 | PDE Loss:  -5.785 | Function Loss:  -3.917\n",
      "Total loss:  -3.8619 | PDE Loss:  -5.7842 | Function Loss:  -3.9172\n",
      "Total loss:  -3.8619 | PDE Loss:  -5.7841 | Function Loss:  -3.9173\n",
      "Total loss:  -3.862 | PDE Loss:  -5.7835 | Function Loss:  -3.9174\n",
      "Total loss:  -3.8621 | PDE Loss:  -5.7832 | Function Loss:  -3.9176\n",
      "Total loss:  -3.8622 | PDE Loss:  -5.7827 | Function Loss:  -3.9177\n",
      "Total loss:  -3.8622 | PDE Loss:  -5.7823 | Function Loss:  -3.9179\n",
      "Total loss:  -3.8624 | PDE Loss:  -5.7825 | Function Loss:  -3.918\n",
      "Total loss:  -3.8624 | PDE Loss:  -5.7828 | Function Loss:  -3.918\n",
      "Total loss:  -3.8626 | PDE Loss:  -5.7828 | Function Loss:  -3.9181\n",
      "Total loss:  -3.8626 | PDE Loss:  -5.7835 | Function Loss:  -3.9181\n",
      "Total loss:  -3.8627 | PDE Loss:  -5.7839 | Function Loss:  -3.9182\n",
      "Total loss:  -3.8628 | PDE Loss:  -5.7843 | Function Loss:  -3.9183\n",
      "Total loss:  -3.8629 | PDE Loss:  -5.7845 | Function Loss:  -3.9183\n",
      "Total loss:  -3.863 | PDE Loss:  -5.7836 | Function Loss:  -3.9186\n",
      "Total loss:  -3.8631 | PDE Loss:  -5.7839 | Function Loss:  -3.9186\n",
      "Total loss:  -3.8632 | PDE Loss:  -5.7837 | Function Loss:  -3.9188\n",
      "Total loss:  -3.8633 | PDE Loss:  -5.7836 | Function Loss:  -3.9189\n",
      "Total loss:  -3.8634 | PDE Loss:  -5.7835 | Function Loss:  -3.919\n",
      "Total loss:  -3.8635 | PDE Loss:  -5.7835 | Function Loss:  -3.9191\n",
      "Total loss:  -3.8636 | PDE Loss:  -5.7834 | Function Loss:  -3.9192\n",
      "Total loss:  -3.8637 | PDE Loss:  -5.7836 | Function Loss:  -3.9193\n",
      "Total loss:  -3.8638 | PDE Loss:  -5.7842 | Function Loss:  -3.9193\n",
      "Total loss:  -3.8638 | PDE Loss:  -5.7846 | Function Loss:  -3.9194\n",
      "Total loss:  -3.8639 | PDE Loss:  -5.7848 | Function Loss:  -3.9194\n",
      "Total loss:  -3.864 | PDE Loss:  -5.7853 | Function Loss:  -3.9194\n",
      "Total loss:  -3.8641 | PDE Loss:  -5.7857 | Function Loss:  -3.9195\n",
      "Total loss:  -3.8641 | PDE Loss:  -5.7859 | Function Loss:  -3.9195\n",
      "Total loss:  -3.8642 | PDE Loss:  -5.7859 | Function Loss:  -3.9196\n",
      "Total loss:  -3.8643 | PDE Loss:  -5.7858 | Function Loss:  -3.9197\n",
      "Total loss:  -3.8644 | PDE Loss:  -5.7852 | Function Loss:  -3.9199\n",
      "Total loss:  -3.8644 | PDE Loss:  -5.7851 | Function Loss:  -3.92\n",
      "Total loss:  -3.8645 | PDE Loss:  -5.7851 | Function Loss:  -3.92\n",
      "Total loss:  -3.8645 | PDE Loss:  -5.7851 | Function Loss:  -3.9201\n",
      "Total loss:  -3.8646 | PDE Loss:  -5.7856 | Function Loss:  -3.9201\n",
      "Total loss:  -3.8646 | PDE Loss:  -5.7859 | Function Loss:  -3.9201\n",
      "Total loss:  -3.8647 | PDE Loss:  -5.7867 | Function Loss:  -3.9201\n",
      "Total loss:  -3.8648 | PDE Loss:  -5.7877 | Function Loss:  -3.92\n",
      "Total loss:  -3.8649 | PDE Loss:  -5.7891 | Function Loss:  -3.92\n",
      "Total loss:  -3.865 | PDE Loss:  -5.7911 | Function Loss:  -3.9198\n",
      "Total loss:  -3.8652 | PDE Loss:  -5.7926 | Function Loss:  -3.9198\n",
      "Total loss:  -3.8653 | PDE Loss:  -5.7939 | Function Loss:  -3.9197\n",
      "Total loss:  -3.8654 | PDE Loss:  -5.7949 | Function Loss:  -3.9197\n",
      "Total loss:  -3.8656 | PDE Loss:  -5.7959 | Function Loss:  -3.9198\n",
      "Total loss:  -3.8657 | PDE Loss:  -5.7968 | Function Loss:  -3.9199\n",
      "Total loss:  -3.8659 | PDE Loss:  -5.7977 | Function Loss:  -3.92\n",
      "Total loss:  -3.8661 | PDE Loss:  -5.798 | Function Loss:  -3.9201\n",
      "Total loss:  -3.8663 | PDE Loss:  -5.7987 | Function Loss:  -3.9202\n",
      "Total loss:  -3.8664 | PDE Loss:  -5.7991 | Function Loss:  -3.9203\n",
      "Total loss:  -3.8666 | PDE Loss:  -5.8001 | Function Loss:  -3.9204\n",
      "Total loss:  -3.8667 | PDE Loss:  -5.8009 | Function Loss:  -3.9204\n",
      "Total loss:  -3.8669 | PDE Loss:  -5.8018 | Function Loss:  -3.9205\n",
      "Total loss:  -3.867 | PDE Loss:  -5.8026 | Function Loss:  -3.9205\n",
      "Total loss:  -3.8672 | PDE Loss:  -5.8037 | Function Loss:  -3.9206\n",
      "Total loss:  -3.8673 | PDE Loss:  -5.8042 | Function Loss:  -3.9207\n",
      "Total loss:  -3.8675 | PDE Loss:  -5.8045 | Function Loss:  -3.9208\n",
      "Total loss:  -3.8676 | PDE Loss:  -5.8045 | Function Loss:  -3.921\n",
      "Total loss:  -3.8677 | PDE Loss:  -5.8042 | Function Loss:  -3.9211\n",
      "Total loss:  -3.8678 | PDE Loss:  -5.8037 | Function Loss:  -3.9213\n",
      "Total loss:  -3.8679 | PDE Loss:  -5.8033 | Function Loss:  -3.9215\n",
      "Total loss:  -3.868 | PDE Loss:  -5.8021 | Function Loss:  -3.9217\n",
      "Total loss:  -3.8681 | PDE Loss:  -5.8017 | Function Loss:  -3.9219\n",
      "Total loss:  -3.8682 | PDE Loss:  -5.8007 | Function Loss:  -3.9221\n",
      "Total loss:  -3.8682 | PDE Loss:  -5.8001 | Function Loss:  -3.9223\n",
      "Total loss:  -3.8683 | PDE Loss:  -5.7993 | Function Loss:  -3.9225\n",
      "Total loss:  -3.8684 | PDE Loss:  -5.7986 | Function Loss:  -3.9227\n",
      "Total loss:  -3.8686 | PDE Loss:  -5.7977 | Function Loss:  -3.923\n",
      "Total loss:  -3.8687 | PDE Loss:  -5.7976 | Function Loss:  -3.9231\n",
      "Total loss:  -3.8688 | PDE Loss:  -5.7969 | Function Loss:  -3.9233\n",
      "Total loss:  -3.8689 | PDE Loss:  -5.7967 | Function Loss:  -3.9235\n",
      "Total loss:  -3.869 | PDE Loss:  -5.7995 | Function Loss:  -3.9232\n",
      "Total loss:  -3.8691 | PDE Loss:  -5.7983 | Function Loss:  -3.9235\n",
      "Total loss:  -3.8691 | PDE Loss:  -5.7979 | Function Loss:  -3.9236\n",
      "Total loss:  -3.8692 | PDE Loss:  -5.7976 | Function Loss:  -3.9237\n",
      "Total loss:  -3.8693 | PDE Loss:  -5.7977 | Function Loss:  -3.9238\n",
      "Total loss:  -3.8695 | PDE Loss:  -5.798 | Function Loss:  -3.924\n",
      "Total loss:  -3.8696 | PDE Loss:  -5.7978 | Function Loss:  -3.9241\n",
      "Total loss:  -3.8698 | PDE Loss:  -5.7982 | Function Loss:  -3.9243\n",
      "Total loss:  -3.87 | PDE Loss:  -5.7985 | Function Loss:  -3.9245\n",
      "Total loss:  -3.8702 | PDE Loss:  -5.7992 | Function Loss:  -3.9246\n",
      "Total loss:  -3.8704 | PDE Loss:  -5.7991 | Function Loss:  -3.9248\n",
      "Total loss:  -3.8705 | PDE Loss:  -5.7996 | Function Loss:  -3.9249\n",
      "Total loss:  -3.8707 | PDE Loss:  -5.7993 | Function Loss:  -3.9251\n",
      "Total loss:  -3.8709 | PDE Loss:  -5.7992 | Function Loss:  -3.9254\n",
      "Total loss:  -3.8711 | PDE Loss:  -5.7985 | Function Loss:  -3.9258\n",
      "Total loss:  -3.8714 | PDE Loss:  -5.7994 | Function Loss:  -3.9259\n",
      "Total loss:  -3.8716 | PDE Loss:  -5.7988 | Function Loss:  -3.9262\n",
      "Total loss:  -3.8717 | PDE Loss:  -5.7998 | Function Loss:  -3.9262\n",
      "Total loss:  -3.8717 | PDE Loss:  -5.7998 | Function Loss:  -3.9263\n",
      "Total loss:  -3.8718 | PDE Loss:  -5.8002 | Function Loss:  -3.9263\n",
      "Total loss:  -3.8718 | PDE Loss:  -5.8002 | Function Loss:  -3.9263\n",
      "Total loss:  -3.8718 | PDE Loss:  -5.8004 | Function Loss:  -3.9263\n",
      "Total loss:  -3.8719 | PDE Loss:  -5.8004 | Function Loss:  -3.9263\n",
      "Total loss:  -3.8719 | PDE Loss:  -5.8004 | Function Loss:  -3.9264\n",
      "Total loss:  -3.8719 | PDE Loss:  -5.8003 | Function Loss:  -3.9264\n",
      "Total loss:  -3.872 | PDE Loss:  -5.8001 | Function Loss:  -3.9265\n",
      "Total loss:  -3.872 | PDE Loss:  -5.8 | Function Loss:  -3.9266\n",
      "Total loss:  -3.8721 | PDE Loss:  -5.7995 | Function Loss:  -3.9267\n",
      "Total loss:  -3.8721 | PDE Loss:  -5.7989 | Function Loss:  -3.9268\n",
      "Total loss:  -3.8721 | PDE Loss:  -5.7979 | Function Loss:  -3.927\n",
      "Total loss:  -3.8722 | PDE Loss:  -5.7975 | Function Loss:  -3.9271\n",
      "Total loss:  -3.8723 | PDE Loss:  -5.7964 | Function Loss:  -3.9274\n",
      "Total loss:  -3.8724 | PDE Loss:  -5.7956 | Function Loss:  -3.9276\n",
      "Total loss:  -3.8725 | PDE Loss:  -5.7949 | Function Loss:  -3.9278\n",
      "Total loss:  -3.8726 | PDE Loss:  -5.7945 | Function Loss:  -3.9279\n",
      "Total loss:  -3.8727 | PDE Loss:  -5.7944 | Function Loss:  -3.9281\n",
      "Total loss:  -3.8728 | PDE Loss:  -5.7941 | Function Loss:  -3.9283\n",
      "Total loss:  -3.873 | PDE Loss:  -5.7931 | Function Loss:  -3.9286\n",
      "Total loss:  -3.8731 | PDE Loss:  -5.7925 | Function Loss:  -3.9288\n",
      "Total loss:  -3.8732 | PDE Loss:  -5.792 | Function Loss:  -3.929\n",
      "Total loss:  -3.8733 | PDE Loss:  -5.7916 | Function Loss:  -3.9292\n",
      "Total loss:  -3.8734 | PDE Loss:  -5.7907 | Function Loss:  -3.9294\n",
      "Total loss:  -3.8735 | PDE Loss:  -5.7903 | Function Loss:  -3.9295\n",
      "Total loss:  -3.8735 | PDE Loss:  -5.7896 | Function Loss:  -3.9297\n",
      "Total loss:  -3.8736 | PDE Loss:  -5.789 | Function Loss:  -3.9298\n",
      "Total loss:  -3.8737 | PDE Loss:  -5.7884 | Function Loss:  -3.93\n",
      "Total loss:  -3.8737 | PDE Loss:  -5.788 | Function Loss:  -3.9302\n",
      "Total loss:  -3.8738 | PDE Loss:  -5.7867 | Function Loss:  -3.9304\n",
      "Total loss:  -3.8739 | PDE Loss:  -5.7871 | Function Loss:  -3.9305\n",
      "Total loss:  -3.874 | PDE Loss:  -5.7872 | Function Loss:  -3.9306\n",
      "Total loss:  -3.8741 | PDE Loss:  -5.7875 | Function Loss:  -3.9307\n",
      "Total loss:  -3.8742 | PDE Loss:  -5.7877 | Function Loss:  -3.9308\n",
      "Total loss:  -3.8744 | PDE Loss:  -5.7869 | Function Loss:  -3.9311\n",
      "Total loss:  -3.8745 | PDE Loss:  -5.7865 | Function Loss:  -3.9313\n",
      "Total loss:  -3.8747 | PDE Loss:  -5.7856 | Function Loss:  -3.9316\n",
      "Total loss:  -3.8749 | PDE Loss:  -5.7834 | Function Loss:  -3.9321\n",
      "Total loss:  -3.875 | PDE Loss:  -5.7829 | Function Loss:  -3.9323\n",
      "Total loss:  -3.8752 | PDE Loss:  -5.7823 | Function Loss:  -3.9326\n",
      "Total loss:  -3.8753 | PDE Loss:  -5.7813 | Function Loss:  -3.9328\n",
      "Total loss:  -3.8754 | PDE Loss:  -5.7811 | Function Loss:  -3.933\n",
      "Total loss:  -3.8755 | PDE Loss:  -5.7802 | Function Loss:  -3.9332\n",
      "Total loss:  -3.8756 | PDE Loss:  -5.7799 | Function Loss:  -3.9334\n",
      "Total loss:  -3.8757 | PDE Loss:  -5.7798 | Function Loss:  -3.9336\n",
      "Total loss:  -3.8758 | PDE Loss:  -5.7794 | Function Loss:  -3.9338\n",
      "Total loss:  -3.8759 | PDE Loss:  -5.7792 | Function Loss:  -3.9339\n",
      "Total loss:  -3.8761 | PDE Loss:  -5.7783 | Function Loss:  -3.9342\n",
      "Total loss:  -3.8762 | PDE Loss:  -5.7779 | Function Loss:  -3.9344\n",
      "Total loss:  -3.8763 | PDE Loss:  -5.7743 | Function Loss:  -3.935\n",
      "Total loss:  -3.8764 | PDE Loss:  -5.7741 | Function Loss:  -3.9352\n",
      "Total loss:  -3.8765 | PDE Loss:  -5.7741 | Function Loss:  -3.9353\n",
      "Total loss:  -3.8767 | PDE Loss:  -5.7733 | Function Loss:  -3.9356\n",
      "Total loss:  -3.8768 | PDE Loss:  -5.7727 | Function Loss:  -3.9359\n",
      "Total loss:  -3.877 | PDE Loss:  -5.7711 | Function Loss:  -3.9363\n",
      "Total loss:  -3.8772 | PDE Loss:  -5.7703 | Function Loss:  -3.9366\n",
      "Total loss:  -3.8774 | PDE Loss:  -5.7686 | Function Loss:  -3.9371\n",
      "Total loss:  -3.8775 | PDE Loss:  -5.7676 | Function Loss:  -3.9374\n",
      "Total loss:  -3.8777 | PDE Loss:  -5.7657 | Function Loss:  -3.9379\n",
      "Total loss:  -3.8778 | PDE Loss:  -5.7651 | Function Loss:  -3.9381\n",
      "Total loss:  -3.8779 | PDE Loss:  -5.7653 | Function Loss:  -3.9382\n",
      "Total loss:  -3.878 | PDE Loss:  -5.7655 | Function Loss:  -3.9382\n",
      "Total loss:  -3.878 | PDE Loss:  -5.7662 | Function Loss:  -3.9382\n",
      "Total loss:  -3.8781 | PDE Loss:  -5.7666 | Function Loss:  -3.9382\n",
      "Total loss:  -3.8781 | PDE Loss:  -5.7673 | Function Loss:  -3.9381\n",
      "Total loss:  -3.8782 | PDE Loss:  -5.7678 | Function Loss:  -3.9381\n",
      "Total loss:  -3.8782 | PDE Loss:  -5.7682 | Function Loss:  -3.9381\n",
      "Total loss:  -3.8783 | PDE Loss:  -5.7688 | Function Loss:  -3.9381\n",
      "Total loss:  -3.8783 | PDE Loss:  -5.7687 | Function Loss:  -3.9381\n",
      "Total loss:  -3.8783 | PDE Loss:  -5.769 | Function Loss:  -3.9381\n",
      "Total loss:  -3.8783 | PDE Loss:  -5.7689 | Function Loss:  -3.9381\n",
      "Total loss:  -3.8784 | PDE Loss:  -5.7689 | Function Loss:  -3.9382\n",
      "Total loss:  -3.8784 | PDE Loss:  -5.7687 | Function Loss:  -3.9382\n",
      "Total loss:  -3.8784 | PDE Loss:  -5.7687 | Function Loss:  -3.9383\n",
      "Total loss:  -3.8785 | PDE Loss:  -5.7684 | Function Loss:  -3.9384\n",
      "Total loss:  -3.8785 | PDE Loss:  -5.7688 | Function Loss:  -3.9384\n",
      "Total loss:  -3.8786 | PDE Loss:  -5.7686 | Function Loss:  -3.9385\n",
      "Total loss:  -3.8787 | PDE Loss:  -5.7687 | Function Loss:  -3.9386\n",
      "Total loss:  -3.8787 | PDE Loss:  -5.7688 | Function Loss:  -3.9386\n",
      "Total loss:  -3.8788 | PDE Loss:  -5.7689 | Function Loss:  -3.9387\n",
      "Total loss:  -3.8788 | PDE Loss:  -5.7691 | Function Loss:  -3.9387\n",
      "Total loss:  -3.8789 | PDE Loss:  -5.7692 | Function Loss:  -3.9387\n",
      "Total loss:  -3.8789 | PDE Loss:  -5.7693 | Function Loss:  -3.9387\n",
      "Total loss:  -3.879 | PDE Loss:  -5.7693 | Function Loss:  -3.9388\n",
      "Total loss:  -3.879 | PDE Loss:  -5.7689 | Function Loss:  -3.9389\n",
      "Total loss:  -3.8791 | PDE Loss:  -5.7687 | Function Loss:  -3.9391\n",
      "Total loss:  -3.8792 | PDE Loss:  -5.7681 | Function Loss:  -3.9393\n",
      "Total loss:  -3.8794 | PDE Loss:  -5.7675 | Function Loss:  -3.9395\n",
      "Total loss:  -3.8795 | PDE Loss:  -5.7664 | Function Loss:  -3.9399\n",
      "Total loss:  -3.8797 | PDE Loss:  -5.7648 | Function Loss:  -3.9403\n",
      "Total loss:  -3.8798 | PDE Loss:  -5.764 | Function Loss:  -3.9405\n",
      "Total loss:  -3.8799 | PDE Loss:  -5.7629 | Function Loss:  -3.9408\n",
      "Total loss:  -3.88 | PDE Loss:  -5.7625 | Function Loss:  -3.9411\n",
      "Total loss:  -3.8801 | PDE Loss:  -5.7617 | Function Loss:  -3.9413\n",
      "Total loss:  -3.8803 | PDE Loss:  -5.7611 | Function Loss:  -3.9415\n",
      "Total loss:  -3.8803 | PDE Loss:  -5.7598 | Function Loss:  -3.9418\n",
      "Total loss:  -3.8804 | PDE Loss:  -5.7594 | Function Loss:  -3.942\n",
      "Total loss:  -3.8805 | PDE Loss:  -5.7585 | Function Loss:  -3.9422\n",
      "Total loss:  -3.8806 | PDE Loss:  -5.7575 | Function Loss:  -3.9425\n",
      "Total loss:  -3.8808 | PDE Loss:  -5.7566 | Function Loss:  -3.9428\n",
      "Total loss:  -3.8809 | PDE Loss:  -5.7557 | Function Loss:  -3.9431\n",
      "Total loss:  -3.881 | PDE Loss:  -5.7545 | Function Loss:  -3.9434\n",
      "Total loss:  -3.8811 | PDE Loss:  -5.7532 | Function Loss:  -3.9437\n",
      "Total loss:  -3.8813 | PDE Loss:  -5.7518 | Function Loss:  -3.9442\n",
      "Total loss:  -3.8815 | PDE Loss:  -5.7496 | Function Loss:  -3.9447\n",
      "Total loss:  -3.8816 | PDE Loss:  -5.7488 | Function Loss:  -3.945\n",
      "Total loss:  -3.8818 | PDE Loss:  -5.7478 | Function Loss:  -3.9454\n",
      "Total loss:  -3.882 | PDE Loss:  -5.7475 | Function Loss:  -3.9456\n",
      "Total loss:  -3.8821 | PDE Loss:  -5.7471 | Function Loss:  -3.9458\n",
      "Total loss:  -3.8822 | PDE Loss:  -5.7471 | Function Loss:  -3.9459\n",
      "Total loss:  -3.8823 | PDE Loss:  -5.7465 | Function Loss:  -3.9461\n",
      "Total loss:  -3.8824 | PDE Loss:  -5.7461 | Function Loss:  -3.9463\n",
      "Total loss:  -3.8825 | PDE Loss:  -5.7457 | Function Loss:  -3.9465\n",
      "Total loss:  -3.8825 | PDE Loss:  -5.7449 | Function Loss:  -3.9467\n",
      "Total loss:  -3.8826 | PDE Loss:  -5.7451 | Function Loss:  -3.9467\n",
      "Total loss:  -3.8827 | PDE Loss:  -5.7447 | Function Loss:  -3.9468\n",
      "Total loss:  -3.8827 | PDE Loss:  -5.7452 | Function Loss:  -3.9468\n",
      "Total loss:  -3.8828 | PDE Loss:  -5.7458 | Function Loss:  -3.9468\n",
      "Total loss:  -3.8828 | PDE Loss:  -5.7468 | Function Loss:  -3.9467\n",
      "Total loss:  -3.8829 | PDE Loss:  -5.7479 | Function Loss:  -3.9466\n",
      "Total loss:  -3.883 | PDE Loss:  -5.7499 | Function Loss:  -3.9464\n",
      "Total loss:  -3.8829 | PDE Loss:  -5.7487 | Function Loss:  -3.9465\n",
      "Total loss:  -3.883 | PDE Loss:  -5.7495 | Function Loss:  -3.9464\n",
      "Total loss:  -3.8831 | PDE Loss:  -5.7515 | Function Loss:  -3.9462\n",
      "Total loss:  -3.8832 | PDE Loss:  -5.7529 | Function Loss:  -3.9461\n",
      "Total loss:  -3.8832 | PDE Loss:  -5.7537 | Function Loss:  -3.9461\n",
      "Total loss:  -3.8832 | PDE Loss:  -5.7542 | Function Loss:  -3.946\n",
      "Total loss:  -3.8833 | PDE Loss:  -5.7539 | Function Loss:  -3.9461\n",
      "Total loss:  -3.8834 | PDE Loss:  -5.7539 | Function Loss:  -3.9462\n",
      "Total loss:  -3.8834 | PDE Loss:  -5.7534 | Function Loss:  -3.9464\n",
      "Total loss:  -3.8835 | PDE Loss:  -5.753 | Function Loss:  -3.9465\n",
      "Total loss:  -3.8836 | PDE Loss:  -5.7528 | Function Loss:  -3.9467\n",
      "Total loss:  -3.8837 | PDE Loss:  -5.7524 | Function Loss:  -3.9468\n",
      "Total loss:  -3.8837 | PDE Loss:  -5.7526 | Function Loss:  -3.9468\n",
      "Total loss:  -3.8838 | PDE Loss:  -5.7533 | Function Loss:  -3.9468\n",
      "Total loss:  -3.8838 | PDE Loss:  -5.7538 | Function Loss:  -3.9468\n",
      "Total loss:  -3.8839 | PDE Loss:  -5.7553 | Function Loss:  -3.9466\n",
      "Total loss:  -3.8839 | PDE Loss:  -5.7556 | Function Loss:  -3.9466\n",
      "Total loss:  -3.884 | PDE Loss:  -5.7556 | Function Loss:  -3.9467\n",
      "Total loss:  -3.884 | PDE Loss:  -5.7555 | Function Loss:  -3.9467\n",
      "Total loss:  -3.8841 | PDE Loss:  -5.7547 | Function Loss:  -3.9469\n",
      "Total loss:  -3.8842 | PDE Loss:  -5.7539 | Function Loss:  -3.9472\n",
      "Total loss:  -3.8843 | PDE Loss:  -5.7524 | Function Loss:  -3.9475\n",
      "Total loss:  -3.8844 | PDE Loss:  -5.7517 | Function Loss:  -3.9477\n",
      "Total loss:  -3.8844 | PDE Loss:  -5.7505 | Function Loss:  -3.948\n",
      "Total loss:  -3.8846 | PDE Loss:  -5.7506 | Function Loss:  -3.9481\n",
      "Total loss:  -3.8847 | PDE Loss:  -5.7491 | Function Loss:  -3.9485\n",
      "Total loss:  -3.8849 | PDE Loss:  -5.7501 | Function Loss:  -3.9485\n",
      "Total loss:  -3.885 | PDE Loss:  -5.7522 | Function Loss:  -3.9484\n",
      "Total loss:  -3.8852 | PDE Loss:  -5.7548 | Function Loss:  -3.9482\n",
      "Total loss:  -3.8854 | PDE Loss:  -5.758 | Function Loss:  -3.9479\n",
      "Total loss:  -3.8856 | PDE Loss:  -5.7606 | Function Loss:  -3.9477\n",
      "Total loss:  -3.8857 | PDE Loss:  -5.7627 | Function Loss:  -3.9476\n",
      "Total loss:  -3.886 | PDE Loss:  -5.765 | Function Loss:  -3.9475\n",
      "Total loss:  -3.8862 | PDE Loss:  -5.7662 | Function Loss:  -3.9476\n",
      "Total loss:  -3.8864 | PDE Loss:  -5.7661 | Function Loss:  -3.9478\n",
      "Total loss:  -3.8866 | PDE Loss:  -5.7655 | Function Loss:  -3.9481\n",
      "Total loss:  -3.8868 | PDE Loss:  -5.7639 | Function Loss:  -3.9487\n",
      "Total loss:  -3.887 | PDE Loss:  -5.7635 | Function Loss:  -3.949\n",
      "Total loss:  -3.8873 | PDE Loss:  -5.7611 | Function Loss:  -3.9496\n",
      "Total loss:  -3.8874 | PDE Loss:  -5.7602 | Function Loss:  -3.9499\n",
      "Total loss:  -3.8876 | PDE Loss:  -5.7598 | Function Loss:  -3.9502\n",
      "Total loss:  -3.8879 | PDE Loss:  -5.7598 | Function Loss:  -3.9505\n",
      "Total loss:  -3.8881 | PDE Loss:  -5.7597 | Function Loss:  -3.9508\n",
      "Total loss:  -3.8883 | PDE Loss:  -5.7603 | Function Loss:  -3.9509\n",
      "Total loss:  -3.8886 | PDE Loss:  -5.7602 | Function Loss:  -3.9513\n",
      "Total loss:  -3.8889 | PDE Loss:  -5.7611 | Function Loss:  -3.9515\n",
      "Total loss:  -3.8892 | PDE Loss:  -5.7605 | Function Loss:  -3.9519\n",
      "Total loss:  -3.8894 | PDE Loss:  -5.7603 | Function Loss:  -3.9522\n",
      "Total loss:  -3.8896 | PDE Loss:  -5.7592 | Function Loss:  -3.9526\n",
      "Total loss:  -3.8898 | PDE Loss:  -5.7583 | Function Loss:  -3.9529\n",
      "Total loss:  -3.8899 | PDE Loss:  -5.7567 | Function Loss:  -3.9534\n",
      "Total loss:  -3.8901 | PDE Loss:  -5.7557 | Function Loss:  -3.9537\n",
      "Total loss:  -3.8903 | PDE Loss:  -5.7553 | Function Loss:  -3.954\n",
      "Total loss:  -3.8905 | PDE Loss:  -5.7549 | Function Loss:  -3.9543\n",
      "Total loss:  -3.8907 | PDE Loss:  -5.7559 | Function Loss:  -3.9543\n",
      "Total loss:  -3.8908 | PDE Loss:  -5.7569 | Function Loss:  -3.9543\n",
      "Total loss:  -3.8908 | PDE Loss:  -5.7578 | Function Loss:  -3.9542\n",
      "Total loss:  -3.8909 | PDE Loss:  -5.7593 | Function Loss:  -3.9541\n",
      "Total loss:  -3.891 | PDE Loss:  -5.7604 | Function Loss:  -3.9541\n",
      "Total loss:  -3.8911 | PDE Loss:  -5.761 | Function Loss:  -3.9541\n",
      "Total loss:  -3.8912 | PDE Loss:  -5.7614 | Function Loss:  -3.9541\n",
      "Total loss:  -3.8913 | PDE Loss:  -5.7604 | Function Loss:  -3.9544\n",
      "Total loss:  -3.8914 | PDE Loss:  -5.7598 | Function Loss:  -3.9546\n",
      "Total loss:  -3.8915 | PDE Loss:  -5.7579 | Function Loss:  -3.955\n",
      "Total loss:  -3.8916 | PDE Loss:  -5.7559 | Function Loss:  -3.9554\n",
      "Total loss:  -3.8916 | PDE Loss:  -5.7533 | Function Loss:  -3.9559\n",
      "Total loss:  -3.8917 | PDE Loss:  -5.7514 | Function Loss:  -3.9563\n",
      "Total loss:  -3.8917 | PDE Loss:  -5.7496 | Function Loss:  -3.9566\n",
      "Total loss:  -3.8918 | PDE Loss:  -5.7486 | Function Loss:  -3.9568\n",
      "Total loss:  -3.8918 | PDE Loss:  -5.747 | Function Loss:  -3.9571\n",
      "Total loss:  -3.8919 | PDE Loss:  -5.7466 | Function Loss:  -3.9572\n",
      "Total loss:  -3.8919 | PDE Loss:  -5.7468 | Function Loss:  -3.9573\n",
      "Total loss:  -3.892 | PDE Loss:  -5.7466 | Function Loss:  -3.9574\n",
      "Total loss:  -3.892 | PDE Loss:  -5.7475 | Function Loss:  -3.9573\n",
      "Total loss:  -3.8921 | PDE Loss:  -5.7481 | Function Loss:  -3.9573\n",
      "Total loss:  -3.8922 | PDE Loss:  -5.7487 | Function Loss:  -3.9572\n",
      "Total loss:  -3.8922 | PDE Loss:  -5.7495 | Function Loss:  -3.9572\n",
      "Total loss:  -3.8924 | PDE Loss:  -5.7503 | Function Loss:  -3.9572\n",
      "Total loss:  -3.8925 | PDE Loss:  -5.751 | Function Loss:  -3.9572\n",
      "Total loss:  -3.8926 | PDE Loss:  -5.751 | Function Loss:  -3.9573\n",
      "Total loss:  -3.8926 | PDE Loss:  -5.7508 | Function Loss:  -3.9574\n",
      "Total loss:  -3.8927 | PDE Loss:  -5.7504 | Function Loss:  -3.9576\n",
      "Total loss:  -3.8928 | PDE Loss:  -5.7493 | Function Loss:  -3.9579\n",
      "Total loss:  -3.8929 | PDE Loss:  -5.7486 | Function Loss:  -3.9581\n",
      "Total loss:  -3.893 | PDE Loss:  -5.7476 | Function Loss:  -3.9584\n",
      "Total loss:  -3.8932 | PDE Loss:  -5.7471 | Function Loss:  -3.9587\n",
      "Total loss:  -3.8934 | PDE Loss:  -5.7462 | Function Loss:  -3.959\n",
      "Total loss:  -3.8935 | PDE Loss:  -5.7459 | Function Loss:  -3.9592\n",
      "Total loss:  -3.8936 | PDE Loss:  -5.7456 | Function Loss:  -3.9594\n",
      "Total loss:  -3.8937 | PDE Loss:  -5.7456 | Function Loss:  -3.9595\n",
      "Total loss:  -3.8937 | PDE Loss:  -5.7452 | Function Loss:  -3.9596\n",
      "Total loss:  -3.8938 | PDE Loss:  -5.745 | Function Loss:  -3.9598\n",
      "Total loss:  -3.8939 | PDE Loss:  -5.7445 | Function Loss:  -3.9599\n",
      "Total loss:  -3.8939 | PDE Loss:  -5.7439 | Function Loss:  -3.9601\n",
      "Total loss:  -3.8941 | PDE Loss:  -5.7431 | Function Loss:  -3.9604\n",
      "Total loss:  -3.8942 | PDE Loss:  -5.7416 | Function Loss:  -3.9608\n",
      "Total loss:  -3.8943 | PDE Loss:  -5.7401 | Function Loss:  -3.9612\n",
      "Total loss:  -3.8944 | PDE Loss:  -5.7391 | Function Loss:  -3.9614\n",
      "Total loss:  -3.8945 | PDE Loss:  -5.7383 | Function Loss:  -3.9617\n",
      "Total loss:  -3.8946 | PDE Loss:  -5.738 | Function Loss:  -3.9619\n",
      "Total loss:  -3.8948 | PDE Loss:  -5.7379 | Function Loss:  -3.962\n",
      "Total loss:  -3.8949 | PDE Loss:  -5.7382 | Function Loss:  -3.9621\n",
      "Total loss:  -3.8949 | PDE Loss:  -5.7382 | Function Loss:  -3.9622\n",
      "Total loss:  -3.895 | PDE Loss:  -5.739 | Function Loss:  -3.9621\n",
      "Total loss:  -3.8951 | PDE Loss:  -5.7393 | Function Loss:  -3.9622\n",
      "Total loss:  -3.8951 | PDE Loss:  -5.7398 | Function Loss:  -3.9622\n",
      "Total loss:  -3.8952 | PDE Loss:  -5.7404 | Function Loss:  -3.9622\n",
      "Total loss:  -3.8953 | PDE Loss:  -5.7404 | Function Loss:  -3.9623\n",
      "Total loss:  -3.8954 | PDE Loss:  -5.741 | Function Loss:  -3.9623\n",
      "Total loss:  -3.8954 | PDE Loss:  -5.7415 | Function Loss:  -3.9622\n",
      "Total loss:  -3.8955 | PDE Loss:  -5.7413 | Function Loss:  -3.9624\n",
      "Total loss:  -3.8956 | PDE Loss:  -5.7413 | Function Loss:  -3.9624\n",
      "Total loss:  -3.8957 | PDE Loss:  -5.7408 | Function Loss:  -3.9626\n",
      "Total loss:  -3.8957 | PDE Loss:  -5.7405 | Function Loss:  -3.9626\n",
      "Total loss:  -3.8956 | PDE Loss:  -5.7406 | Function Loss:  -3.9626\n",
      "Total loss:  -3.8956 | PDE Loss:  -5.7405 | Function Loss:  -3.9626\n",
      "Total loss:  -3.8956 | PDE Loss:  -5.7407 | Function Loss:  -3.9626\n",
      "Total loss:  -3.8957 | PDE Loss:  -5.7408 | Function Loss:  -3.9626\n",
      "Final Loss:  -3.8956613540649414\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x15697ea30>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 960x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hist = pinn.fit(num_epochs=n_epochs,\n",
    "                optimizer=optimizer_LBFGS,\n",
    "                verbose=True)\n",
    "\n",
    "plt.figure(dpi=150)\n",
    "plt.grid(True, which=\"both\", ls=\":\")\n",
    "plt.plot(np.arange(1, len(hist) + 1), hist, label=\"Train Loss\")\n",
    "plt.xscale(\"log\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:  tensor([1., 0.], requires_grad=True)\n",
      "output:  tensor([3.9993, 3.5189], grad_fn=<ReshapeAliasBackward0>)\n"
     ]
    }
   ],
   "source": [
    "t = 1\n",
    "x = 0\n",
    "# inputs [t, x]\n",
    "inputs = torch.tensor([t, x], dtype=torch.float, requires_grad=True)\n",
    "output = pinn.approximate_solution(inputs).reshape(-1, )\n",
    "\n",
    "print(\"inputs: \", inputs)\n",
    "print(\"output: \", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0022, grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tf = output[0]\n",
    "Ts = output[1]\n",
    "t = inputs[0]\n",
    "x = inputs[1]\n",
    "dTf_dx = torch.autograd.grad(Ts, inputs, create_graph=True)[0][1]\n",
    "dTf_dx"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Boundary conditions\n",
    "\n",
    "Here we are going to evaluate the pinn outputs at the points where we have the boundary conditions and we will veridy manually thet they correspond to the correct one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_sb = pinn.soboleng.draw(10)\n",
    "# input_sb.requires_grad_()\n",
    "\n",
    "# # T=0\n",
    "# x0_t0 = pinn.domain_extrema[0, 0]\n",
    "# xL_t0 = pinn.domain_extrema[0, 1]\n",
    "\n",
    "# input_sb_0_t0 = torch.clone(input_sb)\n",
    "# input_sb_0_t0[:, 1] = torch.full(input_sb_0_t0[:, 1].shape, x0_t0)\n",
    "\n",
    "# input_sb_L_t0 = torch.clone(input_sb)\n",
    "# input_sb_L_t0[:, 1] = torch.full(input_sb_L_t0[:, 1].shape, xL_t0)\n",
    "\n",
    "# # T=1\n",
    "# x0_t1 = pinn.domain_extrema[1, 0]\n",
    "# xL_t1 = pinn.domain_extrema[1, 1]\n",
    "\n",
    "# input_sb_0_t1 = torch.clone(input_sb)\n",
    "# input_sb_0_t1[:, 1] = torch.full(input_sb_0_t1[:, 1].shape, x0_t1)\n",
    "\n",
    "# input_sb_L_t1 = torch.clone(input_sb)\n",
    "# input_sb_L_t1[:, 1] = torch.full(input_sb_L_t1[:, 1].shape, xL_t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_sb_0: \n",
      " tensor([[0.0098, 0.0000],\n",
      "        [0.5098, 0.0000],\n",
      "        [0.7598, 0.0000],\n",
      "        [0.2598, 0.0000],\n",
      "        [0.3848, 0.0000],\n",
      "        [0.8848, 0.0000],\n",
      "        [0.6348, 0.0000],\n",
      "        [0.1348, 0.0000],\n",
      "        [0.1973, 0.0000],\n",
      "        [0.6973, 0.0000]], grad_fn=<CopySlices>)\n",
      "\n",
      "input_sb_L: \n",
      " tensor([[0.0098, 1.0000],\n",
      "        [0.5098, 1.0000],\n",
      "        [0.7598, 1.0000],\n",
      "        [0.2598, 1.0000],\n",
      "        [0.3848, 1.0000],\n",
      "        [0.8848, 1.0000],\n",
      "        [0.6348, 1.0000],\n",
      "        [0.1348, 1.0000],\n",
      "        [0.1973, 1.0000],\n",
      "        [0.6973, 1.0000]], grad_fn=<CopySlices>)\n",
      "\n",
      "----------------------------------------------\n",
      "\n",
      "dTs_0_dx: \n",
      " tensor([[-0.0003],\n",
      "        [-0.0004],\n",
      "        [-0.0005],\n",
      "        [-0.0396],\n",
      "        [ 0.0003],\n",
      "        [ 0.0007],\n",
      "        [ 0.0003],\n",
      "        [ 0.0003],\n",
      "        [-0.0002],\n",
      "        [-0.0004]], grad_fn=<ReshapeAliasBackward0>)\n",
      "\n",
      "dTf_L_dx: \n",
      " tensor([[-1.0044e-03],\n",
      "        [ 7.0505e-04],\n",
      "        [ 3.0136e-05],\n",
      "        [-1.6981e-04],\n",
      "        [-4.0656e-04],\n",
      "        [ 1.0697e-03],\n",
      "        [-1.4309e-03],\n",
      "        [-5.9697e-04],\n",
      "        [-3.1179e-04],\n",
      "        [ 9.4765e-04]], grad_fn=<ReshapeAliasBackward0>)\n",
      "\n",
      "dTs_L_dx: \n",
      " tensor([[-5.9496e-04],\n",
      "        [ 8.8563e-04],\n",
      "        [-1.0701e-03],\n",
      "        [-7.2884e-05],\n",
      "        [-1.1547e-04],\n",
      "        [-4.3657e-04],\n",
      "        [ 8.1550e-04],\n",
      "        [-2.0343e-04],\n",
      "        [-3.0730e-05],\n",
      "        [ 8.8216e-04]], grad_fn=<ReshapeAliasBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x0 = pinn.domain_extrema[1, 0]\n",
    "xL = pinn.domain_extrema[1, 1]\n",
    "\n",
    "input_sb = pinn.soboleng.draw(10)\n",
    "input_sb.requires_grad_()\n",
    "\n",
    "input_sb_0 = torch.clone(input_sb)\n",
    "input_sb_0[:, 1] = torch.full(input_sb_0[:, 1].shape, x0)\n",
    "\n",
    "input_sb_L = torch.clone(input_sb)\n",
    "input_sb_L[:, 1] = torch.full(input_sb_L[:, 1].shape, xL)\n",
    "\n",
    "print(\"input_sb_0: \\n\", input_sb_0)\n",
    "print()\n",
    "print(\"input_sb_L: \\n\", input_sb_L)\n",
    "\n",
    "print()\n",
    "print(\"----------------------------------------------\")\n",
    "print()\n",
    "\n",
    "T_0 = pinn.approximate_solution(input_sb_0)\n",
    "Tf_0 = T_0[:,0]\n",
    "Ts_0 = T_0[:,1]\n",
    "dTs_0_dx = torch.autograd.grad(Ts_0.sum(), input_sb_0, create_graph=True)[0][:, 1]\n",
    "\n",
    "T_L = pinn.approximate_solution(input_sb_L)\n",
    "Tf_L = T_L[:,0]\n",
    "Ts_L = T_L[:,1]\n",
    "\n",
    "dTf_L_dx = torch.autograd.grad(Tf_L.sum(), input_sb_L, create_graph=True)[0][:, 1]\n",
    "dTs_L_dx = torch.autograd.grad(Ts_L.sum(), input_sb_L, create_graph=True)[0][:, 1]\n",
    "\n",
    "print(\"dTs_0_dx: \\n\", dTs_0_dx.reshape(-1,1))\n",
    "print()\n",
    "print(\"dTf_L_dx: \\n\", dTf_L_dx.reshape(-1,1))\n",
    "print()\n",
    "print(\"dTs_L_dx: \\n\", dTs_L_dx.reshape(-1,1))\n",
    "\n",
    "\n",
    "\n",
    "# output_sb_0 = ((pinn.T_hot-pinn.T_0)/(1+torch.exp(-200*(input_sb-0.25)))+pinn.T_0)  # is a tensor with two columns [T_f, T_s], however we will just want to use the first one\n",
    "# output_sb_0\n",
    "# save_expected = output_sb_0[:,0]\n",
    "# save_expected"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this other block we are going to call the function that applies the boundary conditions.\n",
    "\n",
    "Note that the output will be $ [T_f, T_s] $ and the 3 conditions are concatenated.\n",
    "\n",
    "Finally note that the values $ 777 $ are for the vacant conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 9.9974e-01,  7.7700e+02],\n",
       "        [ 3.9996e+00,  7.7700e+02],\n",
       "        [ 4.0005e+00,  7.7700e+02],\n",
       "        [ 3.5974e+00,  7.7700e+02],\n",
       "        [ 4.0002e+00,  7.7700e+02],\n",
       "        [ 4.0000e+00,  7.7700e+02],\n",
       "        [ 3.9999e+00,  7.7700e+02],\n",
       "        [ 1.0007e+00,  7.7700e+02],\n",
       "        [ 9.9922e-01,  7.7700e+02],\n",
       "        [ 4.0003e+00,  7.7700e+02],\n",
       "        [ 7.7700e+02, -3.3490e-04],\n",
       "        [ 7.7700e+02, -3.6231e-04],\n",
       "        [ 7.7700e+02, -5.0767e-04],\n",
       "        [ 7.7700e+02, -3.9553e-02],\n",
       "        [ 7.7700e+02,  2.9802e-04],\n",
       "        [ 7.7700e+02,  7.1945e-04],\n",
       "        [ 7.7700e+02,  2.8501e-04],\n",
       "        [ 7.7700e+02,  2.8912e-04],\n",
       "        [ 7.7700e+02, -1.8246e-04],\n",
       "        [ 7.7700e+02, -4.3291e-04],\n",
       "        [-1.0044e-03, -5.9496e-04],\n",
       "        [ 7.0505e-04,  8.8563e-04],\n",
       "        [ 3.0136e-05, -1.0701e-03],\n",
       "        [-1.6981e-04, -7.2884e-05],\n",
       "        [-4.0656e-04, -1.1547e-04],\n",
       "        [ 1.0697e-03, -4.3657e-04],\n",
       "        [-1.4309e-03,  8.1550e-04],\n",
       "        [-5.9697e-04, -2.0343e-04],\n",
       "        [-3.1179e-04, -3.0730e-05],\n",
       "        [ 9.4765e-04,  8.8216e-04]], grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pinn.apply_boundary_conditions(torch.cat([input_sb_0, input_sb_0, input_sb_L],0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 2])\n",
      "tensor([[  1.0000, 999.0000],\n",
      "        [  4.0000, 999.0000],\n",
      "        [  4.0000, 999.0000],\n",
      "        [  2.5000, 999.0000],\n",
      "        [  4.0000, 999.0000],\n",
      "        [  4.0000, 999.0000],\n",
      "        [  4.0000, 999.0000],\n",
      "        [  1.0000, 999.0000],\n",
      "        [  1.0000, 999.0000],\n",
      "        [  4.0000, 999.0000],\n",
      "        [  4.0000, 999.0000],\n",
      "        [  4.0000, 999.0000],\n",
      "        [  4.0000, 999.0000],\n",
      "        [  4.0000, 999.0000],\n",
      "        [  4.0000, 999.0000],\n",
      "        [  1.0000, 999.0000],\n",
      "        [  1.0000, 999.0000],\n",
      "        [  4.0000, 999.0000],\n",
      "        [  4.0000, 999.0000],\n",
      "        [  4.0000, 999.0000],\n",
      "        [  4.0000, 999.0000],\n",
      "        [  4.0000, 999.0000],\n",
      "        [  4.0000, 999.0000],\n",
      "        [  1.0058, 999.0000],\n",
      "        [  1.0000, 999.0000],\n",
      "        [  4.0000, 999.0000],\n",
      "        [  4.0000, 999.0000],\n",
      "        [  4.0000, 999.0000],\n",
      "        [  3.9942, 999.0000],\n",
      "        [  4.0000, 999.0000],\n",
      "        [  4.0000, 999.0000],\n",
      "        [  1.0000, 999.0000],\n",
      "        [  1.0000, 999.0000],\n",
      "        [  4.0000, 999.0000],\n",
      "        [  4.0000, 999.0000],\n",
      "        [  3.9997, 999.0000],\n",
      "        [  4.0000, 999.0000],\n",
      "        [  4.0000, 999.0000],\n",
      "        [  4.0000, 999.0000],\n",
      "        [  1.0000, 999.0000],\n",
      "        [  1.1263, 999.0000],\n",
      "        [  4.0000, 999.0000],\n",
      "        [  4.0000, 999.0000],\n",
      "        [  4.0000, 999.0000],\n",
      "        [  4.0000, 999.0000],\n",
      "        [  4.0000, 999.0000],\n",
      "        [  4.0000, 999.0000],\n",
      "        [  1.0000, 999.0000],\n",
      "        [  1.0000, 999.0000],\n",
      "        [  4.0000, 999.0000],\n",
      "        [  4.0000, 999.0000],\n",
      "        [  4.0000, 999.0000],\n",
      "        [  4.0000, 999.0000],\n",
      "        [  4.0000, 999.0000],\n",
      "        [  4.0000, 999.0000],\n",
      "        [  1.0003, 999.0000],\n",
      "        [  1.0000, 999.0000],\n",
      "        [  4.0000, 999.0000],\n",
      "        [  4.0000, 999.0000],\n",
      "        [  4.0000, 999.0000],\n",
      "        [  3.8737, 999.0000],\n",
      "        [  4.0000, 999.0000],\n",
      "        [  4.0000, 999.0000],\n",
      "        [  1.0000, 999.0000]])\n"
     ]
    }
   ],
   "source": [
    "# Print the output training_set_sb section 1--> Tf_0\n",
    "for input, output in pinn.training_set_sb:\n",
    "    print(output[:int(output.shape[0]/3), :].shape)\n",
    "    print(output[:int(output.shape[0]/3), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 2])\n",
      "tensor([[999.,   0.],\n",
      "        [999.,   0.],\n",
      "        [999.,   0.],\n",
      "        [999.,   0.],\n",
      "        [999.,   0.],\n",
      "        [999.,   0.],\n",
      "        [999.,   0.],\n",
      "        [999.,   0.],\n",
      "        [999.,   0.],\n",
      "        [999.,   0.],\n",
      "        [999.,   0.],\n",
      "        [999.,   0.],\n",
      "        [999.,   0.],\n",
      "        [999.,   0.],\n",
      "        [999.,   0.],\n",
      "        [999.,   0.],\n",
      "        [999.,   0.],\n",
      "        [999.,   0.],\n",
      "        [999.,   0.],\n",
      "        [999.,   0.],\n",
      "        [999.,   0.],\n",
      "        [999.,   0.],\n",
      "        [999.,   0.],\n",
      "        [999.,   0.],\n",
      "        [999.,   0.],\n",
      "        [999.,   0.],\n",
      "        [999.,   0.],\n",
      "        [999.,   0.],\n",
      "        [999.,   0.],\n",
      "        [999.,   0.],\n",
      "        [999.,   0.],\n",
      "        [999.,   0.],\n",
      "        [999.,   0.],\n",
      "        [999.,   0.],\n",
      "        [999.,   0.],\n",
      "        [999.,   0.],\n",
      "        [999.,   0.],\n",
      "        [999.,   0.],\n",
      "        [999.,   0.],\n",
      "        [999.,   0.],\n",
      "        [999.,   0.],\n",
      "        [999.,   0.],\n",
      "        [999.,   0.],\n",
      "        [999.,   0.],\n",
      "        [999.,   0.],\n",
      "        [999.,   0.],\n",
      "        [999.,   0.],\n",
      "        [999.,   0.],\n",
      "        [999.,   0.],\n",
      "        [999.,   0.],\n",
      "        [999.,   0.],\n",
      "        [999.,   0.],\n",
      "        [999.,   0.],\n",
      "        [999.,   0.],\n",
      "        [999.,   0.],\n",
      "        [999.,   0.],\n",
      "        [999.,   0.],\n",
      "        [999.,   0.],\n",
      "        [999.,   0.],\n",
      "        [999.,   0.],\n",
      "        [999.,   0.],\n",
      "        [999.,   0.],\n",
      "        [999.,   0.],\n",
      "        [999.,   0.]])\n"
     ]
    }
   ],
   "source": [
    "# Print the output training_set_sb section 2--> Ts_0_dx\n",
    "for input, output in pinn.training_set_sb:\n",
    "    print(output[int(output.shape[0]/3):int(2*output.shape[0]/3), :].shape)\n",
    "    print(output[int(output.shape[0]/3):int(2*output.shape[0]/3), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 2])\n",
      "tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# Print the output training_set_sb section 3--> Tf/s_0_dx\n",
    "for input, output in pinn.training_set_sb:\n",
    "    print(output[int(2*output.shape[0]/3):, :].shape)\n",
    "    print(output[int(2*output.shape[0]/3):, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  1.0000, 999.0000],\n",
      "        [  4.0000, 999.0000],\n",
      "        [  4.0000, 999.0000],\n",
      "        [  2.5000, 999.0000],\n",
      "        [  4.0000, 999.0000],\n",
      "        [  4.0000, 999.0000],\n",
      "        [  4.0000, 999.0000],\n",
      "        [  1.0000, 999.0000],\n",
      "        [  1.0000, 999.0000],\n",
      "        [  4.0000, 999.0000],\n",
      "        [  4.0000, 999.0000],\n",
      "        [  4.0000, 999.0000],\n",
      "        [  4.0000, 999.0000],\n",
      "        [  4.0000, 999.0000],\n",
      "        [  4.0000, 999.0000],\n",
      "        [  1.0000, 999.0000],\n",
      "        [  1.0000, 999.0000],\n",
      "        [  4.0000, 999.0000],\n",
      "        [  4.0000, 999.0000],\n",
      "        [  4.0000, 999.0000],\n",
      "        [  4.0000, 999.0000],\n",
      "        [  4.0000, 999.0000],\n",
      "        [  4.0000, 999.0000],\n",
      "        [  1.0058, 999.0000],\n",
      "        [  1.0000, 999.0000],\n",
      "        [  4.0000, 999.0000],\n",
      "        [  4.0000, 999.0000],\n",
      "        [  4.0000, 999.0000],\n",
      "        [  3.9942, 999.0000],\n",
      "        [  4.0000, 999.0000],\n",
      "        [  4.0000, 999.0000],\n",
      "        [  1.0000, 999.0000],\n",
      "        [  1.0000, 999.0000],\n",
      "        [  4.0000, 999.0000],\n",
      "        [  4.0000, 999.0000],\n",
      "        [  3.9997, 999.0000],\n",
      "        [  4.0000, 999.0000],\n",
      "        [  4.0000, 999.0000],\n",
      "        [  4.0000, 999.0000],\n",
      "        [  1.0000, 999.0000],\n",
      "        [  1.1263, 999.0000],\n",
      "        [  4.0000, 999.0000],\n",
      "        [  4.0000, 999.0000],\n",
      "        [  4.0000, 999.0000],\n",
      "        [  4.0000, 999.0000],\n",
      "        [  4.0000, 999.0000],\n",
      "        [  4.0000, 999.0000],\n",
      "        [  1.0000, 999.0000],\n",
      "        [  1.0000, 999.0000],\n",
      "        [  4.0000, 999.0000],\n",
      "        [  4.0000, 999.0000],\n",
      "        [  4.0000, 999.0000],\n",
      "        [  4.0000, 999.0000],\n",
      "        [  4.0000, 999.0000],\n",
      "        [  4.0000, 999.0000],\n",
      "        [  1.0003, 999.0000],\n",
      "        [  1.0000, 999.0000],\n",
      "        [  4.0000, 999.0000],\n",
      "        [  4.0000, 999.0000],\n",
      "        [  4.0000, 999.0000],\n",
      "        [  3.8737, 999.0000],\n",
      "        [  4.0000, 999.0000],\n",
      "        [  4.0000, 999.0000],\n",
      "        [  1.0000, 999.0000],\n",
      "        [999.0000,   0.0000],\n",
      "        [999.0000,   0.0000],\n",
      "        [999.0000,   0.0000],\n",
      "        [999.0000,   0.0000],\n",
      "        [999.0000,   0.0000],\n",
      "        [999.0000,   0.0000],\n",
      "        [999.0000,   0.0000],\n",
      "        [999.0000,   0.0000],\n",
      "        [999.0000,   0.0000],\n",
      "        [999.0000,   0.0000],\n",
      "        [999.0000,   0.0000],\n",
      "        [999.0000,   0.0000],\n",
      "        [999.0000,   0.0000],\n",
      "        [999.0000,   0.0000],\n",
      "        [999.0000,   0.0000],\n",
      "        [999.0000,   0.0000],\n",
      "        [999.0000,   0.0000],\n",
      "        [999.0000,   0.0000],\n",
      "        [999.0000,   0.0000],\n",
      "        [999.0000,   0.0000],\n",
      "        [999.0000,   0.0000],\n",
      "        [999.0000,   0.0000],\n",
      "        [999.0000,   0.0000],\n",
      "        [999.0000,   0.0000],\n",
      "        [999.0000,   0.0000],\n",
      "        [999.0000,   0.0000],\n",
      "        [999.0000,   0.0000],\n",
      "        [999.0000,   0.0000],\n",
      "        [999.0000,   0.0000],\n",
      "        [999.0000,   0.0000],\n",
      "        [999.0000,   0.0000],\n",
      "        [999.0000,   0.0000],\n",
      "        [999.0000,   0.0000],\n",
      "        [999.0000,   0.0000],\n",
      "        [999.0000,   0.0000],\n",
      "        [999.0000,   0.0000],\n",
      "        [999.0000,   0.0000],\n",
      "        [999.0000,   0.0000],\n",
      "        [999.0000,   0.0000],\n",
      "        [999.0000,   0.0000],\n",
      "        [999.0000,   0.0000],\n",
      "        [999.0000,   0.0000],\n",
      "        [999.0000,   0.0000],\n",
      "        [999.0000,   0.0000],\n",
      "        [999.0000,   0.0000],\n",
      "        [999.0000,   0.0000],\n",
      "        [999.0000,   0.0000],\n",
      "        [999.0000,   0.0000],\n",
      "        [999.0000,   0.0000],\n",
      "        [999.0000,   0.0000],\n",
      "        [999.0000,   0.0000],\n",
      "        [999.0000,   0.0000],\n",
      "        [999.0000,   0.0000],\n",
      "        [999.0000,   0.0000],\n",
      "        [999.0000,   0.0000],\n",
      "        [999.0000,   0.0000],\n",
      "        [999.0000,   0.0000],\n",
      "        [999.0000,   0.0000],\n",
      "        [999.0000,   0.0000],\n",
      "        [999.0000,   0.0000],\n",
      "        [999.0000,   0.0000],\n",
      "        [999.0000,   0.0000],\n",
      "        [999.0000,   0.0000],\n",
      "        [999.0000,   0.0000],\n",
      "        [  0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000]])\n"
     ]
    }
   ],
   "source": [
    "for input, output in pinn.training_set_sb:\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MasterThesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
