{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x15c01f150>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from Common import NeuralNet\n",
    "import time\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "torch.manual_seed(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pinns:\n",
    "    def __init__(self, n_int_, n_sb_, n_tb_, alpha_f_=0.005, h_f_=5, T_hot_=4, T_0_=1, U_f_=None):\n",
    "        self.n_int = n_int_     # n_int_:= number of intertior points\n",
    "        self.n_sb = n_sb_       # n_sb_ := number of spatial boundary points\n",
    "        self.n_tb = n_tb_       # n_tb_ := number of time boundary points\n",
    "\n",
    "        # Set the paremeters of the equation\n",
    "        self.alpha_f = alpha_f_\n",
    "        self.h_f = h_f_\n",
    "        self.T_hot = T_hot_\n",
    "        self.T_0 = T_0_\n",
    "        self.U_f = U_f_\n",
    "\n",
    "        # Extrema of the solution domain (t,x) in [0, t]x[0, L]\n",
    "        self.domain_extrema = torch.tensor([[0, 8],  # Time dimension\n",
    "                                            [0, 1]])  # Space dimension\n",
    "\n",
    "        # Number of space dimensions\n",
    "        self.space_dimensions = 1\n",
    "\n",
    "        # Parameter to balance role of data and PDE\n",
    "        self.lambda_u = 10\n",
    "\n",
    "        # FF Dense NN to approximate the solution of the underlying reaction-convection-diffusion equations of the fluid\n",
    "        self.approximate_solution = NeuralNet(input_dimension=self.domain_extrema.shape[0], output_dimension=1, # is a NN with input_dim=2 (time & space), output_dim=1 (fluid_temp)\n",
    "                                              n_hidden_layers=4,\n",
    "                                              neurons=20,\n",
    "                                              regularization_param=0.,\n",
    "                                              regularization_exp=2.,\n",
    "                                              retrain_seed=42)\n",
    "        \n",
    "        # FF Dense NN to approximate the solid temperature we wish to infer\n",
    "        self.approximate_coefficient = NeuralNet(input_dimension=self.domain_extrema.shape[0], output_dimension=1,  # is a NN with input_dim=2 (time & space), output_dim=1 (solid_temp)\n",
    "                                                 n_hidden_layers=4,\n",
    "                                                 neurons=20,\n",
    "                                                 regularization_param=0.,\n",
    "                                                 regularization_exp=2.,\n",
    "                                                 retrain_seed=42)\n",
    "\n",
    "        # Generator of Sobol sequences\n",
    "        self.soboleng = torch.quasirandom.SobolEngine(dimension=self.domain_extrema.shape[0])   # it will create a 2 cloumns tensor, the rows nunmber is specified after every time it is used\n",
    "\n",
    "        # Training sets S_sb, S_tb, S_int as torch dataloader\n",
    "        self.training_set_sb, self.training_set_tb, self.training_set_int, self.training_set_meas = self.assemble_datasets()\n",
    "\n",
    "    ################################################################################################\n",
    "    # Function to linearly transform a tensor whose value are between 0 and 1\n",
    "    # to a tensor whose values are between the domain extrema\n",
    "    def convert(self, tens):\n",
    "        assert (tens.shape[1] == self.domain_extrema.shape[0])\n",
    "        return tens * (self.domain_extrema[:, 1] - self.domain_extrema[:, 0]) + self.domain_extrema[:, 0]\n",
    "    \n",
    "    # Function Uf(t) -> given the time it gives back the velocity of the fluid in the relative phase\n",
    "    def fluid_velocity(self, inputs):\n",
    "        for t in inputs:\n",
    "\n",
    "            # Charging Phase\n",
    "            if (t <= 1) or (t > 4 & t <=5 ): Uf = 1\n",
    "\n",
    "            # Discharging Phase\n",
    "            elif (t > 2 & t <= 3) or (t > 6 & t <= 7): Uf = -1\n",
    "\n",
    "            # Idle Phase\n",
    "            elif (t > 1 & t <= 2) or (t > 3 & t <= 4) or (t > 5 & t <= 6) or (t > 7 & t <= 8): Uf = 0\n",
    "\n",
    "        return Uf\n",
    "    \n",
    "    ################################################################################################\n",
    "    # Function returning the input-output tensor required to assemble the training set S_tb corresponding to the temporal boundary\n",
    "    def add_temporal_boundary_points(self):\n",
    "        t0 = self.domain_extrema[0, 0]\n",
    "        input_tb = self.soboleng.draw(self.n_tb)    # input_sb has two columns (t, x) both with random numbers in the two respective domains\n",
    "        input_tb[:, 0] = torch.full(input_tb[:, 0].shape, t0)   # overwrite the entier column of time with t0\n",
    "        output_tb = torch.full(input_tb[:, 0].shape, self.T_0)    # the output has 1 column\n",
    "\n",
    "        return input_tb, output_tb  # input_tb is the sequence of x_n; output_tb is the sequence u0(x_n)\n",
    "\n",
    "    # Function returning the input-output tensor required to assemble the training set S_sb corresponding to the spatial boundary\n",
    "    def add_spatial_boundary_points(self):\n",
    "        x0 = self.domain_extrema[1, 0]\n",
    "        xL = self.domain_extrema[1, 1]\n",
    "\n",
    "        # Dataset with random [t, x] both in [0, 1]\n",
    "        input_sb = self.soboleng.draw(self.n_sb)\n",
    "        \n",
    "        # requires the grad for these tensors as we will have to compute the derivatives\n",
    "        # just need to require it for this since using torch.clone will imply that also the new tensor will have it\n",
    "        input_sb.requires_grad_()\n",
    "\n",
    "        # Assigne the spacial boundary x=x0\n",
    "        input_sb_0 = torch.clone(input_sb)\n",
    "        input_sb_0[:, 1] = torch.full(input_sb_0[:, 1].shape, x0)\n",
    "\n",
    "        # Assigne the spacial boundary x=xL\n",
    "        input_sb_L = torch.clone(input_sb)\n",
    "        input_sb_L[:, 1] = torch.full(input_sb_L[:, 1].shape, xL)\n",
    "\n",
    "        # Def a tensor to add the delta in time to each of the input dataset and have the different phases\n",
    "        # This tenosr is full of [0, 0] and it will be filled on the t-column with the relative delta_t\n",
    "        delta_time = torch.zeros_like(input_sb)\n",
    "\n",
    "        # We are going now to define the input dataset for all the different phases over the 2 cycles.\n",
    "        # However, not that even if the input for the same phase over the 2 cycles is different (the t)\n",
    "        #Â the output is always the same (the spatial boundary conditions are the same), so we just need one output per phase.\n",
    "        \"\"\"Charging Phase\"\"\"\n",
    "        # First charging phase -> t in [0, 1] => delta_t=0\n",
    "        input_sb_0_charging_1 = torch.clone(input_sb_0)\n",
    "        input_sb_L_charging_1 = torch.clone(input_sb_L)\n",
    "        \n",
    "        # Second charging phase -> t in [4, 5] => delta_t=4\n",
    "        delta_time[:, 0] = torch.full(delta_time[:, 0].shape, 4)\n",
    "\n",
    "        input_sb_0_charging_2 = torch.clone(input_sb_0)\n",
    "        input_sb_0_charging_2 = input_sb_0_charging_2 + delta_time\n",
    "\n",
    "        input_sb_L_charging_2 = torch.clone(input_sb_L)\n",
    "        input_sb_L_charging_2 = input_sb_L_charging_2 + delta_time\n",
    "\n",
    "        # Output charging phase\n",
    "        output_sb_0_charging = torch.full(input_sb_0[:, 0].shape, self.T_hot)\n",
    "        output_sb_L_charging = torch.full(input_sb_L[:, 0].shape, 0)\n",
    "\n",
    "        \"\"\"Discharging Phase\"\"\"\n",
    "        # First discharging phase -> t in [2, 3] => delta_t=2\n",
    "        delta_time[:, 0] = torch.full(delta_time[:, 0].shape, 2)\n",
    "\n",
    "        input_sb_0_discharging_1 = torch.clone(input_sb_0)\n",
    "        input_sb_0_discharging_1 = input_sb_0_discharging_1 + delta_time\n",
    "\n",
    "        input_sb_L_discharging_1 = torch.clone(input_sb_L)\n",
    "        input_sb_L_discharging_1 = input_sb_L_discharging_1 + delta_time\n",
    "\n",
    "        # Second discharging phase -> t in [6, 7] => delta_t=6\n",
    "        delta_time[:, 0] = torch.full(delta_time[:, 0].shape, 6)\n",
    "\n",
    "        input_sb_0_discharging_2 = torch.clone(input_sb_0)\n",
    "        input_sb_0_discharging_2 = input_sb_0_discharging_2 + delta_time\n",
    "\n",
    "        input_sb_L_discharging_2 = torch.clone(input_sb_L)\n",
    "        input_sb_L_discharging_2 = input_sb_L_discharging_2 + delta_time\n",
    "\n",
    "        # Output discharging phase\n",
    "        output_sb_0_discharging = torch.full(input_sb_0[:, 0].shape, 0)\n",
    "        output_sb_L_discharging = torch.full(input_sb_L[:, 0].shape, self.T_0)\n",
    "\n",
    "\n",
    "        \"\"\"Idle Phase\"\"\"\n",
    "        # First idle phase -> t in [1, 2] => delta_t=1\n",
    "        delta_time[:, 0] = torch.full(delta_time[:, 0].shape, 1)\n",
    "\n",
    "        input_sb_0_idle_1 = torch.clone(input_sb_0)\n",
    "        input_sb_0_idle_1 = input_sb_0_idle_1 + delta_time\n",
    "\n",
    "        input_sb_L_idle_1 = torch.clone(input_sb_L)\n",
    "        input_sb_L_idle_1 = input_sb_L_idle_1 + delta_time\n",
    "\n",
    "        # Second idle phase -> t in [3, 4] => delta_t=3\n",
    "        delta_time[:, 0] = torch.full(delta_time[:, 0].shape, 3)\n",
    "\n",
    "        input_sb_0_idle_2 = torch.clone(input_sb_0)\n",
    "        input_sb_0_idle_2 = input_sb_0_idle_2 + delta_time\n",
    "\n",
    "        input_sb_L_idle_2 = torch.clone(input_sb_L)\n",
    "        input_sb_L_idle_2 = input_sb_L_idle_2 + delta_time\n",
    "\n",
    "        # Third idle phase -> t in [5, 6] => delta_t=5\n",
    "        delta_time[:, 0] = torch.full(delta_time[:, 0].shape, 5)\n",
    "\n",
    "        input_sb_0_idle_3 = torch.clone(input_sb_0)\n",
    "        input_sb_0_idle_3 = input_sb_0_idle_3 + delta_time\n",
    "\n",
    "        input_sb_L_idle_3 = torch.clone(input_sb_L)\n",
    "        input_sb_L_idle_3 = input_sb_L_idle_3 + delta_time\n",
    "\n",
    "        # Fourth idle phase -> t in [7, 8] => delta_t=7\n",
    "        delta_time[:, 0] = torch.full(delta_time[:, 0].shape, 7)\n",
    "\n",
    "        input_sb_0_idle_4 = torch.clone(input_sb_0)\n",
    "        input_sb_0_idle_4 = input_sb_0_idle_4 + delta_time\n",
    "\n",
    "        input_sb_L_idle_4 = torch.clone(input_sb_L)\n",
    "        input_sb_L_idle_4 = input_sb_L_idle_4 + delta_time\n",
    "\n",
    "        # Output idle phase\n",
    "        output_sb_0_idle = torch.full(input_sb_0[:, 0].shape, 0)\n",
    "        output_sb_L_idle = torch.full(input_sb_L[:, 0].shape, 0)\n",
    "\n",
    "        return torch.cat([  # cycle 1\n",
    "                        input_sb_0_charging_1, input_sb_L_charging_1,\n",
    "                        input_sb_0_idle_1, input_sb_L_idle_1,\n",
    "                        input_sb_0_discharging_1, input_sb_L_discharging_1,\n",
    "                        input_sb_0_idle_2, input_sb_L_idle_2,\n",
    "                            # cycle 2\n",
    "                        input_sb_0_charging_2, input_sb_L_charging_2,\n",
    "                        input_sb_0_idle_3, input_sb_L_idle_3,\n",
    "                        input_sb_0_discharging_2, input_sb_L_discharging_2,\n",
    "                        input_sb_0_idle_4, input_sb_L_idle_4\n",
    "                        ], 0), torch.cat([  # cycle 1\n",
    "                                        output_sb_0_charging, output_sb_L_charging,\n",
    "                                        output_sb_0_idle, output_sb_L_idle,\n",
    "                                        output_sb_0_discharging, output_sb_L_discharging,\n",
    "                                        output_sb_0_idle, output_sb_L_idle,\n",
    "                                            # cycle 2\n",
    "                                        output_sb_0_charging, output_sb_L_charging,\n",
    "                                        output_sb_0_idle, output_sb_L_idle,\n",
    "                                        output_sb_0_discharging, output_sb_L_discharging,\n",
    "                                        output_sb_0_idle, output_sb_L_idle\n",
    "                                        ], 0)\n",
    "\n",
    "    # Function returning the input-output tensor required to assemble the training set S_int corresponding to the interior domain where the PDE is enforced\n",
    "    def add_interior_points(self):\n",
    "        # Now we use the convert fct since we want the t is in [0, 8]\n",
    "        input_int = self.convert(self.soboleng.draw(self.n_int))\n",
    "        output_int = torch.zeros((input_int.shape[0], 1))\n",
    "        \n",
    "        return input_int, output_int\n",
    "    \n",
    "    # Function returning the input-output tensor required to assemble the training set S_meas corresponding to the measured points in the domain.\n",
    "    # These points are read from the file \"DataSolution.txt\"\n",
    "    def get_measurement_data(self):\n",
    "        # Read the CSV file into a DataFrame\n",
    "        df_meas = pd.read_csv('DataSolution.txt')\n",
    "\n",
    "        # Convert the DataFrame to a torch.tensor\n",
    "        tensor_meas = torch.tensor(df_meas.values)\n",
    "        \n",
    "        # The first 2 columns are the inputs: [t, x]\n",
    "        input_meas = tensor_meas[:, :2]\n",
    "        \n",
    "        # The last column are the outputs: [Tf]\n",
    "        output_meas = tensor_meas[:, 2:]\n",
    "        return input_meas, output_meas\n",
    "\n",
    "    # Function returning the training sets S_sb, S_tb, S_int as dataloader\n",
    "    def assemble_datasets(self):\n",
    "        input_sb, output_sb = self.add_spatial_boundary_points()    # S_sb\n",
    "        input_tb, output_tb = self.add_temporal_boundary_points()   # S_tb\n",
    "        input_int, output_int = self.add_interior_points()          # S_int\n",
    "        input_meas, output_meas = self.get_measurement_data()       # S_meas\n",
    "\n",
    "\n",
    "        training_set_sb = DataLoader(torch.utils.data.TensorDataset(input_sb, output_sb), batch_size=16*self.space_dimensions*self.n_sb, shuffle=False)  #batch_size has *8 since there are 8 different phases and for each one we have 2 conditions (x0 & xL)\n",
    "        training_set_tb = DataLoader(torch.utils.data.TensorDataset(input_tb, output_tb), batch_size=self.n_tb, shuffle=False)\n",
    "        training_set_int = DataLoader(torch.utils.data.TensorDataset(input_int, output_int), batch_size=self.n_int, shuffle=False)\n",
    "        training_set_meas = DataLoader(torch.utils.data.TensorDataset(input_meas, output_meas), batch_size=output_meas.shape[0], shuffle=False)\n",
    "\n",
    "        return training_set_sb, training_set_tb, training_set_int, training_set_meas\n",
    "\n",
    "    ################################################################################################\n",
    "    # Function to compute the terms required in the definition of the TEMPORAL boundary residual\n",
    "    def apply_initial_condition(self, input_tb):\n",
    "        u_pred_tb = self.approximate_solution(input_tb)\n",
    "        \n",
    "        return u_pred_tb\n",
    "\n",
    "    # Function to compute the terms required in the definition of the SPATIAL boundary residual\n",
    "    def apply_boundary_conditions(self, input_sb):\n",
    "        # input_tb is a tensor of size [16*self.n_sb, 2]\n",
    "        # as defined in \"add_spatial_boundary_points\" we have 2 boundary conditions for each phase\n",
    "        # we then have to devide the input_sb in 16\n",
    "        assert (input_sb.requires_grad==True)   # make sure the grad is requested so we can compute the derivatives\n",
    "\n",
    "        # Devide all the input datasets\n",
    "            # cycle 1\n",
    "        input_sb_0_charging_1 = input_sb[:int(input_sb.shape[0]/16), :]\n",
    "        input_sb_L_charging_1 = input_sb[int(input_sb.shape[0]/16):2*int(input_sb.shape[0]/16), :]\n",
    "        input_sb_0_idle_1 = input_sb[2*int(input_sb.shape[0]/16):3*int(input_sb.shape[0]/16), :]\n",
    "        input_sb_L_idle_1 = input_sb[3*int(input_sb.shape[0]/16):4*int(input_sb.shape[0]/16), :]\n",
    "        input_sb_0_discharging_1 = input_sb[4*int(input_sb.shape[0]/16):5*int(input_sb.shape[0]/16), :]\n",
    "        input_sb_L_discharging_1 = input_sb[5*int(input_sb.shape[0]/16):6*int(input_sb.shape[0]/16), :]\n",
    "        input_sb_0_idle_2 = input_sb[6*int(input_sb.shape[0]/16):7*int(input_sb.shape[0]/16), :]\n",
    "        input_sb_L_idle_2 = input_sb[7*int(input_sb.shape[0]/16):8*int(input_sb.shape[0]/16), :]\n",
    "            # cycle 2\n",
    "        input_sb_0_charging_2 = input_sb[8*int(input_sb.shape[0]/16):9*int(input_sb.shape[0]/16), :]\n",
    "        input_sb_L_charging_2 = input_sb[9*int(input_sb.shape[0]/16):10*int(input_sb.shape[0]/16), :]\n",
    "        input_sb_0_idle_3 = input_sb[10*int(input_sb.shape[0]/16):11*int(input_sb.shape[0]/16), :]\n",
    "        input_sb_L_idle_3 = input_sb[11*int(input_sb.shape[0]/16):12*int(input_sb.shape[0]/16), :]\n",
    "        input_sb_0_discharging_2 = input_sb[12*int(input_sb.shape[0]/16):13*int(input_sb.shape[0]/16), :]\n",
    "        input_sb_L_discharging_2 = input_sb[13*int(input_sb.shape[0]/16):14*int(input_sb.shape[0]/16), :]\n",
    "        input_sb_0_idle_4 = input_sb[14*int(input_sb.shape[0]/16):15*int(input_sb.shape[0]/16), :]\n",
    "        input_sb_L_idle_4 = input_sb[15*int(input_sb.shape[0]/16):, :]\n",
    "\n",
    "        \"\"\"Charging Phase\"\"\"\n",
    "        # First charging phase\n",
    "            # x0 -> compute Tf\n",
    "        u_pred_sb_0_charging_1 = self.approximate_solution(input_sb_0_charging_1)\n",
    "\n",
    "            # xL -> compute dTf/dx\n",
    "        u_pred_Tf = self.approximate_solution(input_sb_L_charging_1)\n",
    "        u_pred_sb_L_charging_1 = torch.autograd.grad(u_pred_Tf.sum(), input_sb_L_charging_1, create_graph=True)[0][:, 1] # take only d/dx\n",
    "\n",
    "        # Second charging phase\n",
    "            # x0 -> compute Tf\n",
    "        u_pred_sb_0_charging_2 = self.approximate_solution(input_sb_0_charging_2)\n",
    "\n",
    "            # xL -> compute dTf/dx\n",
    "        u_pred_Tf = self.approximate_solution(input_sb_L_charging_2)\n",
    "        u_pred_sb_L_charging_2 = torch.autograd.grad(u_pred_Tf.sum(), input_sb_L_charging_2, create_graph=True)[0][:, 1] # take only d/dx\n",
    "\n",
    "        \"\"\"Discharging Phase\"\"\"\n",
    "        # First discharging phase\n",
    "            # x0 -> compute dTf/dx\n",
    "        u_pred_Tf = self.approximate_solution(input_sb_0_discharging_1)\n",
    "        u_pred_sb_0_discharging_1 = torch.autograd.grad(u_pred_Tf.sum(), input_sb_0_discharging_1, create_graph=True)[0][:, 1] # take only d/dx\n",
    "        \n",
    "            # xL -> compute Tf\n",
    "        u_pred_sb_L_discharging_1 = self.approximate_solution(input_sb_L_discharging_1)\n",
    "\n",
    "        # Second discharging phase\n",
    "            # x0 -> compute dTf/dx\n",
    "        u_pred_Tf = self.approximate_solution(input_sb_0_discharging_2)\n",
    "        u_pred_sb_0_discharging_2 = torch.autograd.grad(u_pred_Tf.sum(), input_sb_0_discharging_2, create_graph=True)[0][:, 1] # take only d/dx\n",
    "        \n",
    "            # xL -> compute Tf\n",
    "        u_pred_sb_L_discharging_2 = self.approximate_solution(input_sb_L_discharging_2)\n",
    "\n",
    "        \"\"\"Idle Phase\"\"\"\n",
    "        # First idle phase\n",
    "            # x0 -> compute dTf/dx\n",
    "        u_pred_Tf = self.approximate_solution(input_sb_0_idle_1)\n",
    "        u_pred_sb_0_idle_1 = torch.autograd.grad(u_pred_Tf.sum(), input_sb_0_idle_1, create_graph=True)[0][:, 1] # take only d/dx\n",
    "\n",
    "            # xL -> compute dTf/dx\n",
    "        u_pred_Tf = self.approximate_solution(input_sb_L_idle_1)\n",
    "        u_pred_sb_L_idle_1 = torch.autograd.grad(u_pred_Tf.sum(), input_sb_L_idle_1, create_graph=True)[0][:, 1] # take only d/dx\n",
    "\n",
    "        # Second idle phase\n",
    "            # x0 -> compute dTf/dx\n",
    "        u_pred_Tf = self.approximate_solution(input_sb_0_idle_2)\n",
    "        u_pred_sb_0_idle_2 = torch.autograd.grad(u_pred_Tf.sum(), input_sb_0_idle_2, create_graph=True)[0][:, 1] # take only d/dx\n",
    "\n",
    "            # xL -> compute dTf/dx\n",
    "        u_pred_Tf = self.approximate_solution(input_sb_L_idle_2)\n",
    "        u_pred_sb_L_idle_2 = torch.autograd.grad(u_pred_Tf.sum(), input_sb_L_idle_2, create_graph=True)[0][:, 1] # take only d/dx\n",
    "\n",
    "        # Third idle phase\n",
    "            # x0 -> compute dTf/dx\n",
    "        u_pred_Tf = self.approximate_solution(input_sb_0_idle_3)\n",
    "        u_pred_sb_0_idle_3 = torch.autograd.grad(u_pred_Tf.sum(), input_sb_0_idle_3, create_graph=True)[0][:, 1] # take only d/dx\n",
    "\n",
    "            # xL -> compute dTf/dx\n",
    "        u_pred_Tf = self.approximate_solution(input_sb_L_idle_3)\n",
    "        u_pred_sb_L_idle_3 = torch.autograd.grad(u_pred_Tf.sum(), input_sb_L_idle_3, create_graph=True)[0][:, 1] # take only d/dx\n",
    "\n",
    "        # Fourth idle phase\n",
    "            # x0 -> compute dTf/dx\n",
    "        u_pred_Tf = self.approximate_solution(input_sb_0_idle_4)\n",
    "        u_pred_sb_0_idle_4 = torch.autograd.grad(u_pred_Tf.sum(), input_sb_0_idle_4, create_graph=True)[0][:, 1] # take only d/dx\n",
    "\n",
    "            # xL -> compute dTf/dx\n",
    "        u_pred_Tf = self.approximate_solution(input_sb_L_idle_3)\n",
    "        u_pred_sb_L_idle_4 = torch.autograd.grad(u_pred_Tf.sum(), input_sb_L_idle_4, create_graph=True)[0][:, 1] # take only d/dx\n",
    "\n",
    "        return torch.cat([  # cycle 1\n",
    "                        u_pred_sb_0_charging_1, u_pred_sb_L_charging_1,\n",
    "                        u_pred_sb_0_idle_1, u_pred_sb_L_idle_1,\n",
    "                        u_pred_sb_0_discharging_1, u_pred_sb_L_discharging_1,\n",
    "                        u_pred_sb_0_idle_2, u_pred_sb_L_idle_2,\n",
    "                            # cycle 2\n",
    "                        u_pred_sb_0_charging_2, u_pred_sb_L_charging_2,\n",
    "                        u_pred_sb_0_idle_3, u_pred_sb_L_idle_3,\n",
    "                        u_pred_sb_0_discharging_2, u_pred_sb_L_discharging_2,\n",
    "                        u_pred_sb_0_idle_4, u_pred_sb_L_idle_4\n",
    "                        ], 0)\n",
    "\n",
    "    # Function to compute the PDE residuals\n",
    "    def compute_pde_residual(self, input_int):\n",
    "        input_int.requires_grad = True\n",
    "        u = self.approximate_solution(input_int).reshape(-1,)       # u is the solution (Tf) of the PDE\n",
    "        g = self.approximate_coefficient(input_int).reshape(-1,)    # g is the function (Ts) that is requested\n",
    "\n",
    "        # grad compute the gradient of a \"SCALAR\" function L with respect to some input nxm TENSOR Z=[[x1, y1],[x2,y2],[x3,y3],...,[xn,yn]], m=2\n",
    "        # it returns grad_L = [[dL/dx1, dL/dy1],[dL/dx2, dL/dy2],[dL/dx3, dL/dy3],...,[dL/dxn, dL/dyn]]\n",
    "        # Note: pytorch considers a tensor [u1, u2,u3, ... ,un] a vectorial function\n",
    "        # whereas sum_u = u1 + u2 + u3 + u4 + ... + un as a \"scalar\" one\n",
    "\n",
    "        # In our case ui = u(xi), therefore the line below returns:\n",
    "        # grad_u = [[dsum_u/dx1, dsum_u/dy1],[dsum_u/dx2, dsum_u/dy2],[dsum_u/dx3, dL/dy3],...,[dsum_u/dxm, dsum_u/dyn]]\n",
    "        # and dsum_u/dxi = d(u1 + u2 + u3 + u4 + ... + un)/dxi = d(u(x1) + u(x2) u3(x3) + u4(x4) + ... + u(xn))/dxi = dui/dxi\n",
    "\n",
    "        # Since u for us is u = (uf, us), we have to devide the two cases\n",
    "\n",
    "        grad_u = torch.autograd.grad(u.sum(), input_int, create_graph=True)[0]\n",
    "        grad_u_t = grad_u[:, 0]\n",
    "        grad_u_x = grad_u[:, 1]\n",
    "        grad_u_xx = torch.autograd.grad(grad_u_x.sum(), input_int, create_graph=True)[0][:, 1]\n",
    "\n",
    "        # Compute the velocity of the fluid Uf(t)\n",
    "        Uf = self.fluid_velocity(input_int[:, 0])\n",
    "\n",
    "        residual = (grad_u_t + Uf*grad_u_x) - (self.alpha_f*grad_u_xx - self.h_f*(u-g))\n",
    "\n",
    "        return residual.reshape(-1, )\n",
    "\n",
    "    # Function to compute the total loss (weighted sum of spatial boundary loss, temporal boundary loss and interior loss)\n",
    "    def compute_loss(self, inp_train_sb, u_train_sb, inp_train_tb, u_train_tb, inp_train_int, inp_train_meas, u_train_meas, verbose=True):\n",
    "        u_pred_sb = self.apply_boundary_conditions(inp_train_sb)\n",
    "        u_pred_tb = self.apply_initial_condition(inp_train_tb)\n",
    "        u_pred_meas = self.approximate_solution(inp_train_meas)\n",
    "\n",
    "        assert (u_pred_sb.shape[1] == u_train_sb.shape[1])\n",
    "        assert (u_pred_tb.shape[1] == u_train_tb.shape[1])\n",
    "        assert (u_pred_meas.shape[1] == u_train_meas.shape[1])\n",
    "\n",
    "\n",
    "        r_int = self.compute_pde_residual(inp_train_int)\n",
    "        r_sb = u_train_sb - u_pred_sb\n",
    "        r_tb = u_train_tb - u_pred_tb\n",
    "        r_meas = u_train_meas - u_pred_meas\n",
    "\n",
    "        loss_sb = torch.mean(abs(r_sb) ** 2)\n",
    "        loss_tb = torch.mean(abs(r_tb) ** 2)\n",
    "        loss_int = torch.mean(abs(r_int) ** 2)\n",
    "        loss_meas = torch.mean(abs(r_meas) ** 2)\n",
    "\n",
    "        loss_u = loss_sb + loss_tb + loss_meas\n",
    "\n",
    "        loss = torch.log10(self.lambda_u * loss_u + loss_int)\n",
    "        if verbose: print(\"Total loss: \", round(loss.item(), 4), \"| PDE Loss: \", round(torch.log10(loss_int).item(), 4), \"| Function Loss: \", round(torch.log10(loss_u).item(), 4))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    ################################################################################################\n",
    "    def fit(self, num_epochs, optimizer, verbose=True):\n",
    "        history = list()\n",
    "\n",
    "        # Loop over epochs\n",
    "        for epoch in range(num_epochs):\n",
    "            if verbose: print(\"################################ \", epoch, \" ################################\")\n",
    "\n",
    "            for j, ((inp_train_sb, u_train_sb), (inp_train_tb, u_train_tb), (inp_train_int, u_train_int), (inp_train_meas, u_train_meas)) in enumerate(zip(self.training_set_sb, self.training_set_tb, self.training_set_int, self.training_set_meas)):\n",
    "                def closure():\n",
    "                    optimizer.zero_grad()\n",
    "                    loss = self.compute_loss(inp_train_sb, u_train_sb, inp_train_tb, u_train_tb, inp_train_int, inp_train_meas, u_train_meas, verbose=verbose)\n",
    "                    loss.backward()\n",
    "\n",
    "                    history.append(loss.item())\n",
    "                    return loss\n",
    "                \n",
    "                optimizer.step(closure=closure)\n",
    "\n",
    "        print('Final Loss: ', history[-1])\n",
    "\n",
    "        return history\n",
    "\n",
    "    ################################################################################################\n",
    "    def plotting(self):\n",
    "        inputs = self.soboleng.draw(100000)\n",
    "\n",
    "        output = self.approximate_solution(inputs).reshape(-1, )\n",
    "        exact_output = self.exact_solution(inputs).reshape(-1, )\n",
    "\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(16, 8), dpi=150)\n",
    "        im1 = axs[0].scatter(inputs[:, 1].detach(), inputs[:, 0].detach(), c=exact_output.detach(), cmap=\"jet\")\n",
    "        axs[0].set_xlabel(\"x\")\n",
    "        axs[0].set_ylabel(\"t\")\n",
    "        plt.colorbar(im1, ax=axs[0])\n",
    "        axs[0].grid(True, which=\"both\", ls=\":\")\n",
    "        im2 = axs[1].scatter(inputs[:, 1].detach(), inputs[:, 0].detach(), c=output.detach(), cmap=\"jet\")\n",
    "        axs[1].set_xlabel(\"x\")\n",
    "        axs[1].set_ylabel(\"t\")\n",
    "        plt.colorbar(im2, ax=axs[1])\n",
    "        axs[1].grid(True, which=\"both\", ls=\":\")\n",
    "        axs[0].set_title(\"Exact Solution\")\n",
    "        axs[1].set_title(\"Approximate Solution\")\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "        err = (torch.mean((output - exact_output) ** 2) / torch.mean(exact_output ** 2)) ** 0.5 * 100\n",
    "        print(\"L2 Relative Error Norm: \", err.item(), \"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_int = 256\n",
    "n_sb = 64\n",
    "n_tb = 64\n",
    "\n",
    "pinn = Pinns(n_int, n_sb, n_tb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 1\n",
    "optimizer_LBFGS = optim.LBFGS(pinn.approximate_solution.parameters(),\n",
    "                              lr=float(0.5),\n",
    "                              max_iter=50000,\n",
    "                              max_eval=50000,\n",
    "                              history_size=150,\n",
    "                              line_search_fn=\"strong_wolfe\",\n",
    "                              tolerance_change=1.0 * np.finfo(float).eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = pinn.fit(num_epochs=n_epochs,\n",
    "                optimizer=optimizer_LBFGS,\n",
    "                verbose=True)\n",
    "\n",
    "plt.figure(dpi=150)\n",
    "plt.grid(True, which=\"both\", ls=\":\")\n",
    "plt.plot(np.arange(1, len(hist) + 1), hist, label=\"Train Loss\")\n",
    "plt.xscale(\"log\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 1\n",
    "x = 0\n",
    "# inputs [t, x]\n",
    "inputs = torch.tensor([t, x], dtype=torch.float, requires_grad=True)\n",
    "output = pinn.approximate_solution(inputs).reshape(-1, )\n",
    "\n",
    "print(\"inputs: \", inputs)\n",
    "print(\"output: \", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tf = output[0]\n",
    "Ts = output[1]\n",
    "t = inputs[0]\n",
    "x = inputs[1]\n",
    "dTf_dx = torch.autograd.grad(Ts, inputs, create_graph=True)[0][1]\n",
    "dTf_dx"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Boundary conditions\n",
    "\n",
    "Here we are going to evaluate the pinn outputs at the points where we have the boundary conditions and we will veridy manually thet they correspond to the correct one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_sb = pinn.soboleng.draw(10)\n",
    "# input_sb.requires_grad_()\n",
    "\n",
    "# # T=0\n",
    "# x0_t0 = pinn.domain_extrema[0, 0]\n",
    "# xL_t0 = pinn.domain_extrema[0, 1]\n",
    "\n",
    "# input_sb_0_t0 = torch.clone(input_sb)\n",
    "# input_sb_0_t0[:, 1] = torch.full(input_sb_0_t0[:, 1].shape, x0_t0)\n",
    "\n",
    "# input_sb_L_t0 = torch.clone(input_sb)\n",
    "# input_sb_L_t0[:, 1] = torch.full(input_sb_L_t0[:, 1].shape, xL_t0)\n",
    "\n",
    "# # T=1\n",
    "# x0_t1 = pinn.domain_extrema[1, 0]\n",
    "# xL_t1 = pinn.domain_extrema[1, 1]\n",
    "\n",
    "# input_sb_0_t1 = torch.clone(input_sb)\n",
    "# input_sb_0_t1[:, 1] = torch.full(input_sb_0_t1[:, 1].shape, x0_t1)\n",
    "\n",
    "# input_sb_L_t1 = torch.clone(input_sb)\n",
    "# input_sb_L_t1[:, 1] = torch.full(input_sb_L_t1[:, 1].shape, xL_t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = pinn.domain_extrema[1, 0]\n",
    "xL = pinn.domain_extrema[1, 1]\n",
    "\n",
    "input_sb = pinn.soboleng.draw(10)\n",
    "input_sb.requires_grad_()\n",
    "\n",
    "input_sb_0 = torch.clone(input_sb)\n",
    "input_sb_0[:, 1] = torch.full(input_sb_0[:, 1].shape, x0)\n",
    "\n",
    "input_sb_L = torch.clone(input_sb)\n",
    "input_sb_L[:, 1] = torch.full(input_sb_L[:, 1].shape, xL)\n",
    "\n",
    "print(\"input_sb_0: \\n\", input_sb_0)\n",
    "print()\n",
    "print(\"input_sb_L: \\n\", input_sb_L)\n",
    "\n",
    "print()\n",
    "print(\"----------------------------------------------\")\n",
    "print()\n",
    "\n",
    "T_0 = pinn.approximate_solution(input_sb_0)\n",
    "Tf_0 = T_0[:,0]\n",
    "Ts_0 = T_0[:,1]\n",
    "dTs_0_dx = torch.autograd.grad(Ts_0.sum(), input_sb_0, create_graph=True)[0][:, 1]\n",
    "\n",
    "T_L = pinn.approximate_solution(input_sb_L)\n",
    "Tf_L = T_L[:,0]\n",
    "Ts_L = T_L[:,1]\n",
    "\n",
    "dTf_L_dx = torch.autograd.grad(Tf_L.sum(), input_sb_L, create_graph=True)[0][:, 1]\n",
    "dTs_L_dx = torch.autograd.grad(Ts_L.sum(), input_sb_L, create_graph=True)[0][:, 1]\n",
    "\n",
    "print(\"dTs_0_dx: \\n\", dTs_0_dx.reshape(-1,1))\n",
    "print()\n",
    "print(\"dTf_L_dx: \\n\", dTf_L_dx.reshape(-1,1))\n",
    "print()\n",
    "print(\"dTs_L_dx: \\n\", dTs_L_dx.reshape(-1,1))\n",
    "\n",
    "\n",
    "\n",
    "# output_sb_0 = ((pinn.T_hot-pinn.T_0)/(1+torch.exp(-200*(input_sb-0.25)))+pinn.T_0)  # is a tensor with two columns [T_f, T_s], however we will just want to use the first one\n",
    "# output_sb_0\n",
    "# save_expected = output_sb_0[:,0]\n",
    "# save_expected"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this other block we are going to call the function that applies the boundary conditions.\n",
    "\n",
    "Note that the output will be $ [T_f, T_s] $ and the 3 conditions are concatenated.\n",
    "\n",
    "Finally note that the values $ 777 $ are for the vacant conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinn.apply_boundary_conditions(torch.cat([input_sb_0, input_sb_0, input_sb_L],0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the output training_set_sb section 1--> Tf_0\n",
    "for input, output in pinn.training_set_sb:\n",
    "    print(output[:int(output.shape[0]/3), :].shape)\n",
    "    print(output[:int(output.shape[0]/3), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the output training_set_sb section 2--> Ts_0_dx\n",
    "for input, output in pinn.training_set_sb:\n",
    "    print(output[int(output.shape[0]/3):int(2*output.shape[0]/3), :].shape)\n",
    "    print(output[int(output.shape[0]/3):int(2*output.shape[0]/3), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the output training_set_sb section 3--> Tf/s_0_dx\n",
    "for input, output in pinn.training_set_sb:\n",
    "    print(output[int(2*output.shape[0]/3):, :].shape)\n",
    "    print(output[int(2*output.shape[0]/3):, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for input, output in pinn.training_set_sb:\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MasterThesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
